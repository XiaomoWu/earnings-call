{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "transient": {
       "display_id": "datatable:css"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "transient": {
       "display_id": "datatable:css"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datatable as dt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from datatable import f\n",
    "from functools import partial\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dt.init_styles()\n",
    "\n",
    "WORK_DIR = '/home/yu/OneDrive/CC'\n",
    "DATA_DIR = f'{WORK_DIR}/data'\n",
    "os.chdir(WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Doc to \"text tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = nlp('She likes movies.')\n",
    "\n",
    "doc.has_annotation(\"DEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DocBin from disk\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "# docs = list(DocBin(store_user_data=True).from_disk('data/doc_sp500_test.spacy').get_docs(nlp.vocab))\n",
    "\n",
    "docs = []\n",
    "for _ in range(1):\n",
    "    docs.extend(list(DocBin(store_user_data=True).from_disk(f'data/doc_sp500_trf_{_}.spacy').get_docs(nlp.vocab)))\n",
    "\n",
    "# register extension for Doc\n",
    "Doc.set_extension('transcriptid', default=None, force=True)\n",
    "\n",
    "# Register extension for Span\n",
    "Span.set_extension('transcriptid', default=None, force=True)\n",
    "Span.set_extension('componentid', default=None, force=True)\n",
    "Span.set_extension('componentorder', default=None, force=True)\n",
    "Span.set_extension('componenttypeid', default=None, force=True)\n",
    "Span.set_extension('speakerid', default=None, force=True)\n",
    "Span.set_extension('speakertypeid', default=None, force=True)\n",
    "Span.set_extension('is_component', default=False, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"text_component_sp500.feather\" (978.0 MB) loaded as \"text_component\" (2s)\n"
     ]
    }
   ],
   "source": [
    "# Select componentid that belongs to MD and QA\n",
    "ld('text_component_sp500', ldname='text_component')\n",
    "text_component = dt.Frame(text_component)\n",
    "\n",
    "# componentid: Management Discussion\n",
    "componentids_md = text_component[(f.transcriptcomponenttypeid==2) & (f.speakertypeid==2), f.transcriptcomponentid].to_list()[0]\n",
    "\n",
    "# componentid: Q & A\n",
    "componentids_qa = text_component[((f.transcriptcomponenttypeid==3) | (f.transcriptcomponenttypeid==4)) & ((f.speakertypeid==2)|(f.speakertypeid==3)), f.transcriptcomponentid].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert Doc to \"text tokens\"\n",
    "\n",
    "# Filtering Rule:\n",
    "# - only keep lemma\n",
    "# - no stop words (stop words is informative while comparing)\n",
    "# - no punctuation\n",
    "# - no \"like number\"\n",
    "\n",
    "def make_text_tokens(docs, componentids):\n",
    "    texttoken = {}\n",
    "    \n",
    "    # For every doc, join the required spans into a list of str\n",
    "    for doc in tqdm(docs):\n",
    "        txttok = []\n",
    "        for span in doc.spans['components']:\n",
    "            if span._.componentid in componentids:\n",
    "                txttok.extend([t.lower_ for t in span \n",
    "                if ((not t.is_punct) & (not t.like_num) & (not t.is_stop))])\n",
    "\n",
    "        # If no text found, add an empty str\n",
    "        if len(txttok)==0:\n",
    "            txttok = ['']\n",
    "\n",
    "        # return\n",
    "        texttoken[doc._.transcriptid] = txttok\n",
    "    \n",
    "    return texttoken\n",
    "\n",
    "texttoken_md = make_text_tokens(docs[:1000], componentids_md)\n",
    "# texttoken_qa = make_text_tokens(docs, componentids_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:00<00:00, 16.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert Doc to \"text tokens\"\n",
    "\n",
    "# Filtering Rule:\n",
    "# - only keep lemma\n",
    "# - no stop words (stop words is informative while comparing)\n",
    "# - no punctuation\n",
    "# - no \"like number\"\n",
    "\n",
    "def make_text_tokens(doc, componentids):\n",
    "    \n",
    "    # For every doc, join the required spans into a list of str\n",
    "    txttok = []\n",
    "    for span in doc.spans['components']:\n",
    "        if span._.componentid in componentids:\n",
    "            txttok.extend([t.lower_ for t in span \n",
    "            if ((not t.is_punct) & (not t.like_num) & (not t.is_stop))])\n",
    "\n",
    "    # If no text found, add an empty str\n",
    "    if len(txttok)==0:\n",
    "        txttok = ['']\n",
    "\n",
    "    # return \n",
    "    return {doc._.transcriptid: txttok}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "    texttoken_md = list(tqdm(executor.map(partial(make_text_tokens, componentids=componentids_md), docs[:1000], chunksize=100), total=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\n# Convert to DTM\\n\\n# Setting:\\n# - keep ALL tokens\\nvectorizer = CountVectorizer(preprocessor=lambda x: x,\\n                             tokenizer=lambda x: x,\\n                             lowercase=False,\\n                             ngram_range=(1,2))\\n\\n# Learn vocabulary \\nvectorizer.fit(texttoken_md.values())\\nvectorizer.fit(texttoken_qa.values())\\n\\n# Make DTM\\ndtm_md = vectorizer.transform(texttoken_md.values())\\ndtm_qa = vectorizer.transform(texttoken_qa.values())\\n\\n# Compute similarity\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nsimilarity = np.diag(cosine_similarity(dtm_md, dtm_qa))\\nsimilarity\\n\\n'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert to DTM\n",
    "\n",
    "# Setting:\n",
    "# - keep ALL tokens\n",
    "vectorizer = CountVectorizer(preprocessor=lambda x: x,\n",
    "                             tokenizer=lambda x: x,\n",
    "                             lowercase=False,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "# Learn vocabulary \n",
    "vectorizer.fit(texttoken_md.values())\n",
    "vectorizer.fit(texttoken_qa.values())\n",
    "\n",
    "# Make DTM\n",
    "dtm_md = vectorizer.transform(texttoken_md.values())\n",
    "dtm_qa = vectorizer.transform(texttoken_qa.values())\n",
    "\n",
    "# Compute similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = np.diag(cosine_similarity(dtm_md, dtm_qa))\n",
    "similarity\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}