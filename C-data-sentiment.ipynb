{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "\n",
    "# pprint is used to format the JSON response\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = \"7f08d854fe9d4919b321c74378c984ce\"\n",
    "endpoint = \"https://earnings-call.cognitiveservices.azure.com\"\n",
    "\n",
    "sentiment_url = endpoint + \"/text/analytics/v3.0/sentiment\"\n",
    "headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets_df = pd.read_feather('data/f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm.feather')\n",
    "\n",
    "text_present = targets_df[['docid', 'text_present']]\n",
    "text_qa = targets_df[['docid', 'text_qa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bad5c4bcd44ae8ab4eafc02a180edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21763.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exceptions encounted = 0\n",
      "Wall time: 6h 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_response(text_df):\n",
    "\n",
    "    results = []\n",
    "    request_docs = []\n",
    "    error_num = 0\n",
    "        \n",
    "    for doc_i, (_, docid, text) in enumerate(tqdm(text_df.itertuples(), total=len(text_df))):\n",
    "        try:\n",
    "            text_chunks = list(chunks(text, 5120))\n",
    "            for chunk_idx, text_chunk in enumerate(text_chunks):\n",
    "                \n",
    "                # if one chunk contains less than 10 characters, skip it.\n",
    "                if len(text_chunk)<=10:\n",
    "                    continue\n",
    "                \n",
    "                # add requests if less than 10 documents\n",
    "                request_docs.append({'id':f'{docid},{chunk_idx}', 'language': 'en', 'text': text_chunk}) \n",
    "\n",
    "                # sanity check\n",
    "                assert len(request_docs)<=10, f'There are more than 10 docs in a request at docid={docid}, chunk_idx={chunk_idx}!'\n",
    "\n",
    "                # otherwise, send request\n",
    "                if len(request_docs)==10 or (doc_i==len(text_present)-1 and chunk_idx==len(text_chunks)-1):\n",
    "\n",
    "                    \n",
    "                    # get response\n",
    "                    response = requests.post(sentiment_url, headers=headers, json={'documents': request_docs})\n",
    "                    response = response.json()\n",
    "\n",
    "                    # check parse errors\n",
    "                    assert len(response['errors']) == 0, f\"There are errors, please check!\\n{response['errors']}\"\n",
    "\n",
    "                    # collect results\n",
    "                    results.extend(response['documents'])\n",
    "\n",
    "                    # reset requests\n",
    "                    request_docs = []\n",
    "\n",
    "        except Exception as e:\n",
    "            # print exceptions\n",
    "            error_num += 1\n",
    "            print(f'error_num={error_num}. Exception caught at docid={docid}, chunk_idx={chunk_idx}!\\n')\n",
    "            print(e)\n",
    "            \n",
    "            # if too many errors, stop\n",
    "            if error_num > 1:\n",
    "                print(f'error_num > 5, stop loop!')\n",
    "                break\n",
    "    \n",
    "    print(f'Exceptions encounted = {error_num}')\n",
    "    return results\n",
    "        \n",
    "    \n",
    "\n",
    "# present_response = get_response(text_present)\n",
    "# sv('present_response')\n",
    "\n",
    "qa_response = get_response(text_qa)\n",
    "sv('qa_response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## format response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing response...\n",
      "Saving as parquet...\n",
      "File saved as \"qa_sentiment.parquet\"\n"
     ]
    }
   ],
   "source": [
    "def format_response(response, save_name):\n",
    "    '''\n",
    "    Args:\n",
    "        response: dict.\n",
    "    '''\n",
    "    sentiment = []\n",
    "\n",
    "    print(f'Parsing response...')\n",
    "    for re in response:\n",
    "        docid, chunk_i = re['id'].split(',')\n",
    "        chunk_sentiment = re['sentiment']\n",
    "        chunk_positive = re['confidenceScores']['positive']\n",
    "        chunk_neutral = re['confidenceScores']['neutral']\n",
    "        chunk_negative = re['confidenceScores']['negative']\n",
    "\n",
    "        for sentence_i, sentence in enumerate(re['sentences']):\n",
    "            sentence_sentiment = sentence['sentiment']\n",
    "            sentence_positive = sentence['confidenceScores']['positive']\n",
    "            sentence_neutral = sentence['confidenceScores']['neutral']\n",
    "            sentence_negative = sentence['confidenceScores']['negative']\n",
    "            sentence = sentence['text']\n",
    "\n",
    "            sentiment.append((docid, chunk_i, chunk_sentiment, chunk_positive, chunk_neutral, \n",
    "                              chunk_negative, sentence_i, sentence_sentiment, sentence_positive, \\\n",
    "                              sentence_neutral, sentence_negative, sentence))\n",
    "            \n",
    "    # save as parquet\n",
    "    print('Saving as parquet...')\n",
    "    sentiment = pd.DataFrame(sentiment, columns=['docid', 'chunk_i', 'chunk_sentiment', 'chunk_positive', 'chunk_neutral', 'chunk_negative', 'sentence_i', 'sentence_sentiment', 'sentence_positive', 'sentence_neutral', 'sentence_negative', 'sentence'])\n",
    "    sentiment_df = pa.Table.from_pandas(sentiment, preserve_index=False)\n",
    "    pq.write_table(sentiment_df, f\"data/{save_name}.parquet\")\n",
    "    print(f'File saved as \"{save_name}.parquet\"')\n",
    "    \n",
    "    # return formated df\n",
    "    return sentiment_df\n",
    "\n",
    "# present_sentiment = format_response(present_response, 'present_sentiment')\n",
    "qa_sentiment = format_response(qa_response, 'qa_sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
