{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# print machine config\\nprint(f'ROOT_DIR: {ROOT_DIR}')\\nprint(f'DATA_DIR: {DATA_DIR}')\\nprint(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\\n\\nif torch.cuda.is_available():\\n    n_cuda = torch.cuda.device_count();\\n    \\n    def log_gpu_memory(verbose=False):\\n        torch.cuda.empty_cache()\\n        if verbose:\\n            for _ in range(n_cuda):\\n                print(f'GPU {_}:')\\n                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\\n        else:\\n            for _ in range(n_cuda):\\n                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\\n                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\\n                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\\n            \\n    print(f'\\n{n_cuda} GPUs found:');\\n    for _ in range(n_cuda):\\n        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\\n        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\\n        \\n    print('\\nGPU memory:');\\n    log_gpu_memory();\\nelse:\\n    print('GPU NOT enabled')\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import argparse\n",
    "import comet_ml\n",
    "import datatable as dt\n",
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import warnings\n",
    "\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict, defaultdict\n",
    "from datatable import f, update\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Init for script use\n",
    "with open(\"/home/yu/OneDrive/App/Settings/jupyter + R + Python/python_startup.py\", 'r') as _:\n",
    "    exec(_.read())\n",
    "\n",
    "os.chdir('/home/yu/OneDrive/CC')\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '/home/yu/OneDrive/CC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = '/home/yu/Data/CC-checkpoints'\n",
    "CHECKPOINT_ARCHIVE_DIR = f'{CHECKPOINT_DIR}/archive'\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "'''\n",
    "# print machine config\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=torch.device('cpu'):\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/archive`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_ARCHIVE_DIR):\n",
    "        os.makedirs(CHECKPOINT_ARCHIVE_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_ARCHIVE_DIR}/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name, force=False):\n",
    "    targets_df = feather.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "    # targets_df = targets_df[targets_df.outlier_flag1==False]\n",
    "    return targets_df\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(window_size):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    return split_df.loc[split_df.window_size==window_size]\n",
    "\n",
    "def load_tid_cid_pair(tid_cid_pair_name):\n",
    "    '''load DataFrame tid_cid_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid:[cid1, cid2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"md\", \"qa\", \"all\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_cid_pair_{tid_cid_pair_name}.feather')\n",
    "    tids = pair.transcriptid.tolist()\n",
    "    cids = [cid.tolist() for cid in pair.componentid]\n",
    "    \n",
    "    return dict(zip(tids, cids))\n",
    "\n",
    "# helpers: load tid_cid_pair\n",
    "def load_tid_from_to_pair(tid_from_to_pair_name):\n",
    "    '''load DataFrame tid_from_to_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid_from:[tid_to1, tid_to2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"3qtr\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_from_to_pair_{tid_from_to_pair_name}.feather')\n",
    "    \n",
    "    tid_from = pair.transcriptid_from\n",
    "    tid_to = [tid.tolist() for tid in pair.transcriptid_to]\n",
    "    \n",
    "    return dict(zip(tid_from, tid_to))\n",
    "\n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, yqtr, window_size):\n",
    "    '''\n",
    "    Given yqtr, find the corresponding ols_rmse from `performance_by_model.feather`.\n",
    "    Always compare to the same model: 'ols: car_stand ~ fr'\n",
    "    then log to Comet\n",
    "    '''\n",
    "    performance = dt.Frame(pd.read_feather('data/performance_by_yqtr.feather'))\n",
    "\n",
    "\n",
    "    ols_rmse = performance[(f.model_name=='ols: car_stand ~ fr') & (f.window_size==window_size) & (f.yqtr==yqtr), f.rmse][0,0]\n",
    "    logger.experiment.log_parameter('ols_rmse', ols_rmse)\n",
    "    \n",
    "def log_test_start(logger, window_size, yqtr):\n",
    "    '''\n",
    "    Given window, find the corresponding star/end date of the training/test periods, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, *_ = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.window_size==window_size)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, yqtr, split_type, text_in_dataset,\n",
    "                 window_size, targets_df, split_df, preemb_dir=None,\n",
    "                 tid_cid_pair1=None, tid_from_to_pair1=None,\n",
    "                 tid_cid_pair2=None, tid_from_to_pair2=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings: dict of pre-embeddings. In the form\n",
    "              `{tid:{cid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for component level and \n",
    "              `{tid:{sid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for sentence level.\n",
    "              \n",
    "            targets_df: DataFrame of targets variables.\n",
    "            split_df: DataFrame that keeps the split of windows\n",
    "            ytqr: str. e.g., \"2008-q3\"\n",
    "            split_type: str. 'train', 'val', or 'test'\n",
    "            text_in_dataset: also output text embedding if true.\n",
    "            \n",
    "            tid_cid_pair: Dict of transcriptid and componentid/sentenceid for\n",
    "              text that will be used. In the form \n",
    "              `{tid:[cid1, cid2, ...]}` or `{tid:[sid1, sid2, ...]}`\n",
    "              Note! [cid1, cid2, ...] must be in the same order as in original \n",
    "              transcript!\n",
    "        '''\n",
    "            \n",
    "        # get split dates from `split_df`\n",
    "        train_start, train_end, test_start, test_end, _, yqtr = \\\n",
    "            tuple(split_df.loc[(split_df.yqtr==yqtr) & \\\n",
    "                               (split_df.window_size==window_size)].iloc[0])\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # generate targets_df for train, val, test \n",
    "        if split_type=='train':\n",
    "            # print current window\n",
    "            print(f'Current window: {yqtr} ({window_size}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "            \n",
    "            targets_df = targets_df[targets_df.ciq_call_date\\\n",
    "                                    .between(train_start, train_end)]\\\n",
    "                                    .sample(frac=1, random_state=42)\n",
    "            # targets_df = targets_df.iloc[:int(len(targets_df)*0.9)]\n",
    "            \n",
    "        elif split_type=='val':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date\\\n",
    "                                    .between(train_start, train_end)]\\\n",
    "                                    .sample(frac=1, random_state=42)\n",
    "            targets_df = targets_df.iloc[int(len(targets_df)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date\n",
    "                                    .between(test_start, test_end)]\n",
    "\n",
    "        \n",
    "        if text_in_dataset:\n",
    "            # make sure targets_df only contains transcriptid that're also \n",
    "            # in preembeddings\n",
    "            # tid_on_disk: valid tids save in \"preemb_dir\"\n",
    "            tid_on_disk = set(int(tid.split('.')[0]) \n",
    "                              for tid in os.listdir(preemb_dir)\n",
    "                              if re.search('\\d+\\.pt', tid))\n",
    "            targets_df = targets_df.loc[targets_df.transcriptid.isin(tid_on_disk)]\n",
    "            \n",
    "            self.tid_cid_pair1 = tid_cid_pair1\n",
    "            self.tid_cid_pair2 = tid_cid_pair2\n",
    "            self.tid_from_to_pair1 = tid_from_to_pair1\n",
    "            self.tid_from_to_pair2 = tid_from_to_pair2\n",
    "            \n",
    "        # Assign states\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        self.preemb_dir = preemb_dir\n",
    "\n",
    "        self.targets_df = targets_df\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.split_type = split_type\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        targets = self.targets_df.iloc[idx]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        transcriptid = targets.transcriptid\n",
    "        car_stand = targets.car_0_30_stand\n",
    "        revision_stand = targets.revision_stand\n",
    "        inflow_stand = targets.inflow_stand\n",
    "        retail_stand = targets.retail_stand\n",
    "        \n",
    "        # using the normalized features\n",
    "        similarity = targets.similarity_bigram_stand\n",
    "        sentiment = targets.sentiment_negative_qa_analyst_stand\n",
    "        sue = targets.sue_stand\n",
    "        sest = targets.sest_stand        \n",
    "        alpha = targets.alpha_stand\n",
    "        volatility = targets.volatility_stand\n",
    "        mcap = targets.mcap_stand\n",
    "        bm = targets.bm_stand\n",
    "        roa = targets.roa_stand\n",
    "        debt_asset = targets.debt_asset_stand\n",
    "        numest = targets.numest_stand\n",
    "        smedest = targets.smedest_stand\n",
    "        sstdest = targets.sstdest_stand\n",
    "        car_m1_m1 = targets.car_m1_m1_stand\n",
    "        car_m2_m2 = targets.car_m2_m2_stand\n",
    "        car_m30_m3 = targets.car_m30_m3_stand\n",
    "        volume = targets.volume_stand\n",
    "\n",
    "        if self.text_in_dataset:\n",
    "            emb1, emb2 = None, None\n",
    "            \n",
    "            emb1 = assemble_embedding(transcriptid, self.preemb_dir,\n",
    "                                      self.tid_cid_pair1,\n",
    "                                      self.tid_from_to_pair1)\n",
    "            if self.tid_cid_pair2 != None:\n",
    "                emb2 = assemble_embedding(transcriptid, self.preemb_dir, \n",
    "                                          self.tid_cid_pair2,\n",
    "                                          self.tid_from_to_pair2)\n",
    "\n",
    "            return transcriptid, \\\n",
    "                   car_stand, inflow_stand, revision_stand, retail_stand, \\\n",
    "                   emb1, emb2, [similarity, sentiment], \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest,\\\n",
    "                    sstdest, smedest, mcap, roa, bm, debt_asset, volatility,\\\n",
    "                    volume]\n",
    "        else:\n",
    "            return torch.tensor(transcriptid,dtype=torch.int64), \\\n",
    "                   torch.tensor(car_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor(inflow_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor(revision_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor(retail_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],\n",
    "                                dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "                                 sest, sue, numest, sstdest, smedest, mcap,\\\n",
    "                                 roa, bm, debt_asset, volatility, volume],\n",
    "                                dtype=torch.float32)\n",
    "      \n",
    "    \n",
    "def assemble_embedding(transcriptid, preemb_dir, \n",
    "                       tid_cid_pair, tid_from_to_pair):\n",
    "    '''Assemble embeddings belonging to the same tid into one Tensor\n",
    "    \n",
    "    Method:\n",
    "        1) Given transcriptid, use it as \"transcriptid_from\" to retrieve all the \n",
    "           corresponding \"transcriptid_to\" from table \"tid_from_to_pair\"\n",
    "        2) For every transcript_to, retrieve all the corresponding cids from table\n",
    "           \"tid_cid_pair\"\n",
    "    '''\n",
    "    # find tids that we'll consider\n",
    "    tids_to = tid_from_to_pair[transcriptid]\n",
    "    \n",
    "    # for every tid, merge its components\n",
    "    output = []\n",
    "    \n",
    "    for tid_to in tids_to:\n",
    "        comps = torch.load(f'{preemb_dir}/{tid_to}.pt')\n",
    "        emb = [torch.as_tensor(comps[cid]['embedding']) \n",
    "               for cid in tid_cid_pair.get(tid_to, [])]\n",
    "        output.extend(emb)\n",
    "        \n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then define DataModule\n",
    "class CCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, yqtr, targets_name, batch_size, val_batch_size,\n",
    "                 test_batch_size, text_in_dataset, window_size, \n",
    "                 preemb_dir=None,\n",
    "                 tid_cid_pair_name1=None, tid_from_to_pair_name1=None,\n",
    "                 tid_cid_pair_name2=None, tid_from_to_pair_name2=None):\n",
    "        '''\n",
    "        preemb_dir: Directory of pre-embedding files, where every transcript is \n",
    "            saved into a single file.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.yqtr = yqtr\n",
    "        self.preemb_dir = preemb_dir\n",
    "        self.targets_name = targets_name\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        self.window_size = window_size\n",
    "        self.tid_cid_pair_name1 = tid_cid_pair_name1\n",
    "        self.tid_cid_pair_name2 = tid_cid_pair_name2\n",
    "        self.tid_from_to_pair_name1 = tid_from_to_pair_name1\n",
    "        self.tid_from_to_pair_name2 = tid_from_to_pair_name2\n",
    "        \n",
    "    # Dataset\n",
    "    def setup(self):\n",
    "        # read the preembedding, targests, and split_df\n",
    "        tid_cid_pair1, tid_cid_pair2 = None, None\n",
    "        tid_from_to_pair1, tid_from_to_pair2 = None, None\n",
    "            \n",
    "        if self.text_in_dataset:\n",
    "            tid_cid_pair1 = load_tid_cid_pair(self.tid_cid_pair_name1)\n",
    "            tid_from_to_pair1 = load_tid_from_to_pair(self.tid_from_to_pair_name1)\n",
    "            \n",
    "            if self.tid_cid_pair_name2 != None:\n",
    "                tid_cid_pair2 = load_tid_cid_pair(self.tid_cid_pair_name2)\n",
    "                tid_from_to_pair2 = \\\n",
    "                    load_tid_from_to_pair(self.tid_from_to_pair_name2)\n",
    "            \n",
    "        targets_df = load_targets(self.targets_name)\n",
    "        split_df = load_split_df(self.window_size)\n",
    "\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.yqtr, \n",
    "                                       split_type='train',\n",
    "                                       text_in_dataset=self.text_in_dataset,\n",
    "                                       window_size=self.window_size,\n",
    "                                       targets_df=targets_df, \n",
    "                                       split_df=split_df,\n",
    "                                       preemb_dir=self.preemb_dir,\n",
    "                                       tid_cid_pair1=tid_cid_pair1,\n",
    "                                       tid_cid_pair2=tid_cid_pair2,\n",
    "                                       tid_from_to_pair1=tid_from_to_pair1,\n",
    "                                       tid_from_to_pair2=tid_from_to_pair2)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.yqtr, split_type='val',\n",
    "                                     text_in_dataset=self.text_in_dataset,\n",
    "                                     window_size=self.window_size,\n",
    "                                     targets_df=targets_df,\n",
    "                                     split_df=split_df,\n",
    "                                     preemb_dir=self.preemb_dir,\n",
    "                                     tid_cid_pair1=tid_cid_pair1,\n",
    "                                     tid_cid_pair2=tid_cid_pair2,\n",
    "                                     tid_from_to_pair1=tid_from_to_pair1,\n",
    "                                     tid_from_to_pair2=tid_from_to_pair2)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.yqtr, split_type='test',\n",
    "                                      text_in_dataset=self.text_in_dataset, \n",
    "                                      window_size=self.window_size,\n",
    "                                      targets_df=targets_df,\n",
    "                                      split_df=split_df,\n",
    "                                      preemb_dir=self.preemb_dir,\n",
    "                                      tid_cid_pair1=tid_cid_pair1,\n",
    "                                      tid_cid_pair2=tid_cid_pair2,\n",
    "                                      tid_from_to_pair1=tid_from_to_pair1,\n",
    "                                      tid_from_to_pair2=tid_from_to_pair2)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=True, drop_last=False, num_workers=4,\n",
    "                          pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - It's safe to set `drop_last=True` without under-counting samples.\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.val_batch_size,\n",
    "                          num_workers=4, pin_memory=True, collate_fn=collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, batch_size=self.test_batch_size, num_workers=4, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            target variables: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        transcriptid, car, inflow, revision, retail, \\\n",
    "        emb1, emb2, manual_text, fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        emb1, mask1 = create_emb(emb1)\n",
    "\n",
    "        mask2 = (None,)*len(emb2)\n",
    "        if sum([_!=None for _ in emb2])>0:\n",
    "            emb2, mask2 = create_emb(emb2)\n",
    "        \n",
    "        return torch.tensor(transcriptid, dtype=torch.float32), \\\n",
    "               torch.tensor(car, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), \\\n",
    "               torch.tensor(retail, dtype=torch.float32),\\\n",
    "               emb1, mask1, emb2, mask2, \\\n",
    "               torch.tensor(manual_text, dtype=torch.float32),\\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)\n",
    "    \n",
    "def create_emb(embeddings):\n",
    "    valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "    embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "    # mask: (N, T)\n",
    "    mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "    for i, length in enumerate(valid_seq_len):\n",
    "        mask[i, :length] = 0\n",
    "    mask = mask == 1\n",
    "    \n",
    "    return embeddings.float(), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # initialize pl.metrics here\n",
    "        self.mse_loss = pl.metrics.MeanSquaredError()\n",
    "        \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_ckpt(datamodule, logger):\n",
    "    # save & log `y_car`\n",
    "    ckpt_path = f\"{CHECKPOINT_DIR}/{trainer_hparams['note']}_{data_hparams['yqtr']}*.ckpt\"\n",
    "    ckpt_path = glob.glob(ckpt_path)\n",
    "    assert len(ckpt_path)==1, f'Multiple checkpoints found: {ckpt_path}'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "\n",
    "    # init model from checkpoint\n",
    "    model = Model.load_from_checkpoint(ckpt_path)\n",
    "    model.eval()\n",
    "\n",
    "    transcriptids = []\n",
    "    y_car, y_rev, y_inf, y_ret = [], [], [], []\n",
    "    t_car, t_rev, t_inf, t_ret = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in datamodule.test_dataloader():\n",
    "            res = model.forward(batch)\n",
    "            \n",
    "            transcriptids.extend(res['transcriptid'].tolist())\n",
    "            y_car.extend(res['y_car'].tolist())\n",
    "            t_car.extend(res['t_car'].tolist())\n",
    "            \n",
    "            if 'y_rev' in res:\n",
    "                y_rev.extend(res['y_rev'].tolist())\n",
    "            if 'y_inf' in res:\n",
    "                y_inf.extend(res['y_inf'].tolist())\n",
    "            if 'y_ret' in res:\n",
    "                y_ret.extend(res['y_ret'].tolist()) \n",
    "            if 't_rev' in res:\n",
    "                t_rev.extend(res['t_rev'].tolist())\n",
    "            if 't_inf' in res:\n",
    "                t_inf.extend(res['t_inf'].tolist())\n",
    "            if 't_ret' in res:\n",
    "                t_ret.extend(res['t_ret'].tolist())\n",
    "\n",
    "    # upload yt\n",
    "    df = dt.Frame({'transcriptid':transcriptids,\n",
    "                   'y_car':y_car,\n",
    "                   't_car':t_car})\n",
    "    if len(y_inf)==len(y_car):\n",
    "        df[:, update(y_inf=dt.Frame(y_inf))]\n",
    "    if len(y_rev)==len(y_car):\n",
    "        df[:, update(y_rev=dt.Frame(y_rev))]\n",
    "    if len(y_ret)==len(y_car):\n",
    "        df[:, update(y_ret=dt.Frame(y_ret))]\n",
    "        \n",
    "    if len(t_rev)==len(t_car):\n",
    "        df[:, update(t_rev=dt.Frame(t_rev))]\n",
    "    if len(t_inf)==len(t_car):\n",
    "        df[:, update(t_inf=dt.Frame(t_inf))]\n",
    "    if len(t_ret)==len(t_car):\n",
    "        df[:, update(t_ret=dt.Frame(t_ret))]\n",
    "    \n",
    "    \n",
    "    test_results = f'y_car_{np.random.randint(1e5)}.feather'\n",
    "    feather.write_feather(df.to_pandas(), test_results)\n",
    "    logger.experiment.log_asset(test_results)\n",
    "    os.unlink(test_results)\n",
    "\n",
    "    \n",
    "    # upload rmse\n",
    "    rmse = pl.metrics.functional.mean_squared_error(torch.Tensor(y_car),\n",
    "                                                    torch.Tensor(t_car))\n",
    "    rmse = torch.sqrt(rmse).item()\n",
    "    logger.experiment.log_parameter('test_rmse', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams):\n",
    "\n",
    "    # ----------------------\n",
    "    # `hparams` sanity check\n",
    "    # ----------------------\n",
    "    \n",
    "    # check: batch_size//len(gpus)==0\n",
    "    assert data_hparams['batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['val_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['val_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: test_batch_size//len(gpus)==0\n",
    "    assert data_hparams['test_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`test_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['test_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Initialize model and trainer\n",
    "    # ----------------------------\n",
    "    \n",
    "    # init model\n",
    "    model = Model(**model_hparams)\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{trainer_hparams['note']}_{data_hparams['yqtr']}\".replace('*', '')\n",
    "    ckpt_prefix = ckpt_prefix + '_{epoch}'\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            verbose=True,\n",
    "            mode='min',\n",
    "            monitor='val_loss',\n",
    "            dirpath=CHECKPOINT_DIR,\n",
    "            filename=ckpt_prefix,\n",
    "            save_top_k=trainer_hparams['save_top_k'],\n",
    "            period=trainer_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=trainer_hparams['early_stop_patience'],\n",
    "        verbose=True,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=trainer_hparams['gpus'], \n",
    "                         precision=trainer_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         callbacks=[early_stop_callback],\n",
    "                         overfit_batches=trainer_hparams['overfit_batches'], \n",
    "                         log_every_n_steps=trainer_hparams['log_every_n_steps'],\n",
    "                         val_check_interval=trainer_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=20, \n",
    "                         accelerator='ddp',\n",
    "                         accumulate_grad_batches=trainer_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=trainer_hparams['min_epochs'],\n",
    "                         max_epochs=trainer_hparams['max_epochs'], \n",
    "                         max_steps=trainer_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # add n_model_params\n",
    "    trainer_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(data_hparams)\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(trainer_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, data_hparams['yqtr'], data_hparams['window_size'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, data_hparams['window_size'], data_hparams['yqtr'])\n",
    "    \n",
    "    # refresh GPU memory\n",
    "    # refresh_cuda_memory()\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # fit and test\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            # create datamodule\n",
    "            datamodule = CCDataModule(**data_hparams)\n",
    "            datamodule.setup()\n",
    "            \n",
    "            # train the model\n",
    "            trainer.fit(model, datamodule)\n",
    "        \n",
    "            # test with best ckpt\n",
    "            test_with_ckpt(datamodule, logger)\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        # refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true",
    "lines_to_next_cell": 1
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, learning_rate, dropout, model_type='MLP'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(17, 32)\n",
    "        self.fc_2 = nn.Linear(32, 1)\n",
    "        #self.fc_3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        \n",
    "        return {'transcriptid': transcriptid,\n",
    "                'y_car': y_car, \n",
    "                't_car': t_car}\n",
    "    \n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, car, car_stand, inflow, inflow_stand, \\\n",
    "        revision, revision_stand, manual_txt, fin_ratios = batch\n",
    "        \n",
    "        x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "        # x = fin_ratios\n",
    "        \n",
    "        x_car = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        y_car = self.fc_2(x_car) # (N, 1)    \n",
    "        \n",
    "        t_car = car_stand\n",
    "        \n",
    "        # regularize dimension\n",
    "        y_car = y_car.squeeze(-1)\n",
    "        \n",
    "        return transcriptid, y_car, t_car \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        \n",
    "        loss = self.mse_loss(y_car, t_car)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        \n",
    "        loss = self.mse_loss(y_car, t_car)\n",
    "        self.log('val_loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# parse arg\n",
    "parser = argparse.ArgumentParser(description='Earnings Call')\n",
    "parser.add_argument('--yqtr', type=str, required=True)\n",
    "parser.add_argument('--window_size', type=str, required=True)\n",
    "parser.add_argument('--note', type=str, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 256,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': args.window_size # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'learning_rate': 1e-2,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    \n",
    "    # last: \n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MLP-03\", # key!\n",
    "    'log_every_n_steps': 50,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\n",
    "    'max_steps': None, # default None\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 25, # default: 25\n",
    "\n",
    "    # Caution:\n",
    "    'checkpoint_period': 1} # default 1\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "# refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "\n",
    "# update current period\n",
    "data_hparams.update({'yqtr': args.yqtr})\n",
    "\n",
    "# train on select periods\n",
    "train_one(Model, args.yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true",
    "lines_to_next_cell": 1
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# CCGRU\n",
    "class CCGRU(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # set model types\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'univariate'\n",
    "        self.model_type = 'gru'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # layers\n",
    "        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\n",
    "                                 dropout=0.1, bidirectional=True)\n",
    "        self.dropout_expert = nn.Dropout(hparams.dropout)\n",
    "        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, valid_seq_len):\n",
    "        # Note: inp is [N, S, E] and **already** been packed\n",
    "        self.gru_expert.flatten_parameters()\n",
    "        \n",
    "        # if S is longer than `max_seq_len`, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\n",
    "        \n",
    "        # RNN layers\n",
    "        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\n",
    "        \n",
    "        # final FC layers\n",
    "        y_car = self.linear_car(x_expert) # (N, E)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'trainer_loss': loss_car}}\n",
    "            \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}        \n",
    "    \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true"
   },
   "source": [
    "# STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt\n",
    "class CCTransformerSTLTxt(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, n_layers_encoder, dff, max_seq_len, model_type='STL', dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for CAR\n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.fc_2 = nn.Linear(32, 1)\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # forward\n",
    "    def shared_step(self, batch):\n",
    "        car, car_stand, inflow, inflow_stand, revision, revision_stand, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # decode with avgpool\n",
    "        x_expert = x_expert.mean(1) # (N, E)\n",
    "        \n",
    "        # decode with maxpool\n",
    "        # x_expert_maxpool = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        # concat\n",
    "        # x_expert = torch.cat([x_expert_avgpool, x_expert_maxpool], dim=-1) # (N, 2E)\n",
    "\n",
    "        # final FC\n",
    "        y_car = self.fc_1(x_expert) # (N, 1)\n",
    "        # y_car = self.fc_2(y_car)\n",
    "        \n",
    "        t_car = car_stand # (N,)\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid, y_car.squeeze(), t_car \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxtFr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt + fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, \n",
    "                 n_layers_encoder, dff, max_seq_len, model_type='STL', n_finratios=15, dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers \n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        # txt_mixer_layer = TxtMixerLayer(self.hparams.d_model)\n",
    "        # self.txt_mixer = FeatureMixer(txt_mixer_layer, self.hparams.n_layers_txtmixer)\n",
    "        \n",
    "        # fr_mixer_layers = FrMixerLayer(self.n_covariate)\n",
    "        # self.fr_mixer = FeatureMixer(fr_mixer_layers, self.hparams.n_layers_frmixer)\n",
    "        \n",
    "        # final prediction layer\n",
    "        # final_fc_mixer_layer = FeatureMixerLayer(self.hparams.d_model+self.n_covariate)\n",
    "        # self.final_fc_mixer_layer = FeatureMixer(final_fc_mixer_layer, self.hparams.n_layers_finalfc)\n",
    "        # self.fc_batchnorm = nn.BatchNorm1d(self.hparams.d_model+self.n_covariate)\n",
    "        self.final_fc = nn.Linear(self.hparams.d_model+self.hparams.n_finratios, 1)\n",
    "        \n",
    "        # self.txt_fc_1 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.txt_fc_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        # self.txt_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_3 = nn.Dropout(self.hparams.dropout) \n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        car, car_stand, inflow, inflow_stand, revision, revision_stand, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        x_expert = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        \n",
    "        # project text embedding to a lower dimension\n",
    "        # x_expert = self.txt_dropout_1(F.relu(self.txt_fc_1(x_expert)))\n",
    "        # x_expert = F.relu(self.txt_fc_2(x_expert))\n",
    "        \n",
    "        # x_expert = self.txt_mixer(x_expert)\n",
    "        \n",
    "        # Mix fin_ratios\n",
    "        # fin_ratios = self.batch_stand(fin_ratios)\n",
    "        # x_fr = self.fr_mixer(fin_ratios)\n",
    "        \n",
    "        # concate `x_final` with `fin_ratios`\n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_finratios)\n",
    "        \n",
    "        # final FC\n",
    "        # x_final = self.fc_dropout_1(F.relu(self.fc_1(x_expert))) # (N, E+X)\n",
    "        # x_car = self.final_fc_mixer_layer(x_final) # (N, E+X)\n",
    "        y_car = self.final_fc(x_final)\n",
    "        \n",
    "        t_car = car_stand\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# choose Model\\nModel = CCTransformerSTLTxt\\n\\n# data hparams\\ndata_hparams = {\\n    # inputs\\n    \\'preembedding_name\\': \\'longformer\\', \\n    \\'targets_name\\': \\'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_stand_outlier\\', \\n    \\'tid_cid_pair_name\\': \\'qa\\', \\n    \\'tid_from_to_pair_name\\': \\'7qtr\\',\\n    \\n    # batch size\\n    \\'batch_size\\': 12,\\n    \\'val_batch_size\\':8,\\n    \\'test_batch_size\\':8,\\n    \\n    # window_size\\n    \\'text_in_dataset\\': True,\\n    \\'window_size\\': \\'2008-2017\\', # key!\\n}\\n\\n# hparams\\nmodel_hparams = {\\n    \\'max_seq_len\\': 768, \\n    \\'learning_rate\\':3e-4, # key!\\n    \\'n_layers_encoder\\': 4,\\n    \\'n_head_encoder\\': 8, \\n    \\'d_model\\': 768,\\n    \\'dff\\': 2048, # default: 2048\\n    \\'attn_dropout\\': 0.1,\\n    # \\'dropout\\': 0.5\\n} \\n\\n# train hparams\\ntrainer_hparams = {\\n    # random seed\\n    \\'seed\\': 42,    # key\\n    \\n    # gpus\\n    \\'gpus\\': [0,1], # key\\n\\n    # last: STL-57\\n    \\'machine\\': \\'yu-workstation\\', # key!\\n    \\'note\\': f\"STL-57\", # key!\\n    \\'log_every_n_steps\\': 10,\\n    \\'save_top_k\\': 1,\\n    \\'val_check_interval\\': 0.2, # key! (Eg: 0.25 - check 4 times in a epoch)\\n\\n    # epochs\\n    \\'precision\\': 32, # key!\\n    \\'overfit_batches\\': 0.0, # default 0.0. decimal or int\\n    \\'min_epochs\\': 3, # default: 3\\n    \\'max_epochs\\': 20, # default: 20\\n    \\'max_steps\\': None, # default: None\\n    \\'accumulate_grad_batches\\': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    \\'early_stop_patience\\': 8,\\n\\n    # Caution:\\n    # In pervious versions, if you check validatoin multiple times within a epoch,\\n    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \\n    # you check validation multiples times within an epoch, you still need to set\\n    # `checkpoint_period=1`.\\n    \\'checkpoint_period\\': 1}\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt()\\n\\n# load split_df\\nsplit_df = load_split_df(data_hparams[\\'window_size\\'])\\n\\n# load tid_cid_pair\\n# loop over windows!\\nfor yqtr in split_df.yqtr:\\n    np.random.seed(trainer_hparams[\\'seed\\'])\\n    torch.manual_seed(trainer_hparams[\\'seed\\'])\\n    \\n    # Enforce yqtr>=\\'2012-q4\\' (the earliest yqtr in window_size==\\'3y\\')\\n    # if yqtr == \\'non-roll-01\\':\\n\\n    # update current period\\n    data_hparams.update({\\'yqtr\\': yqtr})\\n\\n    # train on select periods\\n    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# choose Model\n",
    "Model = CCTransformerSTLTxt\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    # inputs\n",
    "    'preembedding_name': 'longformer', \n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_stand_outlier', \n",
    "    'tid_cid_pair_name': 'qa', \n",
    "    'tid_from_to_pair_name': '7qtr',\n",
    "    \n",
    "    # batch size\n",
    "    'batch_size': 12,\n",
    "    'val_batch_size':8,\n",
    "    'test_batch_size':8,\n",
    "    \n",
    "    # window_size\n",
    "    'text_in_dataset': True,\n",
    "    'window_size': '2008-2017', # key!\n",
    "}\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':3e-4, # key!\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 768,\n",
    "    'dff': 2048, # default: 2048\n",
    "    'attn_dropout': 0.1,\n",
    "    # 'dropout': 0.5\n",
    "} \n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # last: STL-57\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"STL-57\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2, # key! (Eg: 0.25 - check 4 times in a epoch)\n",
    "\n",
    "    # epochs\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0, # default 0.0. decimal or int\n",
    "    'min_epochs': 3, # default: 3\n",
    "    'max_epochs': 20, # default: 20\n",
    "    'max_steps': None, # default: None\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "# load tid_cid_pair\n",
    "# loop over windows!\n",
    "for yqtr in split_df.yqtr:\n",
    "    np.random.seed(trainer_hparams['seed'])\n",
    "    torch.manual_seed(trainer_hparams['seed'])\n",
    "    \n",
    "    # Enforce yqtr>='2012-q4' (the earliest yqtr in window_size=='3y')\n",
    "    # if yqtr == 'non-roll-01':\n",
    "\n",
    "    # update current period\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    # train on select periods\n",
    "    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCMTLFr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMTLFr(CC):\n",
    "    def __init__(self, learning_rate, dropout, alpha, model_type='MTL'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(17, 32)\n",
    "        # self.fc_2 = nn.Linear(16, 16)\n",
    "        self.fc_car = nn.Linear(32, 1)\n",
    "        self.fc_rev = nn.Linear(32, 1)\n",
    "        self.fc_inf = nn.Linear(32, 1)\n",
    "        self.fc_ret = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, y_car, y_rev, y_inf, y_ret, \\\n",
    "        t_car, t_rev, t_inf, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        return {'transcriptid': transcriptid,\n",
    "                'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf, 'y_ret': y_ret,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf, 't_ret': t_ret}\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, t_car, t_inf, t_rev, t_ret, \\\n",
    "        manual_txt, fin_ratios = batch\n",
    "        \n",
    "        x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "        # x = fin_ratios\n",
    "        \n",
    "        x = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        # x = self.dropout_2(F.relu(self.fc_2(x)))\n",
    "        y_car = self.fc_car(x) # (N, 1)    \n",
    "        y_rev = self.fc_rev(x) # (N, 1)\n",
    "        y_inf = self.fc_inf(x) # (N, 1)\n",
    "        y_ret = self.fc_ret(x) # (N, 1)\n",
    "        \n",
    "        # regularize dimension\n",
    "        y_car = y_car.squeeze(-1)\n",
    "        y_rev = y_rev.squeeze(-1)\n",
    "        y_inf = y_inf.squeeze(-1)\n",
    "        y_ret = y_ret.squeeze(-1)\n",
    "        \n",
    "        return transcriptid, y_car, y_rev, y_inf, y_ret, \\\n",
    "               t_car, t_rev, t_inf, t_ret \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, y_rev, y_inf, y_ret, \\\n",
    "        t_car, t_rev, t_inf, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        loss_ret = self.mse_loss(y_ret, t_ret)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(0.6*loss_rev + 0.23*loss_inf + 0.17*loss_ret)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, y_rev, y_inf, y_ret, \\\n",
    "        t_car, t_rev, t_inf, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        loss_ret = self.mse_loss(y_ret, t_ret)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(0.6*loss_rev + 0.23*loss_inf + 0.17*loss_ret)\n",
    "        \n",
    "        self.log('val_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# parse arg\n",
    "parser = argparse.ArgumentParser(description='Earnings Call')\n",
    "parser.add_argument('--yqtr', type=str, required=True)\n",
    "parser.add_argument('--window_size', type=str, required=True)\n",
    "parser.add_argument('--note', type=str, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "yqtr = args.yqtr\n",
    "window_size = args.window_size\n",
    "\n",
    "# choose Model\n",
    "Model = CCMTLFr\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final_addretail', # key!\n",
    "\n",
    "    'batch_size': 256,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': window_size, # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'alpha': 0.1, # key!\n",
    "    'learning_rate': 1e-3,\n",
    "    'dropout': 0.1, # default: 0.5\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': args.note, \n",
    "    'log_every_n_steps': 50,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 300, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\n",
    "    'max_steps': None, \n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 25, # default: 10\n",
    "\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "print(f'Start training...{trainer_hparams[\"note\"]}')\n",
    "\n",
    "\n",
    "# train on select periods\n",
    "data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCMTLFrTxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMTLFrTxt(CC):\n",
    "    def __init__(self, learning_rate, d_model, max_seq_len, dropout, alpha, \n",
    "                 n_head_encoder, n_layers_encoder, dff, model_type='MTLTxt'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model)\n",
    "        \n",
    "        # Build Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(self.hparams.d_model,\n",
    "                                                   self.hparams.n_head_encoder,\n",
    "                                                   self.hparams.dff)\n",
    "        \n",
    "        # atten layers for CAR\n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, self.hparams.n_layers_encoder)\n",
    "        \n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc_1 = nn.Linear(768*3, 16)\n",
    "        self.fc_2 = nn.Linear(33, 32)\n",
    "        self.fc_car = nn.Linear(32, 1)\n",
    "        self.fc_inf = nn.Linear(32, 1)\n",
    "        self.fc_rev = nn.Linear(32, 1)\n",
    "        self.fc_ret = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, \\\n",
    "        y_car, y_inf, y_rev, y_ret, \\\n",
    "        t_car, t_inf, t_rev, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        return {'transcriptid': transcriptid,\n",
    "                'y_car': y_car, 'y_inf': y_inf, 'y_rev': y_rev, 'y_ret': y_ret,\n",
    "                't_car': t_car, 't_inf': t_inf, 't_rev': t_rev, 't_ret': t_ret}\n",
    "    \n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        # embeddings: list\n",
    "        transcriptid, t_car, t_inf, t_rev, t_ret,\\\n",
    "        emb_ana, mask_ana, emb_man, mask_man, \\\n",
    "        manual_text, fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        emb_ana = emb_ana[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        emb_man = emb_man[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        mask_ana = mask_ana[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        mask_man = mask_man[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        emb_ana = emb_ana.transpose(0, 1) # (S, N, E)\n",
    "        emb_man = emb_man.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        emb_ana = self.encoder_pos(emb_ana) # (S, N, E)\n",
    "        emb_man = self.encoder_pos(emb_man) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_ana = self.encoder(emb_ana, src_key_padding_mask=mask_ana)\\\n",
    "            .transpose(0,1) # (N, S, E)\n",
    "        x_man = self.encoder(emb_man, src_key_padding_mask=mask_man)\\\n",
    "            .transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # decode with avgpool\n",
    "        x_ana = x_ana.mean(1) # (N, E)\n",
    "        x_man = x_man.mean(1) # (N, E)\n",
    "        x_diff = x_ana-x_man  # (N, E)\n",
    "        \n",
    "        x_text = torch.cat([x_ana, x_man, x_diff], dim=1)\n",
    "        \n",
    "        # decode with maxpool\n",
    "        # x_expert_maxpool = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        # Reduce dimension of text features\n",
    "        x_text = self.dropout_1(F.relu(self.fc_1(x_text))) # (N, 16)\n",
    "        \n",
    "        x_fr = torch.cat([fin_ratios, manual_text], dim=-1) # (N, 2+15)\n",
    "\n",
    "        # concat: text + fr\n",
    "        x = torch.cat([x_text, x_fr], dim=-1) # (N, 16+17)\n",
    "        \n",
    "        x = self.dropout_2(F.relu(self.fc_2(x)))\n",
    "        y_car = self.fc_car(x) # (N, 1)    \n",
    "        y_inf = self.fc_inf(x) # (N, 1)\n",
    "        y_rev = self.fc_rev(x) # (N, 1)\n",
    "        y_ret = self.fc_ret(x) # (N, 1)\n",
    "        \n",
    "        # regularize dimension\n",
    "        y_car = y_car.squeeze(-1)\n",
    "        y_inf = y_inf.squeeze(-1)\n",
    "        y_rev = y_rev.squeeze(-1)\n",
    "        y_ret = y_ret.squeeze(-1)\n",
    "        \n",
    "        return transcriptid, \\\n",
    "               y_car, y_inf, y_rev, y_ret, \\\n",
    "               t_car, t_inf, t_rev, t_ret\n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, \\\n",
    "        y_car, y_inf, y_rev, y_ret, \\\n",
    "        t_car, t_inf, t_rev, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_ret = self.mse_loss(y_ret, t_ret)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(loss_rev+loss_inf+loss_ret)/3\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, \\\n",
    "        y_car, y_inf, y_rev, y_ret, \\\n",
    "        t_car, t_inf, t_rev, t_ret = self.shared_step(batch)\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_ret = self.mse_loss(y_ret, t_ret)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(loss_rev+loss_inf+loss_ret)/3\n",
    "        \n",
    "        self.log('val_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "# parse arg\n",
    "parser = argparse.ArgumentParser(description='Earnings Call')\n",
    "parser.add_argument('--yqtr', type=str, required=True)\n",
    "parser.add_argument('--window_size', type=str, required=True)\n",
    "parser.add_argument('--note', type=str, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "yqtr = args.yqtr\n",
    "window_size = args.window_size\n",
    "\n",
    "# choose Model\n",
    "Model = CCMTLFrTxt\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final_addretail', # key!\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size':16,\n",
    "    'test_batch_size':16,\n",
    "    \n",
    "    'text_in_dataset': True,\n",
    "    'window_size': window_size,\n",
    "    \n",
    "    'preemb_dir': '/home/yu/OneDrive/CC/data/Embeddings/longformer',\n",
    "    'tid_cid_pair_name1': 'qa_analyst',\n",
    "    'tid_cid_pair_name2': 'qa_manager',\n",
    "    'tid_from_to_pair_name1': '4qtr',\n",
    "    'tid_from_to_pair_name2': '4qtr'\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'alpha': 0.1, # key!\n",
    "    'learning_rate': 3e-4,\n",
    "    'dropout': 0.1,\n",
    "    'd_model': 768,\n",
    "\n",
    "    'n_layers_encoder': 2,\n",
    "    'n_head_encoder': 8,\n",
    "    'max_seq_len': 1024,\n",
    "    'dff': 2048\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': args.note, \n",
    "    'log_every_n_steps': 50,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\n",
    "    'max_steps': None, \n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 25, # default: 10\n",
    "\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "print(f'Start training...{trainer_hparams[\"note\"]}')\n",
    "\n",
    "\n",
    "# train on select periods\n",
    "data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cRT with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ----------------------------\n",
    "# Specify Model and data\n",
    "# ----------------------------\n",
    "Model = CCMTLFr\n",
    "\n",
    "ckpt_name = 'MTL-14'\n",
    "ckpt_paths = [path for path in os.listdir(f'{CHECKPOINT_ARCHIVE_DIR}')\n",
    "              if path.startswith(ckpt_name+'_')]\n",
    "print(f'N checkpoint found: {len(ckpt_paths)}')\n",
    "\n",
    "# load data\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '6y', # key!\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Extract\n",
    "# ----------------------------\n",
    "def extract(model, dataloader):\n",
    "    # Extract y, x using model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        transcriptid = []\n",
    "        features = []\n",
    "        t_car = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            tid, car, f = model.forward(batch)\n",
    "            transcriptid.append(tid)\n",
    "            t_car.append(car)\n",
    "            features.append(f)\n",
    "\n",
    "        transcriptid = dt.Frame(transcriptid=torch.cat(transcriptid).numpy())\n",
    "        t_car = dt.Frame(t_car=torch.cat(t_car).numpy())\n",
    "        features = dt.Frame(torch.cat(features, dim=0).numpy())\n",
    "        features.names = [n.replace('C', 'feature') for n in features.names]\n",
    "\n",
    "        targets = dt.cbind([transcriptid, t_car, features])\n",
    "\n",
    "        t, x = dmatrices(f't_car ~ {\"+\".join(features.names)}',\n",
    "                         data=targets,\n",
    "                         return_type='dataframe')\n",
    "        \n",
    "        return transcriptid['transcriptid'].to_list()[0], t, x\n",
    "\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "yt_extractor = []\n",
    "for yqtr in tqdm(split_df.yqtr):\n",
    "    \n",
    "    if yqtr<'2012-q4':\n",
    "        continue\n",
    "        \n",
    "    # load train/test data\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    datamodule = CCDataModule(**data_hparams)\n",
    "    datamodule.setup()\n",
    "    \n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "    \n",
    "    # load model\n",
    "    ckpt_path = [path for path in ckpt_paths \n",
    "                 if path.startswith(f'{ckpt_name}_{yqtr}')]\n",
    "    assert len(ckpt_path)==1, f'Multiple or no checkpoint found for \"{ckpt_name}_{yqtr}\"'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "    \n",
    "    model = Model.load_from_checkpoint(f'{CHECKPOINT_ARCHIVE_DIR}/{ckpt_path}')\n",
    "    model.eval()\n",
    "    \n",
    "    # extract\n",
    "    import statsmodels.api as sm\n",
    "    from patsy import dmatrices\n",
    "\n",
    "    _, t_train, x_train = extract(model, train_dataloader)\n",
    "    transcriptid, t_test, x_test = extract(model, test_dataloader)\n",
    "        \n",
    "    # Fit OLS on Train\n",
    "    fitted = sm.OLS(t_train, x_train).fit()\n",
    "    \n",
    "    # Apply OLS on Test\n",
    "    y_test = fitted.predict(x_test).to_list()\n",
    "    t_test = t_test['t_car'].to_list()\n",
    "    \n",
    "    df = dt.Frame(transcriptid=transcriptid,\n",
    "                  t_car=t_test,\n",
    "                  y_car=y_test)\n",
    "    df[:, update(model_name=ckpt_name+'_extractor',\n",
    "                 window_size=data_hparams['window_size'],\n",
    "                 yqtr=yqtr)]\n",
    "    \n",
    "    yt_extractor.append(df)\n",
    "    \n",
    "yt_extractor = dt.rbind(yt_extractor)\n",
    "\n",
    "# Combine\n",
    "ld('yt_extractor', 'old_yt_extractor', force=True)\n",
    "all_yt_extractor = dt.rbind([yt_extractor, old_yt_extractor])\n",
    "\n",
    "sv('all_yt_extractor', 'yt_extractor')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cRT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ----------------------------\n",
    "# Specify Model and data\n",
    "# ----------------------------\n",
    "class cRT(CC):\n",
    "    def __init__(self, learning_rate, extractor):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.extractor = extractor\n",
    "        self.fc_1 = nn.Linear(16,1)\n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "        \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, t_car, x = self.extractor(batch)\n",
    "        y = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        return transcriptid, t_car, y\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, t_car, y = self.shared_step(batch) \n",
    "        return transcriptid, t_car, y\n",
    "        \n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, t_car, y = self.shared_step(batch)\n",
    "        return {'y':y, 't':t_car, 'transcriptid':transcriptid}\n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        y = outputs['y']\n",
    "        t = outputs['t']\n",
    "        \n",
    "        loss = self.mse_loss(y,t)\n",
    "        return {'loss':loss}\n",
    "        \n",
    "\n",
    "ckpt_name = 'MTL-11'\n",
    "ckpt_paths = [path for path in os.listdir(f'{CHECKPOINT_ARCHIVE_DIR}')\n",
    "              if path.startswith(ckpt_name+'_')]\n",
    "print(f'N checkpoint found: {len(ckpt_paths)}')\n",
    "\n",
    "# load data\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '6y', # key!\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Extract\n",
    "# ----------------------------\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        transcriptid = []\n",
    "        y_car = []\n",
    "        t_car = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            tid, t, y = model.forward(batch)\n",
    "            transcriptid.append(tid)\n",
    "            t_car.append(t)\n",
    "            y_car.append(y)\n",
    "            \n",
    "        transcriptid = dt.Frame(transcriptid=torch.cat(transcriptid).numpy())\n",
    "        t_car = dt.Frame(t_car=torch.cat(t_car).numpy())\n",
    "        y_car = dt.Frame(torch.cat(y_car, dim=0).numpy())\n",
    "\n",
    "        targets = dt.cbind([transcriptid, t_car, features])\n",
    "\n",
    "        \n",
    "        return targets\n",
    "\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "yt_extractor = []\n",
    "for yqtr in tqdm(split_df.yqtr):\n",
    "    \n",
    "    if yqtr<'2012-q4':\n",
    "        continue\n",
    "        \n",
    "    # load model\n",
    "    ckpt_path = [path for path in ckpt_paths \n",
    "                 if path.startswith(f'{ckpt_name}_{yqtr}')]\n",
    "    assert len(ckpt_path)==1, \\\n",
    "           f'Multiple or no checkpoint found for \"{ckpt_name}_{yqtr}\"'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "    \n",
    "    extractor = CCMTLFr.load_from_checkpoint(f'{CHECKPOINT_ARCHIVE_DIR}/{ckpt_path}')\n",
    "    extractor.eval()\n",
    "        \n",
    "    # load train/test data\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    datamodule = CCDataModule(**data_hparams)\n",
    "    datamodule.setup()\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "    \n",
    "    # Retrain classifier\n",
    "    classrt = cRT(learning_rate=1e-4,\n",
    "                  extractor=extractor)\n",
    "    logger = CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "    logger.experiment.log_parameters({'note':'cRT-test'})\n",
    "    trainer = pl.Trainer(gpus=[1], accelerator='dp',\n",
    "                         max_epochs=20)\n",
    "    trainer.fit(classrt, datamodule)\n",
    "    \n",
    "    # predict\n",
    "    df = predict(classrt, test_dataloader)\n",
    "        \n",
    "    df[:, update(model_name=ckpt_name+'_fcextractor',\n",
    "                 window_size=data_hparams['window_size'],\n",
    "                 yqtr=yqtr)]\n",
    "    \n",
    "    yt_extractor.append(df)\n",
    "    \n",
    "yt_extractor = dt.rbind(yt_extractor)\n",
    "\n",
    "# Combine\n",
    "# ld('yt_extractor', 'old_yt_extractor', force=True)\n",
    "# all_yt_extractor = dt.rbind([yt_extractor, old_yt_extractor])\n",
    "\n",
    "# sv('all_yt_extractor', 'yt_extractor')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
