{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /home/yu/OneDrive/CC\n",
      "DATA_DIR: /home/yu/OneDrive/CC/data\n",
      "CHECKPOINT_DIR: /home/yu/Data/CC-checkpoints\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 10.76 (GB)\n",
      "GPU 1:  0.00/ 10.76 (GB)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import comet_ml\n",
    "import copy\n",
    "import datatable as dt\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict, defaultdict\n",
    "from datatable import f, update\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from spacy.lang.en import English\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Init for script use\n",
    "with open(\"/home/yu/OneDrive/App/Settings/jupyter + R + Python/python_startup.py\", 'r') as _:\n",
    "    exec(_.read())\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('/home/yu/OneDrive/CC')\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '/home/yu/OneDrive/CC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = '/home/yu/Data/CC-checkpoints'\n",
    "CHECKPOINT_TEMP_DIR = f'{CHECKPOINT_DIR}/temp'\n",
    "\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: tensor_to_list\n",
    "def tensor_to_str(tensor, tailing='\\n'):\n",
    "    '''Given a 1d tensor, convert it to a list of string\n",
    "    \n",
    "    tailing: str. Added to the end of every element\n",
    "    '''\n",
    "    assert isinstance(tensor, torch.Tensor), 'Give me a tensor please!'\n",
    "    assert len(tensor.shape)==1, 'Must be 1D tensor!'\n",
    "    tensor = tensor.to('cpu').tolist()\n",
    "    tensor = [f'{str(x)}{tailing}' for x in tensor]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=torch.device('cpu'):\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name, force=False):\n",
    "    targets_df = feather.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "    targest_df = targets_df[targets_df.outlier_flag1==False]\n",
    "    return targets_df\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_name):\n",
    "    if 'preembeddings' not in globals():\n",
    "        \n",
    "        # find the embedding files\n",
    "        emb_paths = [path for path in os.listdir('data/Embeddings')\n",
    "                     if re.search(f'{preembedding_name}_rank', path)]\n",
    "        emb_paths.sort()\n",
    "        assert len(emb_paths)==2, \"Expect two files: rank0 and rank1\"\n",
    "\n",
    "        # load the embedding files\n",
    "        print(f'{datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}, Loading \"{emb_paths[0]}\"...')\n",
    "        emb0 = torch.load(f\"{DATA_DIR}/Embeddings/{emb_paths[0]}\")\n",
    "        print(f'{datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}, Loading \"{emb_paths[1]}\"...')\n",
    "        emb1 = torch.load(f\"{DATA_DIR}/Embeddings/{emb_paths[1]}\")\n",
    "\n",
    "        # merge two ranks into one (update emb0 with emb1)\n",
    "        for tid, cid_emb in emb1.items():\n",
    "            for cid, emb in cid_emb.items():\n",
    "                emb0[tid].update({cid:emb})\n",
    "        print('Merging completes!')\n",
    "\n",
    "        # write embedding to globals()\n",
    "        globals()['preembeddings'] = emb0\n",
    "    \n",
    "    else:\n",
    "        print(f'Pre-embedding \"{preembedding_name}\" already loaded, will not load again!')\n",
    "\n",
    "# helpers: load split_df\n",
    "def load_split_df(window_size):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    return split_df.loc[split_df.window_size==window_size]\n",
    "\n",
    "# helpers: load tid_cid_pair\n",
    "def load_tid_cid_pair(tid_cid_pair_name):\n",
    "    '''load DataFrame tid_cid_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid:[cid1, cid2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"md\", \"qa\", \"all\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_cid_pair_{tid_cid_pair_name}.feather')\n",
    "    output = {}\n",
    "    for tid, group in pair.groupby(['transcriptid']):\n",
    "        cids = group.componentid.to_list()\n",
    "        output[tid] = cids\n",
    "    return output\n",
    "    \n",
    "# helpers: load tid_cid_pair\n",
    "def load_tid_from_to_pair(tid_from_to_pair_name):\n",
    "    '''load DataFrame tid_from_to_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid_from:[tid_to1, tid_to2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"3qtr\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_from_to_pair_{tid_from_to_pair_name}.feather')\n",
    "    \n",
    "    output = {}\n",
    "    for _, (_, tid_from, tid_to) in pair.iterrows():\n",
    "        output[tid_from] = tid_to.tolist()\n",
    "    return output\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, yqtr):\n",
    "    '''\n",
    "    Given yqtr, find the corresponding ols_rmse from `performance_by_model.feather`.\n",
    "    Always compare to the same model: 'ols: car_norm ~ fr'\n",
    "    then log to Comet\n",
    "    '''\n",
    "    performance = dt.Frame(pd.read_feather('data/performance_by_yqtr.feather'))\n",
    "\n",
    "\n",
    "    ols_rmse_norm = performance[(f.model_name=='ols: car_norm ~ fr') & (f.window_size=='3y') & (f.yqtr==yqtr), f.rmse][0,0]\n",
    "    logger.experiment.log_parameter('ols_rmse_norm', ols_rmse_norm)\n",
    "    \n",
    "def log_test_start(logger, window_size, yqtr):\n",
    "    '''\n",
    "    Given window, find the corresponding star/end date of the training/test periods, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, *_ = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.window_size==window_size)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, yqtr, split_type, text_in_dataset, window_size, preembeddings, targets_df, split_df, tid_cid_pair, tid_from_to_pair):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings: dict of pre-embeddings. In the form\n",
    "              `{tid:{cid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for component level and \n",
    "              `{tid:{sid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for sentence level.\n",
    "              \n",
    "            targets_df: DataFrame of targets variables.\n",
    "            split_df: DataFrame that keeps the split of windows\n",
    "            ytqr: str. e.g., \"2008-q3\"\n",
    "            split_type: str. 'train', 'val', or 'test'\n",
    "            text_in_dataset: also output text embedding if true.\n",
    "            \n",
    "            tid_cid_pair: Dict of transcriptid and componentid/sentenceid for\n",
    "              text that will be used. In the form \n",
    "              `{tid:[cid1, cid2, ...]}` or `{tid:[sid1, sid2, ...]}`\n",
    "              Note! [cid1, cid2, ...] must be in the same order as in original \n",
    "              transcript!\n",
    "        '''\n",
    "            \n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _, yqtr = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.window_size==window_size)].iloc[0])\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # generate targets_df for train, val, test \n",
    "        if split_type=='train':\n",
    "            # print current window\n",
    "            print(f'Current window: {yqtr} ({window_size}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "            \n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].sample(frac=1, random_state=42)\n",
    "            targets_df = targets_df.iloc[:int(len(targets_df)*0.9)]\n",
    "            \n",
    "        elif split_type=='val':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].sample(frac=1, random_state=42)\n",
    "            targets_df = targets_df.iloc[int(len(targets_df)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(test_start, test_end)]\n",
    "\n",
    "        \n",
    "        # make sure targets_df only contains transcriptid that're also \n",
    "        # in preembeddings\n",
    "        targets_df = targets_df.loc[targets_df.transcriptid.isin(set(preembeddings.keys()))]\n",
    "        \n",
    "        # Assign states\n",
    "        self.targets_df = targets_df\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.split_type = split_type\n",
    "        self.tid_cid_pair = tid_cid_pair\n",
    "        self.tid_from_to_pair = tid_from_to_pair\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        targets = self.targets_df.iloc[idx]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        transcriptid = targets.transcriptid\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        # using the normalized features\n",
    "        similarity = targets.similarity_bigram_norm\n",
    "        sentiment = targets.qa_positive_sent_norm\n",
    "        sue = targets.sue_norm\n",
    "        sest = targets.sest_norm        \n",
    "        alpha = targets.alpha_norm\n",
    "        volatility = targets.volatility_norm\n",
    "        mcap = targets.mcap_norm\n",
    "        bm = targets.bm_norm\n",
    "        roa = targets.roa_norm\n",
    "        debt_asset = targets.debt_asset_norm\n",
    "        numest = targets.numest_norm\n",
    "        smedest = targets.smedest_norm\n",
    "        sstdest = targets.sstdest_norm\n",
    "        car_m1_m1 = targets.car_m1_m1_norm\n",
    "        car_m2_m2 = targets.car_m2_m2_norm\n",
    "        car_m30_m3 = targets.car_m30_m3_norm\n",
    "        volume = targets.volume_norm\n",
    "\n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = assemble_embedding(transcriptid, \n",
    "                                            self.preembeddings,\n",
    "                                            self.tid_cid_pair,\n",
    "                                            self.tid_from_to_pair)\n",
    "\n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision,\\\n",
    "                   revision_norm,  transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return torch.tensor(transcriptid,dtype=torch.int64), \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],\n",
    "                                dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "                                 sest, sue, numest, sstdest, smedest, mcap,\\\n",
    "                                 roa, bm, debt_asset, volatility, volume],\n",
    "                                dtype=torch.float32)\n",
    "      \n",
    "    \n",
    "def assemble_embedding(transcriptid, preembeddings, \n",
    "                       tid_cid_pair, tid_from_to_pair):\n",
    "    '''Assemble embeddings belonging to the same tid into one Tensor\n",
    "    \n",
    "    Method:\n",
    "        1) Given transcriptid, use it as \"transcriptid_from\" to retrieve all the \n",
    "           corresponding \"transcriptid_to\" from table \"tid_from_to_pair\"\n",
    "        2) For every transcript_to, retrieve all the corresponding cids from table\n",
    "           \"tid_cid_pair\"\n",
    "    '''\n",
    "    # find tids that we'll consider\n",
    "    tids_to = tid_from_to_pair[transcriptid]\n",
    "    \n",
    "    # for every tid, merge its components\n",
    "    output = []\n",
    "    \n",
    "    for tid_to in tids_to:\n",
    "        comps = preembeddings[tid_to]\n",
    "        emb = [comps[cid]['embedding'] \n",
    "               for cid in tid_cid_pair.get(tid_to, [])]\n",
    "        output.extend(emb)\n",
    "        \n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then define DataModule\n",
    "class CCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, yqtr, preembedding_name, targets_name,\n",
    "                 batch_size, val_batch_size, test_batch_size,\n",
    "                 text_in_dataset, window_size, tid_cid_pair_name,\n",
    "                 tid_from_to_pair_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.yqtr = yqtr\n",
    "        self.preembedding_name = preembedding_name\n",
    "        self.targets_name = targets_name\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        self.window_size = window_size\n",
    "        self.tid_cid_pair_name = tid_cid_pair_name\n",
    "        self.tid_from_to_pair_name = tid_from_to_pair_name\n",
    "        \n",
    "    # Dataset\n",
    "    def setup(self):\n",
    "        # read the preembedding, targests, and split_df\n",
    "        global preembeddings\n",
    "        \n",
    "        load_preembeddings(self.preembedding_name)\n",
    "        targets_df = load_targets(self.targets_name)\n",
    "        split_df = load_split_df(self.window_size)\n",
    "        tid_cid_pair = load_tid_cid_pair(self.tid_cid_pair_name)\n",
    "        tid_from_to_pair = load_tid_from_to_pair(self.tid_from_to_pair_name)\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.yqtr, \n",
    "                                       split_type='train',\n",
    "                                       text_in_dataset=self.text_in_dataset,\n",
    "                                       window_size=self.window_size,\n",
    "                                       preembeddings=preembeddings,\n",
    "                                       targets_df=targets_df, \n",
    "                                       split_df=split_df,\n",
    "                                       tid_cid_pair=tid_cid_pair,\n",
    "                                       tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.yqtr, split_type='val',\n",
    "                                     text_in_dataset=self.text_in_dataset,\n",
    "                                     window_size=self.window_size,\n",
    "                                     preembeddings=preembeddings,\n",
    "                                     targets_df=targets_df,\n",
    "                                     split_df=split_df,\n",
    "                                     tid_cid_pair=tid_cid_pair,\n",
    "                                     tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.yqtr, split_type='test',\n",
    "                                      text_in_dataset=self.text_in_dataset, \n",
    "                                      window_size=self.window_size,\n",
    "                                      preembeddings=preembeddings,\n",
    "                                      targets_df=targets_df,\n",
    "                                      split_df=split_df,\n",
    "                                      tid_cid_pair=tid_cid_pair,\n",
    "                                      tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=True, drop_last=False, num_workers=2,\n",
    "                          pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.val_batch_size,\n",
    "                          num_workers=2, pin_memory=True, collate_fn=collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, batch_size=4, num_workers=2, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), \\\n",
    "               torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), \\\n",
    "               torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), \\\n",
    "               embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    '''Mainly define the `*_step_end` methods\n",
    "    '''\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # def training_step_end\n",
    "    def training_step_end(self, outputs):\n",
    "        y = outputs['y_car']\n",
    "        t = outputs['t_car']\n",
    "        loss = self.mse_loss(y, t)\n",
    "        \n",
    "        return {'loss':loss}\n",
    "    \n",
    "    # def validation_step_end\n",
    "    def validation_step_end(self, outputs):\n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        '''\n",
    "        outputs: a list. len(outputs) == num_steps.\n",
    "            e.g., outputs = [{'val_loss': 3}, {'val_loss': 4}]\n",
    "        '''\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('val_rmse', rmse, on_step=False)\n",
    "        \n",
    "    # test step\n",
    "    def test_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        \n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car, 'transcriptid':transcriptid}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \n",
    "        transcriptid = torch.cat([x['transcriptid'] for x in outputs])\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('test_rmse', rmse, on_step=False)\n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_car', rmse_car, on_step=False)\n",
    "            \n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_inflow', rmse_inflow, on_step=False)\n",
    "\n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_revision', rmse_revision, on_step=False)\n",
    "        \n",
    "        # save & log `y_car`\n",
    "        y_car_filename = f'{DATA_DIR}/y_car.feather'\n",
    "        \n",
    "        df = pd.DataFrame({'transcriptid':transcriptid.to('cpu'), 'y_car':y_car.to('cpu')})\n",
    "        feather.write_feather(df, y_car_filename)\n",
    "            \n",
    "        self.logger.experiment.log_asset(y_car_filename)\n",
    "            \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams):\n",
    "\n",
    "    # ----------------------\n",
    "    # `hparams` sanity check\n",
    "    # ----------------------\n",
    "    \n",
    "    # check: batch_size//len(gpus)==0\n",
    "    assert data_hparams['batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['val_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['val_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['test_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`test_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['test_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Initialize model and trainer\n",
    "    # ----------------------------\n",
    "    \n",
    "    # init model\n",
    "    model = Model(**model_hparams)\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{trainer_hparams['note']}_{data_hparams['yqtr']}_\".replace('*', '')\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        monitor='val_rmse',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=trainer_hparams['save_top_k'],\n",
    "        period=trainer_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_rmse',\n",
    "        min_delta=0,\n",
    "        patience=trainer_hparams['early_stop_patience'],\n",
    "        verbose=True,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=trainer_hparams['gpus'], \n",
    "                         precision=trainer_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         callbacks=[early_stop_callback],\n",
    "                         overfit_batches=trainer_hparams['overfit_batches'], \n",
    "                         log_every_n_steps=trainer_hparams['log_every_n_steps'],\n",
    "                         val_check_interval=trainer_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=5, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=trainer_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=trainer_hparams['min_epochs'],\n",
    "                         max_epochs=trainer_hparams['max_epochs'], \n",
    "                         max_steps=trainer_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # add n_model_params\n",
    "    trainer_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(data_hparams)\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(trainer_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, data_hparams['yqtr'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, data_hparams['window_size'], data_hparams['yqtr'])\n",
    "    \n",
    "    # If run on ASU, upload code explicitly\n",
    "    if trainer_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # fit and test\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        # create datamodule\n",
    "        datamodule = CCDataModule(**data_hparams)\n",
    "        datamodule.setup()\n",
    "\n",
    "        # train the model\n",
    "        trainer.fit(model, datamodule)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true",
    "lines_to_next_cell": 1
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, learning_rate, dropout, model_type='MLP'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(17, 32)\n",
    "        self.fc_2 = nn.Linear(32, 1)\n",
    "        #self.fc_3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, car, car_norm, manual_txt, fin_ratios = batch\n",
    "        x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "\n",
    "        x_car = F.relu(self.fc_1(x))\n",
    "        y_car = self.fc_2(x_car) # (N, 1)    \n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# choose Model\\nModel = CCMLP\\n\\n# data hparams\\ndata_hparams = {\\n    \\'preembedding_name\\': \\'all_sbert_roberta_nlistsb_encoded\\', # key!\\n    \\'targets_name\\': \\'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_norm_outlier\\', # key!\\n\\n    \\'batch_size\\': 128,\\n    \\'val_batch_size\\':64,\\n    \\'test_batch_size\\':32,\\n    \\n    \\'text_in_dataset\\': False,\\n    \\'window_size\\': \\'5y\\', # key!\\n}\\n\\n# model hparams\\nmodel_hparams = {\\n    \\'learning_rate\\': 1e-3,\\n    \\'dropout\\': 0.5,\\n}\\n\\n# train hparams\\ntrainer_hparams = {\\n    # random seed\\n    \\'seed\\': 42,    # key\\n    \\n    # gpus\\n    \\'gpus\\': [0,1], # key\\n\\n    # checkpoint & log\\n    \\n    # last: MLP-24\\n    \\'machine\\': \\'yu-workstation\\', # key!\\n    \\'note\\': f\"MLP-25,(car~fr+mtxt),hidden=32,hiddenLayer=1,fc_dropout=no,NormCAR=yes,bsz={data_hparams[\\'batch_size\\']},log(mcap)=yes,lr={model_hparams[\\'learning_rate\\']:.1e}\", # key!\\n    \\'log_every_n_steps\\': 10,\\n    \\'save_top_k\\': 1,\\n    \\'val_check_interval\\': 1.0,\\n\\n    # data size\\n    \\'precision\\': 32, # key!\\n    \\'overfit_batches\\': 0.0,\\n    \\'min_epochs\\': 10, # default: 10\\n    \\'max_epochs\\': 20, \\n    \\'max_steps\\': None,\\n    \\'accumulate_grad_batches\\': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    \\'early_stop_patience\\': 3,\\n\\n    # Caution:\\n    # In pervious versions, if you check validatoin multiple times within a epoch,\\n    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \\n    # you check validation multiples times within an epoch, you still need to set\\n    # `checkpoint_period=1`.\\n    \\'checkpoint_period\\': 1}\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt()\\n\\n# load split_df\\nsplit_df = load_split_df(data_hparams[\\'window_size\\'])\\n    \\n# loop over windows\\nnp.random.seed(trainer_hparams[\\'seed\\'])\\ntorch.manual_seed(trainer_hparams[\\'seed\\'])\\n\\nfor yqtr in split_df.yqtr:\\n    \\n    # update current period\\n    data_hparams.update({\\'yqtr\\': yqtr})\\n    \\n    # train on select periods\\n    # if data_hparams[\\'yqtr\\']==\\'2014-q1\\':\\n    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_norm_outlier', # key!\n",
    "\n",
    "    'batch_size': 128,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':32,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '5y', # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'dropout': 0.5,\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    \n",
    "    # last: MLP-24\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MLP-25,(car~fr+mtxt),hidden=32,hiddenLayer=1,fc_dropout=no,NormCAR=yes,bsz={data_hparams['batch_size']},log(mcap)=yes,lr={model_hparams['learning_rate']:.1e}\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 20, \n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 3,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "for yqtr in split_df.yqtr:\n",
    "    \n",
    "    # update current period\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "    \n",
    "    # train on select periods\n",
    "    # if data_hparams['yqtr']=='2014-q1':\n",
    "    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# ## Model\\n\\n# CCGRU\\nclass CCGRU(CC):\\n    def __init__(self, hparams):\\n        super().__init__(hparams)\\n        \\n        self.hparams = hparams\\n        \\n        # set model types\\n        self.task_type = 'single'\\n        self.feature_type = 'univariate'\\n        self.model_type = 'gru'\\n        self.attn_type = 'dotprod'\\n        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \\n        \\n        # layers\\n        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\\n                                 dropout=0.1, bidirectional=True)\\n        self.dropout_expert = nn.Dropout(hparams.dropout)\\n        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\\n\\n    # forward\\n    def forward(self, inp, valid_seq_len):\\n        # Note: inp is [N, S, E] and **already** been packed\\n        self.gru_expert.flatten_parameters()\\n        \\n        # if S is longer than `max_seq_len`, cut\\n        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\\n        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\\n        \\n        # RNN layers\\n        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\\n        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\\n        \\n        # final FC layers\\n        y_car = self.linear_car(x_expert) # (N, E)\\n        \\n        return y_car\\n    \\n    # train step\\n    def training_step(self, batch, idx):\\n        \\n        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,        sest, sue, numest, sstdest, smedest,         mcap, roa, bm, debt_asset, volatility = batch\\n        \\n        # get valid seq_len\\n        valid_seq_len = torch.sum(~mask, -1)\\n        \\n        # forward\\n        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\\n\\n        # compute loss\\n        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\\n        \\n        # logging\\n        return {'loss': loss_car, 'log': {'trainer_loss': loss_car}}\\n            \\n    # validation step\\n    def validation_step(self, batch, idx):\\n        \\n        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,        sest, sue, numest, sstdest, smedest,         mcap, roa, bm, debt_asset, volatility = batch\\n        \\n        # get valid seq_len\\n        valid_seq_len = torch.sum(~mask, -1)\\n        \\n        # forward\\n        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\\n\\n        # compute loss\\n        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\\n        \\n        # logging\\n        return {'val_loss': loss_car}        \\n    \\n    # test step\\n    def test_step(self, batch, idx):\\n        \\n        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,        sest, sue, numest, sstdest, smedest,         mcap, roa, bm, debt_asset, volatility = batch\\n        \\n        # get valid seq_len\\n        valid_seq_len = torch.sum(~mask, -1)\\n        \\n        # forward\\n        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\\n\\n        # compute loss\\n        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\\n        \\n        # logging\\n        return {'test_loss': loss_car}  \\n\\n\\n# + [markdown] toc-hr-collapsed=true toc-nb-collapsed=true\\n# ## run\\n\\n# +\\n# loop over 24 windows\\nload_split_df()\\nload_targets()\\n\\n# model hparams\\nmodel_hparams = {\\n    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key\\n    'batch_size': 8, # key\\n    'val_batch_size': 1, # key\\n    \\n    'max_seq_len': 1024, \\n    'learning_rate': 3e-4,\\n    'task_weight': 1,\\n\\n    'n_layers_encoder': 6,\\n    'n_head_encoder': 8, # optional\\n    'd_model': 1024,\\n    'rnn_hidden_size': 64,\\n    'final_tdim': 1024, # optional\\n    'dff': 2048,\\n    'attn_dropout': 0.1,\\n    'dropout': 0.5,\\n    'n_head_decoder': 8} # optional\\n\\n# train hparams\\ntrainer_hparams = {\\n    # checkpoint & log\\n    'note': 'temp',\\n    'checkpoint_path': 'D:\\\\Checkpoints\\\\earnings-call',\\n    'row_log_interval': 1,\\n    'save_top_k': 1,\\n    'val_check_interval': 0.25,\\n\\n    # data size\\n    'overfit_pct': 1,\\n    'min_epochs': 0,\\n    'max_epochs': 1,\\n    'max_steps': None,\\n    'accumulate_grad_batches': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    'early_stop_patience': 5,\\n\\n    # Caution:\\n    # If set to 1, then save ckpt every 1 epoch\\n    # If set to 0, then save ckpt on every val!!! (if val improves)\\n    'checkpoint_period': 0}\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt(trainer_hparams['checkpoint_path'])\\n    \\n# loop over 24!\\nfor window_i in range(len(split_df)):\\n    # load preembeddings\\n    load_preembeddings(model_hparams['preembedding_name'])\\n\\n    # train one window\\n    trainer_one(CCGRU, window_i, model_hparams, trainer_hparams)\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# ## Model\n",
    "\n",
    "# CCGRU\n",
    "class CCGRU(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # set model types\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'univariate'\n",
    "        self.model_type = 'gru'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # layers\n",
    "        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\n",
    "                                 dropout=0.1, bidirectional=True)\n",
    "        self.dropout_expert = nn.Dropout(hparams.dropout)\n",
    "        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, valid_seq_len):\n",
    "        # Note: inp is [N, S, E] and **already** been packed\n",
    "        self.gru_expert.flatten_parameters()\n",
    "        \n",
    "        # if S is longer than `max_seq_len`, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\n",
    "        \n",
    "        # RNN layers\n",
    "        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\n",
    "        \n",
    "        # final FC layers\n",
    "        y_car = self.linear_car(x_expert) # (N, E)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'trainer_loss': loss_car}}\n",
    "            \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}        \n",
    "    \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  \n",
    "\n",
    "\n",
    "# + [markdown] toc-hr-collapsed=true toc-nb-collapsed=true\n",
    "# ## run\n",
    "\n",
    "# +\n",
    "# loop over 24 windows\n",
    "load_split_df()\n",
    "load_targets()\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key\n",
    "    'batch_size': 8, # key\n",
    "    'val_batch_size': 1, # key\n",
    "    \n",
    "    'max_seq_len': 1024, \n",
    "    'learning_rate': 3e-4,\n",
    "    'task_weight': 1,\n",
    "\n",
    "    'n_layers_encoder': 6,\n",
    "    'n_head_encoder': 8, # optional\n",
    "    'd_model': 1024,\n",
    "    'rnn_hidden_size': 64,\n",
    "    'final_tdim': 1024, # optional\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} # optional\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # checkpoint & log\n",
    "    'note': 'temp',\n",
    "    'checkpoint_path': 'D:\\Checkpoints\\earnings-call',\n",
    "    'row_log_interval': 1,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.25,\n",
    "\n",
    "    # data size\n",
    "    'overfit_pct': 1,\n",
    "    'min_epochs': 0,\n",
    "    'max_epochs': 1,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 5,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt(trainer_hparams['checkpoint_path'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df)):\n",
    "    # load preembeddings\n",
    "    load_preembeddings(model_hparams['preembedding_name'])\n",
    "\n",
    "    # train one window\n",
    "    trainer_one(CCGRU, window_i, model_hparams, trainer_hparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt\n",
    "class CCTransformerSTLTxt(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, n_layers_encoder, dff, max_seq_len, model_type='STL', dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for CAR\n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.fc_2 = nn.Linear(32, 1)\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # forward\n",
    "    def shared_step(self, batch):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # decode with avgpool\n",
    "        x_expert = x_expert.mean(1) # (N, E)\n",
    "        \n",
    "        # decode with maxpool\n",
    "        # x_expert_maxpool = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        # concat\n",
    "        # x_expert = torch.cat([x_expert_avgpool, x_expert_maxpool], dim=-1) # (N, 2E)\n",
    "\n",
    "        # final FC\n",
    "        y_car = self.fc_1(x_expert) # (N, 1)\n",
    "        # y_car = self.fc_2(y_car)\n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxtFr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt + fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, \n",
    "                 n_layers_encoder, dff, max_seq_len, model_type='STL', n_finratios=15, dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers \n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        # txt_mixer_layer = TxtMixerLayer(self.hparams.d_model)\n",
    "        # self.txt_mixer = FeatureMixer(txt_mixer_layer, self.hparams.n_layers_txtmixer)\n",
    "        \n",
    "        # fr_mixer_layers = FrMixerLayer(self.n_covariate)\n",
    "        # self.fr_mixer = FeatureMixer(fr_mixer_layers, self.hparams.n_layers_frmixer)\n",
    "        \n",
    "        # final prediction layer\n",
    "        # final_fc_mixer_layer = FeatureMixerLayer(self.hparams.d_model+self.n_covariate)\n",
    "        # self.final_fc_mixer_layer = FeatureMixer(final_fc_mixer_layer, self.hparams.n_layers_finalfc)\n",
    "        # self.fc_batchnorm = nn.BatchNorm1d(self.hparams.d_model+self.n_covariate)\n",
    "        self.final_fc = nn.Linear(self.hparams.d_model+self.hparams.n_finratios, 1)\n",
    "        \n",
    "        # self.txt_fc_1 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.txt_fc_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        # self.txt_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_3 = nn.Dropout(self.hparams.dropout) \n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        x_expert = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        \n",
    "        # project text embedding to a lower dimension\n",
    "        # x_expert = self.txt_dropout_1(F.relu(self.txt_fc_1(x_expert)))\n",
    "        # x_expert = F.relu(self.txt_fc_2(x_expert))\n",
    "        \n",
    "        # x_expert = self.txt_mixer(x_expert)\n",
    "        \n",
    "        # Mix fin_ratios\n",
    "        # fin_ratios = self.batch_norm(fin_ratios)\n",
    "        # x_fr = self.fr_mixer(fin_ratios)\n",
    "        \n",
    "        # concate `x_final` with `fin_ratios`\n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_finratios)\n",
    "        \n",
    "        # final FC\n",
    "        # x_final = self.fc_dropout_1(F.relu(self.fc_1(x_expert))) # (N, E+X)\n",
    "        # x_car = self.final_fc_mixer_layer(x_final) # (N, E+X)\n",
    "        y_car = self.final_fc(x_final)\n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1de572e10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory /home/yu/Data/CC-checkpoints exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "CometLogger will be initialized in online mode\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/1869bef18f29424d815c1cd68907e29f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-embedding \"longformer\" already loaded, will not load again!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder_pos    | PositionalEncoding | 0     \n",
      "1 | encoder_expert | TransformerEncoder | 22.1 M\n",
      "2 | fc_1           | Linear             | 769   \n",
      "------------------------------------------------------\n",
      "22.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2017-q4 (all) \n",
      "(train: 2008-01-01 to 2017-12-31) (test: 2018-01-01 to 2018-12-31)\n",
      "N train = 17931\n",
      "N val = 1993\n",
      "N train+val = 19924\n",
      "N test = 1834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce59f4fedc39415f8ba8ffb3e97fe9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b593ab4a7f943dfa10a2b73e43ee6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb1ddd6edc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/yu/Software/Anaconda/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Epoch 0, global step 168: val_rmse reached 1.13287 (best 1.13287), saving model to \"/home/yu/Data/CC-checkpoints/STL-test_2017-q4_-epoch=0-step=168.ckpt\" as top 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-424570d6a5e6>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_checkpoint_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mcheck_checkpoint_callback\u001b[0;34m(self, should_save, is_last)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_top_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_top_k_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_top_k_checkpoints\u001b[0;34m(self, trainer, pl_module, metrics)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_monitor_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_best_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_update_best_and_save\u001b[0;34m(self, current, epoch, step, trainer, pl_module, ckpt_name_metrics)\u001b[0m\n\u001b[1;32m    644\u001b[0m             )\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, filepath, trainer, pl_module)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 \u001b[0matomic_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36matomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Anaconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-3915b702f642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# train on select periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myqtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-424570d6a5e6>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mrefresh_cuda_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-0d259d08d536>\u001b[0m in \u001b[0;36mrefresh_cuda_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Run a full garbage collect first so any dangling tensors are released\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Then move all tensors to the CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# choose Model\n",
    "Model = CCTransformerSTLTxt\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    # inputs\n",
    "    'preembedding_name': 'longformer', \n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_norm_outlier', \n",
    "    'tid_cid_pair_name': 'qa', \n",
    "    'tid_from_to_pair_name': '6qtr',\n",
    "    \n",
    "    # batch size\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size':8,\n",
    "    'test_batch_size':8,\n",
    "    \n",
    "    # window_size\n",
    "    'text_in_dataset': True,\n",
    "    'window_size': 'all', # key!\n",
    "}\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':3e-4, # key!\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 768,\n",
    "    'dff': 2048, # default: 2048\n",
    "    'attn_dropout': 0.1,\n",
    "    # 'dropout': 0.5\n",
    "} \n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # last: STL-50\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"STL-test\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2, # key! (Eg: 0.25 - check 4 times in a epoch)\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3, # default: 3\n",
    "    'max_epochs': 20, # default: 20\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "# load tid_cid_pair\n",
    "# loop over windows!\n",
    "for yqtr in split_df.yqtr:\n",
    "    np.random.seed(trainer_hparams['seed'])\n",
    "    torch.manual_seed(trainer_hparams['seed'])\n",
    "    \n",
    "    # Enforce yqtr>='2012-q4' (the earliest yqtr in window_size=='3y')\n",
    "    # if yqtr == 'non-roll-01':\n",
    "\n",
    "    # update current period\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    # train on select periods\n",
    "    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MTL, hardshare) car + inf/rev ~ txt + fr\n",
    "class CCTransformerMTLHard(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # check task weights sum to one\n",
    "        assert self.hparams.car_weight+self.hparams.inflow_weight+self.hparams.revision_weight==1, 'car_weight + inflow_weight + revision_weight != 1'\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = f'{self.hparams.car_weight:.2f}*car+{self.hparams.inflow_weight:.2f}*inf+{self.hparams.revision_weight:.2f}*rev' # key!\n",
    "        self.feature_type = 'txt'    # key!\n",
    "        self.emb_share = 'hard'      # key!\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_car_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.fc_inflow_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.fc_revision_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # aggregate with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # final FC\n",
    "        y_car = self.fc_car_1(x_expert) # (N, 1)\n",
    "        # y_inflow = self.fc_inflow_1(x_expert) # (N,1)\n",
    "        y_revision = self.fc_revision_1(x_expert)\n",
    "        \n",
    "        # log hist: x_expert\n",
    "        if self.global_step%50==True:\n",
    "            self.logger.experiment.log_histogram_3d(x_expert.detach().cpu().numpy(), name='feature:x_expert', step=self.global_step)\n",
    "            # self.logger.experiment.log_histogram_3d(x_car.detach().cpu().numpy(), name='feature:x_car', step=self.global_step)\n",
    "            # self.logger.experiment.log_histogram_3d(x_inflow.detach().cpu().numpy(), name='feature:x_inflow', step=self.global_step)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_revision\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # decide if log activation histogram\n",
    "        # log_histogram = True if idx%50==0 else False\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'trainer_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        # return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_inflow': loss_inflow}\n",
    "        return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_revision': loss_revision}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        # return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_inflow': loss_inflow}  \n",
    "        return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_revision': loss_revision}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# (MTL, softshare) x*car + (1-x)*inf ~ txt + fr\n",
    "class CCTransformerMTLSoft(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car+inf'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.emb_share = 'hard'\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(self.hparams.d_model, self.hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_4 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_5 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        self.linear_inflow = nn.Linear(self.hparams.final_tdim, 1)\n",
    "        # self.linear_revision = nn.Linear(hparam.final_tdim, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.final_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_3 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(self.hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # multiply with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # mix with covariate\n",
    "        x_expert = self.final_dropout_1(F.relu(self.linear_car_1(x_expert))) # (N, E)\n",
    "        x_expert = F.relu(self.linear_car_2(x_expert)) # (N, final_tdim)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratio = self.batch_norm(fin_ratios)\n",
    "        \n",
    "        x_car = torch.cat([x_expert, fin_ratios], dim=-1) # (N, X + final_tdim) where X is the number of covariate (n_covariate)\n",
    "\n",
    "            \n",
    "        # final FC\n",
    "        y_inflow = self.linear_inflow(x_expert)\n",
    "        # y_revision = self.linear_revision(x_expert)\n",
    "        \n",
    "        x_car = self.final_dropout_2(F.relu(self.linear_car_3(x_car))) # (N, X + final_tdim)\n",
    "        y_car = self.linear_car_5(x_car) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_inflow\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        \n",
    "        assert self.hparams.car_weight+self.hparams.inflow_weight==1, 'car_weight + inflow_weight != 1'\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.inflow_weight*loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'trainer_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_inflow': loss_inflow}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_inflow': loss_inflow} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "endofcell": "--"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# choose Model\n",
    "Model = CCTransformerSTLTxtFr\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'seed': 42, # key!\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_sentiment_text_norm_wsrz', # key!\n",
    "    'window_size': '3y',  # key!\n",
    "    'gpus': [0,1],\n",
    "    \n",
    "    # task weight\n",
    "    'car_weight': 1,      # Key!\n",
    "    'inflow_weight': 0,   # key!\n",
    "    'revision_weight': 0, # key!\n",
    "    \n",
    "    'batch_size': 28,     # key!\n",
    "    'val_batch_size': 28,\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':1e-4, # key!\n",
    "    'task_weight': 1,\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_layers_txtmixer': 2, # key!\n",
    "    'n_layers_frmixer': 2,  # key!\n",
    "    'n_layers_finalfc': 2,  # key!\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 1024,\n",
    "    'final_tdim': 1024, # key!\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} \n",
    "\n",
    "trainer_hparams = {\n",
    "    # log\n",
    "    'machine': 'yu-workstation',  # key!\n",
    "    'note': f\"STL-37,(car~txt+fr 3y BatchNormFr BatchNormTxt),txtMixer=2({model_hparams['final_tdim']}),fcMixer=2,standCAR=yes,standFr=no,bsz={model_hparams['batch_size']},seed={model_hparams['seed']},log(mcap)=yes,lr={model_hparams['learning_rate']:.2g}\", # key!\n",
    "    'row_log_interval': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32,\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3,\n",
    "    'max_epochs': 20,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['window_size'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_name'])\n",
    "    \n",
    "# loop over 24!\n",
    "np.random.seed(model_hparams['seed'])\n",
    "torch.manual_seed(model_hparams['seed'])\n",
    "\n",
    "for window_i in range(len(split_df)):\n",
    "\n",
    "    # train one window\n",
    "    trainer_one(Model, window_i, model_hparams, trainer_hparams)\n",
    "'''\n",
    "# -"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
