{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: .\n",
      "DATA_DIR: ./data\n",
      "CHECKPOINT_DIR: d:/checkpoints/earnings-call\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 11.00 (GB)\n",
      "GPU 1:  0.00/ 11.00 (GB)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import comet_ml\n",
    "import copy\n",
    "import datatable as dt\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from datatable import f, update\n",
    "from spacy.lang.en import English\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = 'd:/checkpoints/earnings-call'\n",
    "CHECKPOINT_TEMP_DIR = f'{ROOT_DIR}/checkpoint/earnings-call/temp'\n",
    "\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: tensor_to_list\n",
    "def tensor_to_str(tensor, tailing='\\n'):\n",
    "    '''Given a 1d tensor, convert it to a list of string\n",
    "    \n",
    "    tailing: str. Added to the end of every element\n",
    "    '''\n",
    "    assert isinstance(tensor, torch.Tensor), 'Give me a tensor please!'\n",
    "    assert len(tensor.shape)==1, 'Must be 1D tensor!'\n",
    "    tensor = tensor.to('cpu').tolist()\n",
    "    tensor = [f'{str(x)}{tailing}' for x in tensor]\n",
    "    return tensor\n",
    "\n",
    "# helper: str_to_int\n",
    "def str_to_int(string):\n",
    "    '''Given a string, convert it into int\n",
    "    '''\n",
    "    str_as_bytes = string.encode('utf-8')\n",
    "    return int.from_bytes(str_as_bytes, 'little')\n",
    "\n",
    "# helper: int_to_str\n",
    "def int_to_str(integer):\n",
    "    '''Given a int, convert it into str\n",
    "    '''\n",
    "    int_as_bytes = integer.to_bytes((integer.bit_length() + 7) // 8, 'little')\n",
    "    return int_as_bytes.decode('utf-8')\n",
    "\n",
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=torch.device('cpu'):\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name, force=False):\n",
    "    targets_df = feather.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "    return targets_df\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_name):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings: {preembedding_name}...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_name}.pt\")\n",
    "        print(f'Loading preembeddings finished. @{Now()}')\n",
    "        \n",
    "\n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    return split_df.loc[split_df.roll_type==roll_type]\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, yqtr):\n",
    "    '''\n",
    "    Given yqtr, find the corresponding ols_rmse from `performance_by_model.feather`.\n",
    "    Always compare to the same model: 'ols: car_norm ~ fr'\n",
    "    then log to Comet\n",
    "    '''\n",
    "    performance = dt.Frame(pd.read_feather('data/performance_by_yqtr.feather'))\n",
    "\n",
    "    ols_rmse_norm = performance[(f.model_name=='ols: car_norm ~ fr') & (f.roll_type=='3y') & (f.yqtr==yqtr), f.rmse][0,0]\n",
    "    logger.experiment.log_parameter('ols_rmse_norm', ols_rmse_norm)\n",
    "    \n",
    "def log_test_start(logger, roll_type, yqtr):\n",
    "    '''\n",
    "    Given window, find the corresponding star/end date of the training/test periods, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, _, _, _ = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, yqtr, split_type, text_in_dataset, roll_type, preembeddings, targets_df, split_df, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            ytqr: str. e.g., \"2008-q3\"\n",
    "            split_type: str. 'train', 'val', or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `yqtr` and `split_type`\n",
    "        '''\n",
    "\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # Given the stage (train/val/test), find the valid `transcriptids` from `targets_df`\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        \n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _, yqtr, is_test = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            # print current window\n",
    "            print(f'Current window: {yqtr} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            # print(f'Dateset -> N train: {len(transcriptids)}')\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "            # print(f'Dataset -> N val: {len(transcriptids)}')\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "            # print(f'Dataset -> N test: {len(transcriptids)}')\n",
    "\n",
    "        self.valid_preemb_keys = list(set(transcriptids).intersection(set(preembeddings.keys())))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = list(self.valid_preemb_keys.intersection(set(valid_transcriptids)))\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.valid_preemb_keys[idx]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        transcriptid = targets.transcriptid\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        # using the normalized features\n",
    "        similarity = targets.similarity_bigram_norm\n",
    "        sentiment = targets.qa_positive_sent_norm\n",
    "        sue = targets.sue_norm\n",
    "        sest = targets.sest_norm        \n",
    "        alpha = targets.alpha_norm\n",
    "        volatility = targets.volatility_norm\n",
    "        mcap = targets.mcap_norm\n",
    "        bm = targets.bm_norm\n",
    "        roa = targets.roa_norm\n",
    "        debt_asset = targets.debt_asset_norm\n",
    "        numest = targets.numest_norm\n",
    "        smedest = targets.smedest_norm\n",
    "        sstdest = targets.sstdest_norm\n",
    "        car_m1_m1 = targets.car_m1_m1_norm\n",
    "        car_m2_m2 = targets.car_m2_m2_norm\n",
    "        car_m30_m3 = targets.car_m30_m3_norm\n",
    "        volume = targets.volume_norm\n",
    "\n",
    "        # using the unnormalized features\n",
    "#         similarity = targets.similarity_bigram\n",
    "#         sentiment = targets.qa_positive_sent\n",
    "#         sue = targets.sue\n",
    "#         sest = targets.sest        \n",
    "#         alpha = targets.alpha\n",
    "#         volatility = targets.volatility\n",
    "#         mcap = targets.mcap\n",
    "#         bm = targets.bm\n",
    "#         roa = targets.roa\n",
    "#         debt_asset = targets.debt_asset\n",
    "#         numest = targets.numest\n",
    "#         smedest = targets.smedest\n",
    "#         sstdest = targets.sstdest\n",
    "#         car_m1_m1 = targets.car_m1_m1\n",
    "#         car_m2_m2 = targets.car_m2_m2\n",
    "#         car_m30_m3 = targets.car_m30_m3\n",
    "#         volume = targets.volume\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "\n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return torch.tensor(transcriptid,dtype=torch.int64), \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then define DataModule\n",
    "class CCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, yqtr, preembedding_name, targets_name,\n",
    "                 batch_size, val_batch_size, test_batch_size, text_in_dataset, roll_type):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.yqtr = yqtr\n",
    "        self.preembedding_name = preembedding_name\n",
    "        self.targets_name = targets_name\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        self.roll_type = roll_type\n",
    "        \n",
    "    # Dataset\n",
    "    def setup(self):\n",
    "        # read the preembedding, targests, and split_df\n",
    "        global preembeddings\n",
    "        \n",
    "        load_preembeddings(self.preembedding_name)\n",
    "        targets_df = load_targets(self.targets_name)\n",
    "        split_df = load_split_df(self.roll_type)\n",
    "        \n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.yqtr, split_type='train', text_in_dataset=self.text_in_dataset,\n",
    "                                       roll_type=self.roll_type,\n",
    "                                       preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.yqtr, split_type='val', text_in_dataset=self.text_in_dataset,\n",
    "                                     roll_type=self.roll_type,\n",
    "                                     preembeddings=self.preembeddings,\n",
    "                                     targets_df=self.targets_df, split_df=self.split_df)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.yqtr, split_type='test', text_in_dataset=self.text_in_dataset, \n",
    "                                      roll_type=self.roll_type,\n",
    "                                      preembeddings=self.preembeddings,\n",
    "                                      targets_df=self.targets_df, split_df=self.split_df)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=True, drop_last=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.val_batch_size, num_workers=0, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, batch_size=4, num_workers=0, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Model: FeatureMixer\n",
    "class FeatureMixerLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        '''\n",
    "        d_model: the dimension in the FC layers. For text, usually be 1024; for fin-ratios, should be 15\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.size = [bsz, d_model]\n",
    "        '''\n",
    "        x = self.dropout(self.batch_norm(x))\n",
    "        y = F.relu(self.fc(x))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class FrMixerLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        '''\n",
    "        d_model: the dimension in the FC layers. For text, usually be 1024; for fin-ratios, should be 15\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.size = [bsz, d_model]\n",
    "        '''\n",
    "        x = self.dropout(self.batch_norm(x))\n",
    "        y = F.relu(self.fc(x))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class FeatureMixer(nn.Module):\n",
    "    def __init__(self, featuremixer_layer, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([copy.deepcopy(featuremixer_layer) for i in range(n_layers)])\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "            \n",
    "        return output\n",
    "\n",
    "        \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    '''Mainly define the `*_step_end` methods\n",
    "    '''\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # def training_step_end\n",
    "    def training_step_end(self, outputs):\n",
    "        y = outputs['y_car']\n",
    "        t = outputs['t_car']\n",
    "        loss = self.mse_loss(y, t)\n",
    "        \n",
    "        return {'loss':loss}\n",
    "    \n",
    "    # def validation_step_end\n",
    "    def validation_step_end(self, outputs):\n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        '''\n",
    "        outputs: a list. len(outputs) == num_steps.\n",
    "            e.g., outputs = [{'val_loss': 3}, {'val_loss': 4}]\n",
    "        '''\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('val_rmse', rmse, on_step=False)\n",
    "        \n",
    "    # test step\n",
    "    def test_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        \n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car, 'transcriptid':transcriptid}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \n",
    "        transcriptid = torch.cat([x['transcriptid'] for x in outputs])\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('test_rmse', rmse, on_step=False)\n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_car', rmse_car, on_step=False)\n",
    "            \n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_inflow', rmse_inflow, on_step=False)\n",
    "\n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_revision', rmse_revision, on_step=False)\n",
    "        \n",
    "        # save & log `y_car`\n",
    "        y_car_filename = f'{DATA_DIR}/y_car.feather'\n",
    "        \n",
    "        df = pd.DataFrame({'transcriptid':transcriptid.to('cpu'), 'y_car':y_car.to('cpu')})\n",
    "        feather.write_feather(df, y_car_filename)\n",
    "            \n",
    "        self.logger.experiment.log_asset(y_car_filename)\n",
    "            \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams):\n",
    "\n",
    "    # ----------------------\n",
    "    # `hparams` sanity check\n",
    "    # ----------------------\n",
    "    \n",
    "    # check: batch_size//len(gpus)==0\n",
    "    assert data_hparams['batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['val_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['val_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['test_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`test_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['test_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "        \n",
    "    # init model\n",
    "    model = Model(**model_hparams)\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{trainer_hparams['note']}_{data_hparams['yqtr']}_\".replace('*', '')\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        monitor='val_rmse',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=trainer_hparams['save_top_k'],\n",
    "        period=trainer_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_rmse',\n",
    "        min_delta=0,\n",
    "        patience=trainer_hparams['early_stop_patience'],\n",
    "        verbose=True,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=trainer_hparams['gpus'], \n",
    "                         precision=trainer_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         callbacks=[early_stop_callback],\n",
    "                         overfit_batches=trainer_hparams['overfit_batches'], \n",
    "                         log_every_n_steps=trainer_hparams['log_every_n_steps'],\n",
    "                         val_check_interval=trainer_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=5, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=trainer_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=trainer_hparams['min_epochs'],\n",
    "                         max_epochs=trainer_hparams['max_epochs'], \n",
    "                         max_steps=trainer_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # add n_model_params\n",
    "    trainer_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(data_hparams)\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(trainer_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, data_hparams['yqtr'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, data_hparams['roll_type'], data_hparams['yqtr'])\n",
    "    \n",
    "    # If run on ASU, upload code explicitly\n",
    "    if trainer_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # create datamodule\n",
    "        datamodule = CCDataModule(**data_hparams)\n",
    "        datamodule.setup()\n",
    "\n",
    "        # train the model\n",
    "        trainer.fit(model, datamodule)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, learning_rate, dropout, model_type='MLP'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(17, 32)\n",
    "        self.fc_2 = nn.Linear(32, 1)\n",
    "        #self.fc_3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, car, car_norm, manual_txt, fin_ratios = batch\n",
    "        x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "\n",
    "        x_car = F.relu(self.fc_1(x))\n",
    "        y_car = self.fc_2(x_car) # (N, 1)    \n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_norm_outlier', # key!\n",
    "\n",
    "    'batch_size': 128,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':32,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'roll_type': '5y', # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'dropout': 0.5,\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    \n",
    "    # last: MLP-24\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MLP-24,(car~fr+mtxt),hidden=32,hiddenLayer=1,fc_dropout=no,NormCAR=yes,bsz={data_hparams['batch_size']},log(mcap)=yes,lr={model_hparams['learning_rate']:.1e}\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 20, \n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 3,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['roll_type'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "for yqtr in split_df.yqtr:\n",
    "    \n",
    "    # update current period\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "    \n",
    "    # train on select periods\n",
    "    # if data_hparams['yqtr']=='2014-q1':\n",
    "    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCGRU\n",
    "class CCGRU(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # set model types\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'univariate'\n",
    "        self.model_type = 'gru'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # layers\n",
    "        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\n",
    "                                 dropout=0.1, bidirectional=True)\n",
    "        self.dropout_expert = nn.Dropout(hparams.dropout)\n",
    "        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, valid_seq_len):\n",
    "        # Note: inp is [N, S, E] and **already** been packed\n",
    "        self.gru_expert.flatten_parameters()\n",
    "        \n",
    "        # if S is longer than `max_seq_len`, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\n",
    "        \n",
    "        # RNN layers\n",
    "        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\n",
    "        \n",
    "        # final FC layers\n",
    "        y_car = self.linear_car(x_expert) # (N, E)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'trainer_loss': loss_car}}\n",
    "            \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}        \n",
    "    \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over 24 windows\n",
    "load_split_df()\n",
    "load_targets()\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key\n",
    "    'batch_size': 8, # key\n",
    "    'val_batch_size': 1, # key\n",
    "    \n",
    "    'max_seq_len': 1024, \n",
    "    'learning_rate': 3e-4,\n",
    "    'task_weight': 1,\n",
    "\n",
    "    'n_layers_encoder': 6,\n",
    "    'n_head_encoder': 8, # optional\n",
    "    'd_model': 1024,\n",
    "    'rnn_hidden_size': 64,\n",
    "    'final_tdim': 1024, # optional\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} # optional\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # checkpoint & log\n",
    "    'note': 'temp',\n",
    "    'checkpoint_path': 'D:\\Checkpoints\\earnings-call',\n",
    "    'row_log_interval': 1,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.25,\n",
    "\n",
    "    # data size\n",
    "    'overfit_pct': 1,\n",
    "    'min_epochs': 0,\n",
    "    'max_epochs': 1,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 5,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt(trainer_hparams['checkpoint_path'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df)):\n",
    "    # load preembeddings\n",
    "    load_preembeddings(model_hparams['preembedding_name'])\n",
    "\n",
    "    # train one window\n",
    "    trainer_one(CCGRU, window_i, model_hparams, trainer_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CCTransformerSTLTxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt\n",
    "class CCTransformerSTLTxt(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, n_layers_encoder, dff, max_seq_len, model_type='STL', dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for CAR\n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model, 32)\n",
    "        self.fc_2 = nn.Linear(32, 1)\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # forward\n",
    "    def shared_step(self, batch):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # decode with avgpool\n",
    "        x_expert = x_expert.mean(1) # (N, E)\n",
    "        \n",
    "        # decode with maxpool\n",
    "        # x_expert_maxpool = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        # concat\n",
    "        # x_expert = torch.cat([x_expert_avgpool, x_expert_maxpool], dim=-1) # (N, 2E)\n",
    "\n",
    "        # final FC\n",
    "        y_car = F.relu(self.fc_1(x_expert)) # (N, 1)\n",
    "        y_car = self.fc_2(y_car)\n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CCTransformerSTLTxtFr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt + fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, \n",
    "                 n_layers_encoder, dff, max_seq_len, model_type='STL', n_finratios=15, dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers \n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        # txt_mixer_layer = TxtMixerLayer(self.hparams.d_model)\n",
    "        # self.txt_mixer = FeatureMixer(txt_mixer_layer, self.hparams.n_layers_txtmixer)\n",
    "        \n",
    "        # fr_mixer_layers = FrMixerLayer(self.n_covariate)\n",
    "        # self.fr_mixer = FeatureMixer(fr_mixer_layers, self.hparams.n_layers_frmixer)\n",
    "        \n",
    "        # final prediction layer\n",
    "        # final_fc_mixer_layer = FeatureMixerLayer(self.hparams.d_model+self.n_covariate)\n",
    "        # self.final_fc_mixer_layer = FeatureMixer(final_fc_mixer_layer, self.hparams.n_layers_finalfc)\n",
    "        # self.fc_batchnorm = nn.BatchNorm1d(self.hparams.d_model+self.n_covariate)\n",
    "        self.final_fc = nn.Linear(self.hparams.d_model+self.hparams.n_finratios, 1)\n",
    "        \n",
    "        # self.txt_fc_1 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.txt_fc_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        # self.txt_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_3 = nn.Dropout(self.hparams.dropout) \n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        x_expert = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        \n",
    "        # project text embedding to a lower dimension\n",
    "        # x_expert = self.txt_dropout_1(F.relu(self.txt_fc_1(x_expert)))\n",
    "        # x_expert = F.relu(self.txt_fc_2(x_expert))\n",
    "        \n",
    "        # x_expert = self.txt_mixer(x_expert)\n",
    "        \n",
    "        # Mix fin_ratios\n",
    "        # fin_ratios = self.batch_norm(fin_ratios)\n",
    "        # x_fr = self.fr_mixer(fin_ratios)\n",
    "        \n",
    "        # concate `x_final` with `fin_ratios`\n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_finratios)\n",
    "        \n",
    "        # final FC\n",
    "        # x_final = self.fc_dropout_1(F.relu(self.fc_1(x_expert))) # (N, E+X)\n",
    "        # x_car = self.final_fc_mixer_layer(x_final) # (N, E+X)\n",
    "        y_car = self.final_fc(x_final)\n",
    "        \n",
    "        t_car = car_norm\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a444189610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Checkpoint directory d:/checkpoints/earnings-call exists and is not empty. With save_top_k=1, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "CometLogger will be initialized in online mode\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/b5c3769d279740e8a995aa0d907b271e\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preembeddings: all_sbert_roberta_nlistsb_encoded...@03:57:36\n",
      "Loading preembeddings finished. @03:58:13\n",
      "Loading targets...@03:58:13\n",
      "Loading finished. @03:58:16\n",
      "Current window: 2012-q4 (3y) \n",
      "(train: 2010-01-01 to 2012-12-31) (test: 2013-01-01 to 2013-03-31)\n",
      "N train = 3912\n",
      "N val = 432\n",
      "N train+val = 4344\n",
      "N test = 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder_pos    | PositionalEncoding | 0     \n",
      "1 | encoder_expert | TransformerEncoder | 33 M  \n",
      "2 | fc_1           | Linear             | 32 K  \n",
      "3 | fc_2           | Linear             | 33    \n",
      "4 | dropout_1      | Dropout            | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7c65cc710f4e0ebd36333d4fde6190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c851b00de5d24e5ea672fa4a94597bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 11.00 GiB total capacity; 8.82 GiB already allocated; 382.94 MiB free; 8.87 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0d3f406d824a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# train on select periods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtrainer_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myqtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_hparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_hparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer_hparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-c4bf34dc9e96>\u001b[0m in \u001b[0;36mtrainer_one\u001b[1;34m(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c4bf34dc9e96>\u001b[0m in \u001b[0;36mtrainer_one\u001b[1;34m(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# test on the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'on_fit_start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\accelerators\\dp_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# train or test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[1;31m# run train epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;31m# TRAINING_STEP + TRAINING_STEP_END\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[1;31m# ------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m             \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;31m# when returning -1 from train_step, we end epoch early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                         \u001b[1;31m# optimizer step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer_step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;31m# optimizer step lightningModule hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             self.trainer.accelerator_backend.optimizer_step(\n\u001b[0m\u001b[0;32m    453\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m             )\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, batch_idx, opt_idx, lambda_closure)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# model hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         model_ref.optimizer_step(\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m     def optimizer_zero_grad(\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain_step_and_backward_closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                         \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m                             result = self.training_step_and_backward(\n\u001b[0m\u001b[0;32m    704\u001b[0m                                 \u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                                 \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtraining_step_and_backward\u001b[1;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[0;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m    797\u001b[0m         \u001b[1;31m# lightning module hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_curr_step_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_forward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_train_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mtraining_step_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mtraining_step_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_step_end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_step_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\accelerators\\dp_accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\overrides\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\overrides\\data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\overrides\\data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\overrides\\data_parallel.py\u001b[0m in \u001b[0;36m_worker\u001b[1;34m(i, module, input, kwargs, device)\u001b[0m\n\u001b[0;32m    261\u001b[0m                 \u001b[1;31m# CHANGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m                     \u001b[0mfx_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'training_step'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-dcf96d968b4a>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch, idx)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# traning step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mtranscriptid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_car\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_car\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'y_car'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_car\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m't_car'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt_car\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-dcf96d968b4a>\u001b[0m in \u001b[0;36mshared_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mx_expert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_expert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (N, S, E)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# decode with attn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[1;32mclass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[0;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m    976\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[0;32m    977\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[0;32m    979\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   4304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4305\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4306\u001b[1;33m         attn_output_weights = attn_output_weights.masked_fill(\n\u001b[0m\u001b[0;32m   4307\u001b[0m             \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4308\u001b[0m             \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 11.00 GiB total capacity; 8.82 GiB already allocated; 382.94 MiB free; 8.87 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# choose Model\n",
    "Model = CCTransformerSTLTxt\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_sentiment_text_norm_wsrz', # key!\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'val_batch_size':32,\n",
    "    'test_batch_size':32,\n",
    "    \n",
    "    'text_in_dataset': True,\n",
    "    'roll_type': '3y', # key!\n",
    "}\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':3e-4, # key!\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 16, \n",
    "    'd_model': 1024,\n",
    "    'dff': 2048, # default: 2048\n",
    "    'attn_dropout': 0.1,\n",
    "    # 'dropout': 0.5\n",
    "} \n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # last: STL-39\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"STL-50\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2, # key! (0.25: check 4 times in a epoch)\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3, # default: 3\n",
    "    'max_epochs': 20, # default: 20\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(data_hparams['roll_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "\n",
    "# Customize preembeddings\n",
    "\n",
    "\n",
    "for yqtr in split_df.yqtr:\n",
    "\n",
    "    # Enforce yqtr>='2012-q4' (the earliest yqtr in roll_type=='3y')\n",
    "    if yqtr>='2012-q4':\n",
    "\n",
    "        # update current period\n",
    "        data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "        # train on select periods\n",
    "        train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MTL, hardshare) car + inf/rev ~ txt + fr\n",
    "class CCTransformerMTLHard(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # check task weights sum to one\n",
    "        assert self.hparams.car_weight+self.hparams.inflow_weight+self.hparams.revision_weight==1, 'car_weight + inflow_weight + revision_weight != 1'\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = f'{self.hparams.car_weight:.2f}*car+{self.hparams.inflow_weight:.2f}*inf+{self.hparams.revision_weight:.2f}*rev' # key!\n",
    "        self.feature_type = 'txt'    # key!\n",
    "        self.emb_share = 'hard'      # key!\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_car_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.fc_inflow_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.fc_revision_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # aggregate with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # final FC\n",
    "        y_car = self.fc_car_1(x_expert) # (N, 1)\n",
    "        # y_inflow = self.fc_inflow_1(x_expert) # (N,1)\n",
    "        y_revision = self.fc_revision_1(x_expert)\n",
    "        \n",
    "        # log hist: x_expert\n",
    "        if self.global_step%50==True:\n",
    "            self.logger.experiment.log_histogram_3d(x_expert.detach().cpu().numpy(), name='feature:x_expert', step=self.global_step)\n",
    "            # self.logger.experiment.log_histogram_3d(x_car.detach().cpu().numpy(), name='feature:x_car', step=self.global_step)\n",
    "            # self.logger.experiment.log_histogram_3d(x_inflow.detach().cpu().numpy(), name='feature:x_inflow', step=self.global_step)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_revision\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # decide if log activation histogram\n",
    "        # log_histogram = True if idx%50==0 else False\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'trainer_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        # return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_inflow': loss_inflow}\n",
    "        return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_revision': loss_revision}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_revision = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        # loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_revision = self.mse_loss(y_revision, revision_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.revision_weight*loss_revision\n",
    "        \n",
    "        # logging\n",
    "        # return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_inflow': loss_inflow}  \n",
    "        return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_revision': loss_revision}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MTL, softshare) x*car + (1-x)*inf ~ txt + fr\n",
    "class CCTransformerMTLSoft(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car+inf'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.emb_share = 'hard'\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(self.hparams.d_model, self.hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_4 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_5 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        self.linear_inflow = nn.Linear(self.hparams.final_tdim, 1)\n",
    "        # self.linear_revision = nn.Linear(hparam.final_tdim, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.final_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_3 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(self.hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # multiply with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # mix with covariate\n",
    "        x_expert = self.final_dropout_1(F.relu(self.linear_car_1(x_expert))) # (N, E)\n",
    "        x_expert = F.relu(self.linear_car_2(x_expert)) # (N, final_tdim)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratio = self.batch_norm(fin_ratios)\n",
    "        \n",
    "        x_car = torch.cat([x_expert, fin_ratios], dim=-1) # (N, X + final_tdim) where X is the number of covariate (n_covariate)\n",
    "\n",
    "            \n",
    "        # final FC\n",
    "        y_inflow = self.linear_inflow(x_expert)\n",
    "        # y_revision = self.linear_revision(x_expert)\n",
    "        \n",
    "        x_car = self.final_dropout_2(F.relu(self.linear_car_3(x_car))) # (N, X + final_tdim)\n",
    "        y_car = self.linear_car_5(x_car) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_inflow\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        \n",
    "        assert self.hparams.car_weight+self.hparams.inflow_weight==1, 'car_weight + inflow_weight != 1'\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.inflow_weight*loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'trainer_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_inflow': loss_inflow}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_inflow': loss_inflow} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose Model\n",
    "Model = CCTransformerSTLTxtFr\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'seed': 42, # key!\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_sentiment_text_norm_wsrz', # key!\n",
    "    'roll_type': '3y',  # key!\n",
    "    'gpus': [0,1],\n",
    "    \n",
    "    # task weight\n",
    "    'car_weight': 1,      # Key!\n",
    "    'inflow_weight': 0,   # key!\n",
    "    'revision_weight': 0, # key!\n",
    "    \n",
    "    'batch_size': 28,     # key!\n",
    "    'val_batch_size': 28,\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':1e-4, # key!\n",
    "    'task_weight': 1,\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_layers_txtmixer': 2, # key!\n",
    "    'n_layers_frmixer': 2,  # key!\n",
    "    'n_layers_finalfc': 2,  # key!\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 1024,\n",
    "    'final_tdim': 1024, # key!\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} \n",
    "\n",
    "trainer_hparams = {\n",
    "    # log\n",
    "    'machine': 'yu-workstation',  # key!\n",
    "    'note': f\"STL-37,(car~txt+fr 3y BatchNormFr BatchNormTxt),txtMixer=2({model_hparams['final_tdim']}),fcMixer=2,standCAR=yes,standFr=no,bsz={model_hparams['batch_size']},seed={model_hparams['seed']},log(mcap)=yes,lr={model_hparams['learning_rate']:.2g}\", # key!\n",
    "    'row_log_interval': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32,\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3,\n",
    "    'max_epochs': 20,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_name'])\n",
    "    \n",
    "# loop over 24!\n",
    "np.random.seed(model_hparams['seed'])\n",
    "torch.manual_seed(model_hparams['seed'])\n",
    "\n",
    "for window_i in range(len(split_df)):\n",
    "\n",
    "    # train one window\n",
    "    trainer_one(Model, window_i, model_hparams, trainer_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hparams = {\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm', # key!\n",
    "    'roll_type': '3y',  # key!\n",
    "}    \n",
    "\n",
    "# load split_df\n",
    "load_split_df(data_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (3y) \n",
      "(train: 2008-01-01 to 2010-12-31) (test: 2011-01-01 to 2011-03-31)\n"
     ]
    }
   ],
   "source": [
    "model = CCTransformerMTLInfHard.load_from_checkpoint('D:/Checkpoints/earnings-call/3y-TSFM-(0.5car+0.5inf~txt+fr)-hardshare-norm/TSFM_roll-01_epoch=9_v0.ckpt')\n",
    "\n",
    "model.freeze()\n",
    "\n",
    "model.prepare_data()\n",
    "dataloader = model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor = torch.nn.Sequential(*list(model.children())[:-8])\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    if i > 1: break\n",
    "    car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "    transcriptid, embeddings, mask, \\\n",
    "    fin_ratios = batch\n",
    "\n",
    "    # forward\n",
    "    output = extractor(embeddings, mask, fin_ratios)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test on one batch\n",
    "def test_step_fr(model, batch):\n",
    "    # get input\n",
    "    docid, car, inp = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(inp).item() # (N, 1)\n",
    "\n",
    "    return y_car, docid[0]\n",
    "    \n",
    "    \n",
    "def test_step_text(model, batch):\n",
    "    car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "    sest, sue, numest, sstdest, smedest, \\\n",
    "    mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(embeddings, mask).item() # (N, 1)\n",
    "    transcriptid = transcriptid.int().item()\n",
    "    docid = targets_df.loc[targets_df.transcriptid==transcriptid].docid.iloc[0]\n",
    "\n",
    "    return y_car, docid\n",
    "\n",
    "def test_step_text_fr(model, batch):\n",
    "    car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "    sest, sue, numest, sstdest, smedest, \\\n",
    "    mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                  smedest, mcap, roa, bm, debt_asset, volatility, volume).item() # (N, 1)\n",
    "    transcriptid = transcriptid.int().item()\n",
    "    docid = targets_df.loc[targets_df.transcriptid==transcriptid].docid.iloc[0]\n",
    "\n",
    "    return y_car, docid\n",
    "\n",
    "# test on one window\n",
    "def test_one_window(model, dataloader):\n",
    "    # select test_step\n",
    "    if type(model) in [CCMLP]:\n",
    "        test_step = test_step_fr\n",
    "    elif type(model) in [CCTransformerSTLTxtFr]:\n",
    "        test_step = test_step_text_fr\n",
    "    # elif type(model) in [CCTransformerSTLTxtFr]:\n",
    "    #    test_step = test_step_text_fr\n",
    "    \n",
    "    ys = []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        ys.append(test_step(model, batch))\n",
    "        # if i>=2: break\n",
    "    return pd.DataFrame(ys, columns=['y_car', 'docid'])\n",
    "\n",
    "# get prediction\n",
    "def predict_car():\n",
    "    global hparams\n",
    "    hparams = Namespace(**hparams)\n",
    "\n",
    "    # get roll_type\n",
    "    hparams.roll_type = hparams.ckpt_folder.split('/')[-1].split('-')[0]\n",
    "    \n",
    "    # load split_df\n",
    "    load_split_df(hparams.roll_type)\n",
    "\n",
    "    # load targets_df\n",
    "    load_targets(hparams.targets_name)\n",
    "\n",
    "    # load preembedding\n",
    "    load_preembeddings(hparams.preembedding_name)\n",
    "    \n",
    "    car_prediction = []\n",
    "    for i, name in enumerate(sorted(os.listdir(f'D:/Checkpoints/earnings-call/{hparams.ckpt_folder}'))):\n",
    "        # if i>=1: break\n",
    "            \n",
    "        if name.endswith('.ckpt'):\n",
    "\n",
    "            # get window\n",
    "            window = re.search(r'roll-\\d{2}', name).group()\n",
    "\n",
    "            # load model\n",
    "            model = hparams.Model.load_from_checkpoint(f'D:/Checkpoints/earnings-call/{hparams.ckpt_folder}/{name}')\n",
    "\n",
    "            # get testloader\n",
    "            model.prepare_data()\n",
    "            test_dataloader = model.test_dataloader()\n",
    "            model.freeze()\n",
    "            \n",
    "            # predict\n",
    "            y = test_one_window(model, test_dataloader)\n",
    "            y['window'] = window\n",
    "            y['roll_type'] = hparams.roll_type\n",
    "\n",
    "            # append to ys\n",
    "            car_prediction.append(y)\n",
    "    car_prediction = pd.concat(car_prediction)\n",
    "    \n",
    "    car_prediction.reset_index().to_feather(f'data/{hparams.save_name}.feather')\n",
    "    return car_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'Model': CCTransformerSTLTxtFr, # key!\n",
    "    'ckpt_folder': '3y-TSFM-txt-fr', # key!\n",
    "    'save_name': 'car_pred_tsfm_txt_fr', # key!\n",
    "\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_text',\n",
    "    'preembedding_name': 'all_sbert_roberta_nlistsb_encoded',\n",
    "}\n",
    "\n",
    "# get car_ranking_predict\n",
    "car_pred_mlp = predict_car()\n",
    "\n",
    "# car_pred_mlp.reset_index().to_feather('data/car_pred_mlp.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_car</th>\n",
       "      <th>docid</th>\n",
       "      <th>window</th>\n",
       "      <th>roll_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>15.298223</td>\n",
       "      <td>031122-2011-01-10</td>\n",
       "      <td>roll-01</td>\n",
       "      <td>3y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_car              docid   window roll_type\n",
       "224  15.298223  031122-2011-01-10  roll-01        3y"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_pred_mlp.loc[car_pred_mlp.docid=='031122-2011-01-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true"
   },
   "source": [
    "### `val` is after `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, valid_transcriptids=None, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "        '''\n",
    "\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        \n",
    "        # decalre data as globals so don't   need to create/reload\n",
    "        global preembeddings, targets_df, split_df\n",
    "        \n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, val_start, val_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (val: {val_start} to {val_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        val_start = datetime.strptime(val_start, '%Y-%m-%d').date()\n",
    "        val_end = datetime.strptime(val_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.tolist()\n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(val_start, val_end)].transcriptid.tolist()\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "        \n",
    "        # self attributes\n",
    "        self.targets_df = targets_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.transform = transform\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # inputs: preembeddings\n",
    "        embeddings = self.preembeddings[transcriptid]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        sue = targets.sue\n",
    "        sest = targets.sest\n",
    "        car_0_30 = targets.car_0_30\n",
    "        \n",
    "        alpha = targets.alpha\n",
    "        volatility = targets.volatility\n",
    "        mcap = targets.mcap/1e6\n",
    "        bm = targets.bm\n",
    "        roa = targets.roa\n",
    "        debt_asset = targets.debt_asset\n",
    "        numest = targets.numest\n",
    "        smedest = targets.smedest\n",
    "        sstdest = targets.sstdest\n",
    "        car_m1_m1 = targets.car_m1_m1\n",
    "        car_m2_m2 = targets.car_m2_m2\n",
    "        car_m30_m3 = targets.car_m30_m3\n",
    "        volume = targets.volume\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            return car_0_30, transcriptid, embeddings, alpha, car_m1_m1, car_m2_m2, car_m30_m3, \\\n",
    "                   sest, sue, numest, sstdest, smedest, \\\n",
    "                   mcap, roa, bm, debt_asset, volatility, volume\n",
    "        else:\n",
    "            return torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
