{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /home/yu/OneDrive/CC\n",
      "DATA_DIR: /home/yu/OneDrive/CC/data\n",
      "CHECKPOINT_DIR: /home/yu/Data/CC-checkpoints\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 10.76 (GB)\n",
      "GPU 1:  0.00/ 10.76 (GB)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import comet_ml\n",
    "import datatable as dt\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict, defaultdict\n",
    "from datatable import f, update\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Init for script use\n",
    "with open(\"/home/yu/OneDrive/App/Settings/jupyter + R + Python/python_startup.py\", 'r') as _:\n",
    "    exec(_.read())\n",
    "\n",
    "os.chdir('/home/yu/OneDrive/CC')\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '/home/yu/OneDrive/CC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = '/home/yu/Data/CC-checkpoints'\n",
    "CHECKPOINT_TEMP_DIR = f'{CHECKPOINT_DIR}/archive'\n",
    "\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: tensor_to_list\n",
    "def tensor_to_str(tensor, tailing='\\n'):\n",
    "    '''Given a 1d tensor, convert it to a list of string\n",
    "    \n",
    "    tailing: str. Added to the end of every element\n",
    "    '''\n",
    "    assert isinstance(tensor, torch.Tensor), 'Give me a tensor please!'\n",
    "    assert len(tensor.shape)==1, 'Must be 1D tensor!'\n",
    "    tensor = tensor.to('cpu').tolist()\n",
    "    tensor = [f'{str(x)}{tailing}' for x in tensor]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=torch.device('cpu'):\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/archive`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_TEMP_DIR}/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name, force=False):\n",
    "    targets_df = feather.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "    targets_df = targets_df[targets_df.outlier_flag1==False]\n",
    "    return targets_df\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_name):\n",
    "    if 'preembeddings' not in globals():\n",
    "        \n",
    "        # find the embedding files\n",
    "        emb_paths = [path for path in os.listdir('data/Embeddings')\n",
    "                     if re.search(f'{preembedding_name}_rank', path)]\n",
    "        emb_paths.sort()\n",
    "        assert len(emb_paths)==2, \"Expect two files: rank0 and rank1\"\n",
    "\n",
    "        # load the embedding files\n",
    "        print(f'{datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}, Loading \"{emb_paths[0]}\"...')\n",
    "        emb0 = torch.load(f\"{DATA_DIR}/Embeddings/{emb_paths[0]}\")\n",
    "        print(f'{datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}, Loading \"{emb_paths[1]}\"...')\n",
    "        emb1 = torch.load(f\"{DATA_DIR}/Embeddings/{emb_paths[1]}\")\n",
    "\n",
    "        # merge two ranks into one (update emb0 with emb1)\n",
    "        for tid, cid_emb in emb1.items():\n",
    "            for cid, emb in cid_emb.items():\n",
    "                emb0[tid].update({cid:emb})\n",
    "        print('Merging completes!')\n",
    "\n",
    "        # write embedding to globals()\n",
    "        globals()['preembeddings'] = emb0\n",
    "    \n",
    "    else:\n",
    "        print(f'Pre-embedding \"{preembedding_name}\" already loaded, will not load again!')\n",
    "\n",
    "# helpers: load split_df\n",
    "def load_split_df(window_size):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    return split_df.loc[split_df.window_size==window_size]\n",
    "\n",
    "# helpers: load tid_cid_pair\n",
    "def load_tid_cid_pair(tid_cid_pair_name):\n",
    "    '''load DataFrame tid_cid_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid:[cid1, cid2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"md\", \"qa\", \"all\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_cid_pair_{tid_cid_pair_name}.feather')\n",
    "    output = {}\n",
    "    for tid, group in pair.groupby(['transcriptid']):\n",
    "        cids = group.componentid.to_list()\n",
    "        output[tid] = cids\n",
    "    return output\n",
    "    \n",
    "# helpers: load tid_cid_pair\n",
    "def load_tid_from_to_pair(tid_from_to_pair_name):\n",
    "    '''load DataFrame tid_from_to_pair, convert it into a Dict\n",
    "    \n",
    "    output: {tid_from:[tid_to1, tid_to2, ...]}\n",
    "    \n",
    "    tid_cid_pair_name: str. e.g., \"3qtr\"\n",
    "    '''\n",
    "    pair = feather.read_feather(f'data/tid_from_to_pair_{tid_from_to_pair_name}.feather')\n",
    "    \n",
    "    output = {}\n",
    "    for _, (_, tid_from, tid_to) in pair.iterrows():\n",
    "        output[tid_from] = tid_to.tolist()\n",
    "    return output\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, yqtr, window_size):\n",
    "    '''\n",
    "    Given yqtr, find the corresponding ols_rmse from `performance_by_model.feather`.\n",
    "    Always compare to the same model: 'ols: car_stand ~ fr'\n",
    "    then log to Comet\n",
    "    '''\n",
    "    performance = dt.Frame(pd.read_feather('data/performance_by_yqtr.feather'))\n",
    "\n",
    "\n",
    "    ols_rmse = performance[(f.model_name=='ols: car_stand ~ fr') & (f.window_size==window_size) & (f.yqtr==yqtr), f.rmse][0,0]\n",
    "    logger.experiment.log_parameter('ols_rmse', ols_rmse)\n",
    "    \n",
    "def log_test_start(logger, window_size, yqtr):\n",
    "    '''\n",
    "    Given window, find the corresponding star/end date of the training/test periods, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, *_ = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.window_size==window_size)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, yqtr, split_type, text_in_dataset, window_size, targets_df, split_df, tid_cid_pair=None, tid_from_to_pair=None, preembeddings=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings: dict of pre-embeddings. In the form\n",
    "              `{tid:{cid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for component level and \n",
    "              `{tid:{sid:{'embedding':Tensor, other-key-value-pair}}}` \n",
    "              for sentence level.\n",
    "              \n",
    "            targets_df: DataFrame of targets variables.\n",
    "            split_df: DataFrame that keeps the split of windows\n",
    "            ytqr: str. e.g., \"2008-q3\"\n",
    "            split_type: str. 'train', 'val', or 'test'\n",
    "            text_in_dataset: also output text embedding if true.\n",
    "            \n",
    "            tid_cid_pair: Dict of transcriptid and componentid/sentenceid for\n",
    "              text that will be used. In the form \n",
    "              `{tid:[cid1, cid2, ...]}` or `{tid:[sid1, sid2, ...]}`\n",
    "              Note! [cid1, cid2, ...] must be in the same order as in original \n",
    "              transcript!\n",
    "        '''\n",
    "            \n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _, yqtr = tuple(split_df.loc[(split_df.yqtr==yqtr) & (split_df.window_size==window_size)].iloc[0])\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # generate targets_df for train, val, test \n",
    "        if split_type=='train':\n",
    "            # print current window\n",
    "            print(f'Current window: {yqtr} ({window_size}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "            \n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].sample(frac=1, random_state=42)\n",
    "            # targets_df = targets_df.iloc[:int(len(targets_df)*0.9)]\n",
    "            \n",
    "        elif split_type=='val':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].sample(frac=1, random_state=42)\n",
    "            targets_df = targets_df.iloc[int(len(targets_df)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            targets_df = targets_df[targets_df.ciq_call_date.between(test_start, test_end)]\n",
    "\n",
    "        \n",
    "        if text_in_dataset:\n",
    "            # make sure targets_df only contains transcriptid that're also \n",
    "            # in preembeddings\n",
    "            targets_df = targets_df.loc[targets_df.transcriptid.isin(set(preembeddings.keys()))]\n",
    "            \n",
    "            self.tid_cid_pair = tid_cid_pair\n",
    "            self.tid_from_to_pair = tid_from_to_pair\n",
    "            self.preembeddings = preembeddings\n",
    "        \n",
    "        # Assign states\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "\n",
    "        self.targets_df = targets_df\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.split_type = split_type\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        targets = self.targets_df.iloc[idx]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        transcriptid = targets.transcriptid\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_stand = targets.car_0_30_stand\n",
    "        revision = targets.revision\n",
    "        revision_stand = targets.revision_stand\n",
    "        inflow = targets.inflow\n",
    "        inflow_stand = targets.inflow_stand\n",
    "        \n",
    "        # using the normalized features\n",
    "        similarity = targets.similarity_bigram_stand\n",
    "        sentiment = targets.qa_positive_sent_stand\n",
    "        sue = targets.sue_stand\n",
    "        sest = targets.sest_stand        \n",
    "        alpha = targets.alpha_stand\n",
    "        volatility = targets.volatility_stand\n",
    "        mcap = targets.mcap_stand\n",
    "        bm = targets.bm_stand\n",
    "        roa = targets.roa_stand\n",
    "        debt_asset = targets.debt_asset_stand\n",
    "        numest = targets.numest_stand\n",
    "        smedest = targets.smedest_stand\n",
    "        sstdest = targets.sstdest_stand\n",
    "        car_m1_m1 = targets.car_m1_m1_stand\n",
    "        car_m2_m2 = targets.car_m2_m2_stand\n",
    "        car_m30_m3 = targets.car_m30_m3_stand\n",
    "        volume = targets.volume_stand\n",
    "\n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = assemble_embedding(transcriptid, \n",
    "                                            self.preembeddings,\n",
    "                                            self.tid_cid_pair,\n",
    "                                            self.tid_from_to_pair)\n",
    "\n",
    "            return car_0_30, car_0_30_stand, inflow, inflow_stand, revision,\\\n",
    "                   revision_stand,  transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return torch.tensor(transcriptid,dtype=torch.int64), \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor(inflow,dtype=torch.float32), \\\n",
    "                   torch.tensor(inflow_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor(revision,dtype=torch.float32), \\\n",
    "                   torch.tensor(revision_stand,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],\n",
    "                                dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "                                 sest, sue, numest, sstdest, smedest, mcap,\\\n",
    "                                 roa, bm, debt_asset, volatility, volume],\n",
    "                                dtype=torch.float32)\n",
    "      \n",
    "    \n",
    "def assemble_embedding(transcriptid, preembeddings, \n",
    "                       tid_cid_pair, tid_from_to_pair):\n",
    "    '''Assemble embeddings belonging to the same tid into one Tensor\n",
    "    \n",
    "    Method:\n",
    "        1) Given transcriptid, use it as \"transcriptid_from\" to retrieve all the \n",
    "           corresponding \"transcriptid_to\" from table \"tid_from_to_pair\"\n",
    "        2) For every transcript_to, retrieve all the corresponding cids from table\n",
    "           \"tid_cid_pair\"\n",
    "    '''\n",
    "    # find tids that we'll consider\n",
    "    tids_to = tid_from_to_pair[transcriptid]\n",
    "    \n",
    "    # for every tid, merge its components\n",
    "    output = []\n",
    "    \n",
    "    for tid_to in tids_to:\n",
    "        comps = preembeddings[tid_to]\n",
    "        emb = [comps[cid]['embedding'] \n",
    "               for cid in tid_cid_pair.get(tid_to, [])]\n",
    "        output.extend(emb)\n",
    "        \n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then define DataModule\n",
    "class CCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, yqtr, targets_name, batch_size, val_batch_size,\n",
    "                 test_batch_size, text_in_dataset, window_size, \n",
    "                 preembedding_name=None, tid_cid_pair_name=None,\n",
    "                 tid_from_to_pair_name=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.yqtr = yqtr\n",
    "        self.preembedding_name = preembedding_name\n",
    "        self.targets_name = targets_name\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        self.window_size = window_size\n",
    "        self.tid_cid_pair_name = tid_cid_pair_name\n",
    "        self.tid_from_to_pair_name = tid_from_to_pair_name\n",
    "        \n",
    "    # Dataset\n",
    "    def setup(self):\n",
    "        # read the preembedding, targests, and split_df\n",
    "        if self.text_in_dataset:\n",
    "            global preembeddings\n",
    "            load_preembeddings(self.preembedding_name)\n",
    "            tid_cid_pair = load_tid_cid_pair(self.tid_cid_pair_name)\n",
    "            tid_from_to_pair = load_tid_from_to_pair(self.tid_from_to_pair_name)\n",
    "        else:\n",
    "            preembeddings = None\n",
    "            tid_cid_pair = None\n",
    "            tid_from_to_pair = None\n",
    "            \n",
    "        targets_df = load_targets(self.targets_name)\n",
    "        split_df = load_split_df(self.window_size)\n",
    "\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.yqtr, \n",
    "                                       split_type='train',\n",
    "                                       text_in_dataset=self.text_in_dataset,\n",
    "                                       window_size=self.window_size,\n",
    "                                       preembeddings=preembeddings,\n",
    "                                       targets_df=targets_df, \n",
    "                                       split_df=split_df,\n",
    "                                       tid_cid_pair=tid_cid_pair,\n",
    "                                       tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.yqtr, split_type='val',\n",
    "                                     text_in_dataset=self.text_in_dataset,\n",
    "                                     window_size=self.window_size,\n",
    "                                     preembeddings=preembeddings,\n",
    "                                     targets_df=targets_df,\n",
    "                                     split_df=split_df,\n",
    "                                     tid_cid_pair=tid_cid_pair,\n",
    "                                     tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.yqtr, split_type='test',\n",
    "                                      text_in_dataset=self.text_in_dataset, \n",
    "                                      window_size=self.window_size,\n",
    "                                      preembeddings=preembeddings,\n",
    "                                      targets_df=targets_df,\n",
    "                                      split_df=split_df,\n",
    "                                      tid_cid_pair=tid_cid_pair,\n",
    "                                      tid_from_to_pair=tid_from_to_pair)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=True, drop_last=False, num_workers=2,\n",
    "                          pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - It's safe to set `drop_last=True` without under-counting samples.\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.val_batch_size,\n",
    "                          num_workers=2, pin_memory=True, collate_fn=collate_fn,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, batch_size=self.test_batch_size, num_workers=2, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_stand, inflow, inflow_stand, revision, revision_stand, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), \\\n",
    "               torch.tensor(car_0_30_stand, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow_stand, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), \\\n",
    "               torch.tensor(revision_stand, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), \\\n",
    "               embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    '''Mainly define the `*_step_end` methods\n",
    "    '''\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # def training_step_end\n",
    "    def training_step_end(self, outputs):\n",
    "        y = outputs['y_car']\n",
    "        t = outputs['t_car']\n",
    "        loss = self.mse_loss(y, t)\n",
    "        \n",
    "        return {'loss':loss}\n",
    "    \n",
    "    # def validation_step_end\n",
    "    def validation_step_end(self, outputs):\n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        '''\n",
    "        outputs: a list. len(outputs) == num_steps.\n",
    "            e.g., outputs = [{'val_loss': 3}, {'val_loss': 4}]\n",
    "        '''\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('val_rmse', rmse, on_step=False)\n",
    "        \n",
    "    # test step\n",
    "    def test_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        \n",
    "        y_car = outputs['y_car']\n",
    "        t_car = outputs['t_car']\n",
    "        \n",
    "        return {'y_car':y_car, 't_car':t_car, 'transcriptid':transcriptid}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \n",
    "        transcriptid = torch.cat([x['transcriptid'] for x in outputs])\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        \n",
    "        rmse = torch.sqrt(self.mse_loss(y_car, t_car))\n",
    "        self.log('test_rmse', rmse, on_step=False)\n",
    "        \n",
    "        # save & log `y_car`\n",
    "        y_car_filename = f'{DATA_DIR}/y_car.feather'\n",
    "        \n",
    "        df = pd.DataFrame({'transcriptid':transcriptid.to('cpu'),\n",
    "                           'y_car':y_car.to('cpu'),\n",
    "                           't_car':t_car.to('cpu')})\n",
    "        feather.write_feather(df, y_car_filename)\n",
    "            \n",
    "        self.logger.experiment.log_asset(y_car_filename)\n",
    "            \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   \n",
    "    \n",
    "# Model: MTL\n",
    "class CCMTL(CC):\n",
    "    '''Mainly define the `*_step_end` methods\n",
    "    '''\n",
    "    # loss\n",
    "    def binary_cross_entropy_loss(self, y, t):\n",
    "        return F.binary_cross_entropy(y, t)\n",
    "        \n",
    "    # def training_step_end\n",
    "    def training_step_end(self, outputs):\n",
    "        y_car = outputs['y_car']\n",
    "        y_rev = outputs['y_rev']\n",
    "        y_inf = outputs['y_inf']\n",
    "        \n",
    "        t_car = outputs['t_car']\n",
    "        t_rev = outputs['t_rev']\n",
    "        t_inf = outputs['t_inf']\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(loss_rev + loss_inf)/2\n",
    "        \n",
    "        return {'loss':loss}\n",
    "    \n",
    "    # def validation_step_end\n",
    "    def validation_step_end(self, outputs):\n",
    "        y_car = outputs['y_car']\n",
    "        y_rev = outputs['y_rev']\n",
    "        y_inf = outputs['y_inf']\n",
    "        \n",
    "        t_car = outputs['t_car']\n",
    "        t_rev = outputs['t_rev']\n",
    "        t_inf = outputs['t_inf']\n",
    "        \n",
    "        return {'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        '''\n",
    "        outputs: a list. len(outputs) == num_steps.\n",
    "            e.g., outputs = [{'val_loss': 3}, {'val_loss': 4}]\n",
    "        '''\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        y_rev = torch.cat([x['y_rev'] for x in outputs])\n",
    "        y_inf = torch.cat([x['y_inf'] for x in outputs])\n",
    "        \n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        t_rev = torch.cat([x['t_rev'] for x in outputs])\n",
    "        t_inf = torch.cat([x['t_inf'] for x in outputs])\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        \n",
    "        loss = loss_car + self.hparams.alpha*(loss_rev + loss_inf)/2\n",
    "        \n",
    "        rmse = torch.sqrt(loss)\n",
    "        self.log('val_rmse', rmse, on_step=False)\n",
    "        \n",
    "    # test step\n",
    "    def test_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        \n",
    "        y_car = outputs['y_car']\n",
    "        y_rev = outputs['y_rev']\n",
    "        y_inf = outputs['y_inf']\n",
    "        \n",
    "        t_car = outputs['t_car']\n",
    "        t_rev = outputs['t_rev']\n",
    "        t_inf = outputs['t_inf']\n",
    "        \n",
    "        return {'transcriptid': transcriptid,\n",
    "                'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \n",
    "        transcriptid = torch.cat([x['transcriptid'] for x in outputs])\n",
    "        y_car = torch.cat([x['y_car'] for x in outputs])\n",
    "        y_rev = torch.cat([x['y_rev'] for x in outputs])\n",
    "        y_inf = torch.cat([x['y_inf'] for x in outputs])\n",
    "        \n",
    "        t_car = torch.cat([x['t_car'] for x in outputs])\n",
    "        t_rev = torch.cat([x['t_rev'] for x in outputs])\n",
    "        t_inf = torch.cat([x['t_inf'] for x in outputs])\n",
    "        \n",
    "        loss_car = self.mse_loss(y_car, t_car)\n",
    "        loss_rev = self.mse_loss(y_rev, t_rev)\n",
    "        loss_inf = self.mse_loss(y_inf, t_inf)\n",
    "        \n",
    "        loss = loss_car\n",
    "        \n",
    "        rmse = torch.sqrt(loss_car)\n",
    "        self.log('test_rmse', rmse, on_step=False)\n",
    "        \n",
    "        # save & log `y_car`\n",
    "        y_car_filename = f'{DATA_DIR}/y_car.feather'\n",
    "        \n",
    "        df = pd.DataFrame({'transcriptid':transcriptid.to('cpu'),\n",
    "                           'y_car':y_car.to('cpu'),\n",
    "                           'y_rev':y_rev.to('cpu'),\n",
    "                           'y_inf':y_inf.to('cpu'),\n",
    "                           't_car':t_car.to('cpu'),\n",
    "                           't_rev':t_rev.to('cpu'),\n",
    "                           't_inf':t_inf.to('cpu')})\n",
    "        feather.write_feather(df, y_car_filename)\n",
    "            \n",
    "        self.logger.experiment.log_asset(y_car_filename)\n",
    "            \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams):\n",
    "\n",
    "    # ----------------------\n",
    "    # `hparams` sanity check\n",
    "    # ----------------------\n",
    "    \n",
    "    # check: batch_size//len(gpus)==0\n",
    "    assert data_hparams['batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['val_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['val_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "    \n",
    "    # check: val_batch_size//len(gpus)==0\n",
    "    assert data_hparams['test_batch_size']%len(trainer_hparams['gpus'])==0, \\\n",
    "        f\"`test_batch_size` must be divisible by `len(gpus)`. Currently batch_size={model_hparams['test_batch_size']}, gpus={trainer_hparams['gpus']}\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Initialize model and trainer\n",
    "    # ----------------------------\n",
    "    \n",
    "    # init model\n",
    "    model = Model(**model_hparams)\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{trainer_hparams['note']}_{data_hparams['yqtr']}\".replace('*', '')\n",
    "    ckpt_prefix = ckpt_prefix + '_{epoch}'\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        monitor='val_rmse',\n",
    "        dirpath=CHECKPOINT_DIR,\n",
    "        filename=ckpt_prefix,\n",
    "        save_top_k=trainer_hparams['save_top_k'],\n",
    "        period=trainer_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_rmse',\n",
    "        min_delta=0,\n",
    "        patience=trainer_hparams['early_stop_patience'],\n",
    "        verbose=True,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=trainer_hparams['gpus'], \n",
    "                         precision=trainer_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         callbacks=[early_stop_callback],\n",
    "                         overfit_batches=trainer_hparams['overfit_batches'], \n",
    "                         log_every_n_steps=trainer_hparams['log_every_n_steps'],\n",
    "                         val_check_interval=trainer_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=5, \n",
    "                         accelerator='dp',\n",
    "                         accumulate_grad_batches=trainer_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=trainer_hparams['min_epochs'],\n",
    "                         max_epochs=trainer_hparams['max_epochs'], \n",
    "                         max_steps=trainer_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # add n_model_params\n",
    "    trainer_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(data_hparams)\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(trainer_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    # log_ols_rmse(logger, data_hparams['yqtr'], data_hparams['window_size'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, data_hparams['window_size'], data_hparams['yqtr'])\n",
    "    \n",
    "    # If run on ASU, upload code explicitly\n",
    "    if trainer_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    # refresh_cuda_memory()\n",
    "\n",
    "    \n",
    "    # ----------------------------\n",
    "    # fit and test\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        # create datamodule\n",
    "        datamodule = CCDataModule(**data_hparams)\n",
    "        datamodule.setup()\n",
    "\n",
    "        # train the model\n",
    "        trainer.fit(model, datamodule)\n",
    "        \n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path=None)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        # refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true",
    "lines_to_next_cell": 1
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, learning_rate, dropout, model_type='MLP'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(15, 16)\n",
    "        self.fc_2 = nn.Linear(16, 1)\n",
    "        #self.fc_3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, car, car_stand, inflow, inflow_stand, \\\n",
    "        revision, revision_stand, manual_txt, fin_ratios = batch\n",
    "        # x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "    \n",
    "        x = fin_ratios\n",
    "        \n",
    "        x_car = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        y_car = self.fc_2(x_car) # (N, 1)    \n",
    "        \n",
    "        t_car = car_stand\n",
    "        \n",
    "        return transcriptid, y_car.squeeze(), t_car \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# choose Model\\nModel = CCMLP\\n\\n# data hparams\\ndata_hparams = {\\n    \\'targets_name\\': \\'targets_final\\', # key!\\n\\n    \\'batch_size\\': 64,\\n    \\'val_batch_size\\':64,\\n    \\'test_batch_size\\':64,\\n    \\n    \\'text_in_dataset\\': False,\\n    \\'window_size\\': \\'5y\\', # key!\\n}\\n\\n# model hparams\\nmodel_hparams = {\\n    \\'learning_rate\\': 1e-4,\\n    \\'dropout\\': 0.1,\\n}\\n\\n# train hparams\\ntrainer_hparams = {\\n    # random seed\\n    \\'seed\\': 42,    # key\\n    \\n    # gpus\\n    \\'gpus\\': [0,1], # key\\n\\n    # checkpoint & log\\n    \\n    # last: \\n    \\'machine\\': \\'yu-workstation\\', # key!\\n    \\'note\\': f\"MLP-01\", # key!\\n    \\'log_every_n_steps\\': 10,\\n    \\'save_top_k\\': 1,\\n    \\'val_check_interval\\': 1.0,\\n\\n    # data size\\n    \\'precision\\': 32, # key!\\n    \\'overfit_batches\\': 0.0,\\n    \\'min_epochs\\': 10, # default: 10\\n    \\'max_epochs\\': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\\n    \\'max_steps\\': None, # default None\\n    \\'accumulate_grad_batches\\': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    \\'early_stop_patience\\': 10, # default: 3\\n\\n    # Caution:\\n    # In pervious versions, if you check validatoin multiple times within a epoch,\\n    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \\n    # you check validation multiples times within an epoch, you still need to set\\n    # `checkpoint_period=1`.\\n    \\'checkpoint_period\\': 1} # default 1\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt()\\n\\n# load split_df\\nsplit_df = load_split_df(data_hparams[\\'window_size\\'])\\n    \\n# loop over windows\\nnp.random.seed(trainer_hparams[\\'seed\\'])\\ntorch.manual_seed(trainer_hparams[\\'seed\\'])\\n\\nfor yqtr in split_df.yqtr:\\n\\n    # Only test after 2012-q4\\n    if yqtr>=\\'2012-q4\\':\\n\\n        # update current period\\n        data_hparams.update({\\'yqtr\\': yqtr})\\n\\n        # train on select periods\\n        train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\\n        \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '6y', # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    \n",
    "    # last: \n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MLP-02\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\n",
    "    'max_steps': None, # default None\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 10, # default: 3\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1} # default 1\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "for yqtr in split_df.yqtr:\n",
    "\n",
    "    # Only test after 2012-q4\n",
    "    if yqtr>='2012-q4':\n",
    "\n",
    "        # update current period\n",
    "        data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "        # train on select periods\n",
    "        train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true",
    "lines_to_next_cell": 1
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# CCGRU\n",
    "class CCGRU(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # set model types\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'univariate'\n",
    "        self.model_type = 'gru'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # layers\n",
    "        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\n",
    "                                 dropout=0.1, bidirectional=True)\n",
    "        self.dropout_expert = nn.Dropout(hparams.dropout)\n",
    "        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, valid_seq_len):\n",
    "        # Note: inp is [N, S, E] and **already** been packed\n",
    "        self.gru_expert.flatten_parameters()\n",
    "        \n",
    "        # if S is longer than `max_seq_len`, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\n",
    "        \n",
    "        # RNN layers\n",
    "        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\n",
    "        \n",
    "        # final FC layers\n",
    "        y_car = self.linear_car(x_expert) # (N, E)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'trainer_loss': loss_car}}\n",
    "            \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}        \n",
    "    \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true toc-hr-collapsed=true"
   },
   "source": [
    "# STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt\n",
    "class CCTransformerSTLTxt(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, n_layers_encoder, dff, max_seq_len, model_type='STL', dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for CAR\n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.fc_2 = nn.Linear(32, 1)\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # forward\n",
    "    def shared_step(self, batch):\n",
    "        car, car_stand, inflow, inflow_stand, revision, revision_stand, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # decode with avgpool\n",
    "        x_expert = x_expert.mean(1) # (N, E)\n",
    "        \n",
    "        # decode with maxpool\n",
    "        # x_expert_maxpool = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        # concat\n",
    "        # x_expert = torch.cat([x_expert_avgpool, x_expert_maxpool], dim=-1) # (N, 2E)\n",
    "\n",
    "        # final FC\n",
    "        y_car = self.fc_1(x_expert) # (N, 1)\n",
    "        # y_car = self.fc_2(y_car)\n",
    "        \n",
    "        t_car = car_stand # (N,)\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid, y_car.squeeze(), t_car \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCTransformerSTLTxtFr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt + fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, d_model, learning_rate, attn_dropout, n_head_encoder, \n",
    "                 n_layers_encoder, dff, max_seq_len, model_type='STL', n_finratios=15, dropout=0.5):\n",
    "        '''\n",
    "        d_model: dimension of embedding. (default=1024)\n",
    "        dff: fully-connected layer inside the transformer block. (default=2048)\n",
    "        '''\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers \n",
    "        # self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        # self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        # txt_mixer_layer = TxtMixerLayer(self.hparams.d_model)\n",
    "        # self.txt_mixer = FeatureMixer(txt_mixer_layer, self.hparams.n_layers_txtmixer)\n",
    "        \n",
    "        # fr_mixer_layers = FrMixerLayer(self.n_covariate)\n",
    "        # self.fr_mixer = FeatureMixer(fr_mixer_layers, self.hparams.n_layers_frmixer)\n",
    "        \n",
    "        # final prediction layer\n",
    "        # final_fc_mixer_layer = FeatureMixerLayer(self.hparams.d_model+self.n_covariate)\n",
    "        # self.final_fc_mixer_layer = FeatureMixer(final_fc_mixer_layer, self.hparams.n_layers_finalfc)\n",
    "        # self.fc_batchnorm = nn.BatchNorm1d(self.hparams.d_model+self.n_covariate)\n",
    "        self.final_fc = nn.Linear(self.hparams.d_model+self.hparams.n_finratios, 1)\n",
    "        \n",
    "        # self.txt_fc_1 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.txt_fc_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        # self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        # self.fc_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        # self.txt_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_3 = nn.Dropout(self.hparams.dropout) \n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        car, car_stand, inflow, inflow_stand, revision, revision_stand, \\\n",
    "        transcriptid, embeddings, src_key_padding_mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        # x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        # x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        x_expert = x_expert.max(1)[0] # (N, E)\n",
    "        \n",
    "        \n",
    "        # project text embedding to a lower dimension\n",
    "        # x_expert = self.txt_dropout_1(F.relu(self.txt_fc_1(x_expert)))\n",
    "        # x_expert = F.relu(self.txt_fc_2(x_expert))\n",
    "        \n",
    "        # x_expert = self.txt_mixer(x_expert)\n",
    "        \n",
    "        # Mix fin_ratios\n",
    "        # fin_ratios = self.batch_stand(fin_ratios)\n",
    "        # x_fr = self.fr_mixer(fin_ratios)\n",
    "        \n",
    "        # concate `x_final` with `fin_ratios`\n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_finratios)\n",
    "        \n",
    "        # final FC\n",
    "        # x_final = self.fc_dropout_1(F.relu(self.fc_1(x_expert))) # (N, E+X)\n",
    "        # x_car = self.final_fc_mixer_layer(x_final) # (N, E+X)\n",
    "        y_car = self.final_fc(x_final)\n",
    "        \n",
    "        t_car = car_stand\n",
    "        \n",
    "        # final output\n",
    "        return transcriptid.squeeze(), y_car.squeeze(), t_car.squeeze() \n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 't_car': t_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, t_car = self.shared_step(batch)\n",
    "        return {'transcriptid':transcriptid, 'y_car':y_car, 't_car': t_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# choose Model\\nModel = CCTransformerSTLTxt\\n\\n# data hparams\\ndata_hparams = {\\n    # inputs\\n    \\'preembedding_name\\': \\'longformer\\', \\n    \\'targets_name\\': \\'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_stand_outlier\\', \\n    \\'tid_cid_pair_name\\': \\'qa\\', \\n    \\'tid_from_to_pair_name\\': \\'7qtr\\',\\n    \\n    # batch size\\n    \\'batch_size\\': 12,\\n    \\'val_batch_size\\':8,\\n    \\'test_batch_size\\':8,\\n    \\n    # window_size\\n    \\'text_in_dataset\\': True,\\n    \\'window_size\\': \\'2008-2017\\', # key!\\n}\\n\\n# hparams\\nmodel_hparams = {\\n    \\'max_seq_len\\': 768, \\n    \\'learning_rate\\':3e-4, # key!\\n    \\'n_layers_encoder\\': 4,\\n    \\'n_head_encoder\\': 8, \\n    \\'d_model\\': 768,\\n    \\'dff\\': 2048, # default: 2048\\n    \\'attn_dropout\\': 0.1,\\n    # \\'dropout\\': 0.5\\n} \\n\\n# train hparams\\ntrainer_hparams = {\\n    # random seed\\n    \\'seed\\': 42,    # key\\n    \\n    # gpus\\n    \\'gpus\\': [0,1], # key\\n\\n    # last: STL-57\\n    \\'machine\\': \\'yu-workstation\\', # key!\\n    \\'note\\': f\"STL-57\", # key!\\n    \\'log_every_n_steps\\': 10,\\n    \\'save_top_k\\': 1,\\n    \\'val_check_interval\\': 0.2, # key! (Eg: 0.25 - check 4 times in a epoch)\\n\\n    # epochs\\n    \\'precision\\': 32, # key!\\n    \\'overfit_batches\\': 0.0, # default 0.0. decimal or int\\n    \\'min_epochs\\': 3, # default: 3\\n    \\'max_epochs\\': 20, # default: 20\\n    \\'max_steps\\': None, # default: None\\n    \\'accumulate_grad_batches\\': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    \\'early_stop_patience\\': 8,\\n\\n    # Caution:\\n    # In pervious versions, if you check validatoin multiple times within a epoch,\\n    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \\n    # you check validation multiples times within an epoch, you still need to set\\n    # `checkpoint_period=1`.\\n    \\'checkpoint_period\\': 1}\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt()\\n\\n# load split_df\\nsplit_df = load_split_df(data_hparams[\\'window_size\\'])\\n\\n# load tid_cid_pair\\n# loop over windows!\\nfor yqtr in split_df.yqtr:\\n    np.random.seed(trainer_hparams[\\'seed\\'])\\n    torch.manual_seed(trainer_hparams[\\'seed\\'])\\n    \\n    # Enforce yqtr>=\\'2012-q4\\' (the earliest yqtr in window_size==\\'3y\\')\\n    # if yqtr == \\'non-roll-01\\':\\n\\n    # update current period\\n    data_hparams.update({\\'yqtr\\': yqtr})\\n\\n    # train on select periods\\n    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# choose Model\n",
    "Model = CCTransformerSTLTxt\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    # inputs\n",
    "    'preembedding_name': 'longformer', \n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_retail_sentiment_stand_outlier', \n",
    "    'tid_cid_pair_name': 'qa', \n",
    "    'tid_from_to_pair_name': '7qtr',\n",
    "    \n",
    "    # batch size\n",
    "    'batch_size': 12,\n",
    "    'val_batch_size':8,\n",
    "    'test_batch_size':8,\n",
    "    \n",
    "    # window_size\n",
    "    'text_in_dataset': True,\n",
    "    'window_size': '2008-2017', # key!\n",
    "}\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':3e-4, # key!\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 768,\n",
    "    'dff': 2048, # default: 2048\n",
    "    'attn_dropout': 0.1,\n",
    "    # 'dropout': 0.5\n",
    "} \n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # last: STL-57\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"STL-57\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2, # key! (Eg: 0.25 - check 4 times in a epoch)\n",
    "\n",
    "    # epochs\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0, # default 0.0. decimal or int\n",
    "    'min_epochs': 3, # default: 3\n",
    "    'max_epochs': 20, # default: 20\n",
    "    'max_steps': None, # default: None\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "# load tid_cid_pair\n",
    "# loop over windows!\n",
    "for yqtr in split_df.yqtr:\n",
    "    np.random.seed(trainer_hparams['seed'])\n",
    "    torch.manual_seed(trainer_hparams['seed'])\n",
    "    \n",
    "    # Enforce yqtr>='2012-q4' (the earliest yqtr in window_size=='3y')\n",
    "    # if yqtr == 'non-roll-01':\n",
    "\n",
    "    # update current period\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    # train on select periods\n",
    "    train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCMTLFr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMTLFr(CCMTL):\n",
    "    def __init__(self, learning_rate, dropout, alpha, model_type='MTL'):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(15, 16)\n",
    "        # self.fc_2 = nn.Linear(16, 16)\n",
    "        self.fc_car = nn.Linear(16, 1)\n",
    "        self.fc_rev = nn.Linear(16, 1)\n",
    "        self.fc_inf = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, car, car_stand, inflow, inflow_stand, \\\n",
    "        revision, revision_stand, manual_txt, fin_ratios = batch\n",
    "        \n",
    "        t_car = car_stand\n",
    "        t_rev = revision_stand\n",
    "        t_inf = inflow_stand\n",
    "\n",
    "        x = fin_ratios\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        # x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        return transcriptid, t_car, x\n",
    "    \n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, car, car_stand, inflow, inflow_stand, \\\n",
    "        revision, revision_stand, manual_txt, fin_ratios = batch\n",
    "        \n",
    "        t_car = car_stand\n",
    "        t_rev = revision_stand\n",
    "        t_inf = inflow_stand\n",
    "\n",
    "        # x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "        x = fin_ratios\n",
    "        \n",
    "        x = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        # x = self.dropout_2(F.relu(self.fc_2(x)))\n",
    "        y_car = self.fc_car(x) # (N, 1)    \n",
    "        y_rev = self.fc_rev(x) # (N, 1)\n",
    "        y_inf = self.fc_inf(x) # (N, 1)\n",
    "        \n",
    "        return transcriptid, y_car.squeeze(), y_rev.squeeze(), \\\n",
    "               y_inf.squeeze(), t_car, t_rev, t_inf \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, y_car, y_rev, y_inf, \\\n",
    "        t_car, t_rev, t_inf = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        transcriptid, y_car, y_rev, y_inf, \\\n",
    "        t_car, t_rev, t_inf = self.shared_step(batch)\n",
    "        return {'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf}\n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        transcriptid, y_car, y_rev, y_inf, \\\n",
    "        t_car, t_rev, t_inf = self.shared_step(batch)\n",
    "        return {'transcriptid': transcriptid,\n",
    "                'y_car': y_car, 'y_rev': y_rev, 'y_inf': y_inf,\n",
    "                't_car': t_car, 't_rev': t_rev, 't_inf': t_inf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# choose Model\\nModel = CCMTLFr\\n\\n# data hparams\\ndata_hparams = {\\n    \\'targets_name\\': \\'targets_final\\', # key!\\n\\n    \\'batch_size\\': 64,\\n    \\'val_batch_size\\':64,\\n    \\'test_batch_size\\':64,\\n    \\n    \\'text_in_dataset\\': False,\\n    \\'window_size\\': \\'6y\\', # key!\\n}\\n\\n# model hparams\\nmodel_hparams = {\\n    \\'alpha\\': 0.1, # key!\\n    \\'learning_rate\\': 1e-4,\\n    \\'dropout\\': 0.1, # default: 0.5\\n}\\n\\n# train hparams\\ntrainer_hparams = {\\n    # random seed\\n    \\'seed\\': 42,    # key\\n    \\n    # gpus\\n    \\'gpus\\': [0,1], # key\\n\\n    # checkpoint & log\\n    \\n    # last: MLP-24\\n    \\'machine\\': \\'yu-workstation\\', # key!\\n    \\'note\\': f\"MTL-14\", # key!\\n    \\'log_every_n_steps\\': 10,\\n    \\'save_top_k\\': 1,\\n    \\'val_check_interval\\': 1.0,\\n\\n    # data size\\n    \\'precision\\': 32, # key!\\n    \\'overfit_batches\\': 0.0,\\n    \\'min_epochs\\': 10, # default: 10\\n    \\'max_epochs\\': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\\n    \\'max_steps\\': None, # default None\\n    \\'accumulate_grad_batches\\': 1,\\n\\n    # Caution:\\n    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \\n    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\\n    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\\n    \\'early_stop_patience\\': 10, # default: 3\\n\\n    # Caution:\\n    # In pervious versions, if you check validatoin multiple times within a epoch,\\n    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \\n    # you check validation multiples times within an epoch, you still need to set\\n    # `checkpoint_period=1`.\\n    \\'checkpoint_period\\': 1} # default 1\\n\\n# delete all existing .ckpt files\\nrefresh_ckpt()\\n\\n# load split_df\\nsplit_df = load_split_df(data_hparams[\\'window_size\\'])\\n    \\n# loop over windows\\nnp.random.seed(trainer_hparams[\\'seed\\'])\\ntorch.manual_seed(trainer_hparams[\\'seed\\'])\\n\\nprint(f\\'Start training...{trainer_hparams[\"note\"]}\\')\\n\\nfor yqtr in split_df.yqtr:\\n\\n    # Only test after 2012-q4\\n    if yqtr>=\\'2012-q4\\':\\n\\n        # update current period\\n        data_hparams.update({\\'yqtr\\': yqtr})\\n\\n        # train on select periods\\n        train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '''\n",
    "# choose Model\n",
    "Model = CCMTLFr\n",
    "\n",
    "# data hparams\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '5y', # key!\n",
    "}\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'alpha': 0.1, # key!\n",
    "    'learning_rate': 1e-4,\n",
    "    'dropout': 0.1, # default: 0.5\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "trainer_hparams = {\n",
    "    # random seed\n",
    "    'seed': 42,    # key\n",
    "    \n",
    "    # gpus\n",
    "    'gpus': [0,1], # key\n",
    "\n",
    "    # checkpoint & log\n",
    "    \n",
    "    # last: MLP-24\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MTL-16\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, # default: 10\n",
    "    'max_epochs': 500, # default: 20. Must be larger enough to have at least one \"val_rmse is not in the top 1\"\n",
    "    'max_steps': None, # default None\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 10, # default: 3\n",
    "\n",
    "    # Caution:\n",
    "    # In pervious versions, if you check validatoin multiple times within a epoch,\n",
    "    # you have to set `check_point_period=0`. However, starting from 1.0.7, even if \n",
    "    # you check validation multiples times within an epoch, you still need to set\n",
    "    # `checkpoint_period=1`.\n",
    "    'checkpoint_period': 1} # default 1\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "    \n",
    "# loop over windows\n",
    "np.random.seed(trainer_hparams['seed'])\n",
    "torch.manual_seed(trainer_hparams['seed'])\n",
    "\n",
    "print(f'Start training...{trainer_hparams[\"note\"]}')\n",
    "\n",
    "for yqtr in split_df.yqtr:\n",
    "\n",
    "    # Only test after 2012-q4\n",
    "    if yqtr == '2013-q4':\n",
    "\n",
    "        # update current period\n",
    "        data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "        # train on select periods\n",
    "        train_one(Model, yqtr, data_hparams, model_hparams, trainer_hparams)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cRT with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N checkpoint found: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70c04c0dea3453cbe8a98ea715fddce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2013-q4 (6y) \n",
      "(train: 2008-01-01 to 2013-12-31) (test: 2014-01-01 to 2014-03-31)\n",
      "N train = 7884\n",
      "N val = 876\n",
      "N train+val = 8760\n",
      "N test = 374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2014-q1 (6y) \n",
      "(train: 2008-04-01 to 2014-03-31) (test: 2014-04-01 to 2014-06-30)\n",
      "N train = 7933\n",
      "N val = 882\n",
      "N train+val = 8815\n",
      "N test = 393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2014-q2 (6y) \n",
      "(train: 2008-07-01 to 2014-06-30) (test: 2014-07-01 to 2014-09-30)\n",
      "N train = 7962\n",
      "N val = 885\n",
      "N train+val = 8847\n",
      "N test = 395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2014-q3 (6y) \n",
      "(train: 2008-10-01 to 2014-09-30) (test: 2014-10-01 to 2014-12-31)\n",
      "N train = 7991\n",
      "N val = 888\n",
      "N train+val = 8879\n",
      "N test = 396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2014-q4 (6y) \n",
      "(train: 2009-01-01 to 2014-12-31) (test: 2015-01-01 to 2015-03-31)\n",
      "N train = 8029\n",
      "N val = 893\n",
      "N train+val = 8922\n",
      "N test = 386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2015-q1 (6y) \n",
      "(train: 2009-04-01 to 2015-03-31) (test: 2015-04-01 to 2015-06-30)\n",
      "N train = 8077\n",
      "N val = 898\n",
      "N train+val = 8975\n",
      "N test = 404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2015-q2 (6y) \n",
      "(train: 2009-07-01 to 2015-06-30) (test: 2015-07-01 to 2015-09-30)\n",
      "N train = 8136\n",
      "N val = 904\n",
      "N train+val = 9040\n",
      "N test = 403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2015-q3 (6y) \n",
      "(train: 2009-10-01 to 2015-09-30) (test: 2015-10-01 to 2015-12-31)\n",
      "N train = 8159\n",
      "N val = 907\n",
      "N train+val = 9066\n",
      "N test = 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2015-q4 (6y) \n",
      "(train: 2010-01-01 to 2015-12-31) (test: 2016-01-01 to 2016-03-31)\n",
      "N train = 8191\n",
      "N val = 911\n",
      "N train+val = 9102\n",
      "N test = 394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2016-q1 (6y) \n",
      "(train: 2010-04-01 to 2016-03-31) (test: 2016-04-01 to 2016-06-30)\n",
      "N train = 8226\n",
      "N val = 914\n",
      "N train+val = 9140\n",
      "N test = 406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2016-q2 (6y) \n",
      "(train: 2010-07-01 to 2016-06-30) (test: 2016-07-01 to 2016-09-30)\n",
      "N train = 8257\n",
      "N val = 918\n",
      "N train+val = 9175\n",
      "N test = 401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2016-q3 (6y) \n",
      "(train: 2010-10-01 to 2016-09-30) (test: 2016-10-01 to 2016-12-31)\n",
      "N train = 8276\n",
      "N val = 920\n",
      "N train+val = 9196\n",
      "N test = 403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2016-q4 (6y) \n",
      "(train: 2011-01-01 to 2016-12-31) (test: 2017-01-01 to 2017-03-31)\n",
      "N train = 8306\n",
      "N val = 923\n",
      "N train+val = 9229\n",
      "N test = 401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2017-q1 (6y) \n",
      "(train: 2011-04-01 to 2017-03-31) (test: 2017-04-01 to 2017-06-30)\n",
      "N train = 8339\n",
      "N val = 927\n",
      "N train+val = 9266\n",
      "N test = 414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2017-q2 (6y) \n",
      "(train: 2011-07-01 to 2017-06-30) (test: 2017-07-01 to 2017-09-30)\n",
      "N train = 8379\n",
      "N val = 931\n",
      "N train+val = 9310\n",
      "N test = 404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2017-q3 (6y) \n",
      "(train: 2011-10-01 to 2017-09-30) (test: 2017-10-01 to 2017-12-31)\n",
      "N train = 8405\n",
      "N val = 934\n",
      "N train+val = 9339\n",
      "N test = 402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2017-q4 (6y) \n",
      "(train: 2012-01-01 to 2017-12-31) (test: 2018-01-01 to 2018-03-31)\n",
      "N train = 8431\n",
      "N val = 937\n",
      "N train+val = 9368\n",
      "N test = 386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2018-q1 (6y) \n",
      "(train: 2012-04-01 to 2018-03-31) (test: 2018-04-01 to 2018-06-30)\n",
      "N train = 8459\n",
      "N val = 940\n",
      "N train+val = 9399\n",
      "N test = 405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2018-q2 (6y) \n",
      "(train: 2012-07-01 to 2018-06-30) (test: 2018-07-01 to 2018-09-30)\n",
      "N train = 8487\n",
      "N val = 943\n",
      "N train+val = 9430\n",
      "N test = 417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: 2018-q3 (6y) \n",
      "(train: 2012-10-01 to 2018-09-30) (test: 2018-10-01 to 2018-12-31)\n",
      "N train = 8514\n",
      "N val = 947\n",
      "N train+val = 9461\n",
      "N test = 418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CCMTLFr(\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (fc_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "  (fc_car): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_rev): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (fc_inf): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"yt_extractor.feather\" (3.9 MB) loaded as \"old_yt_extractor\" (<1s)\n",
      "\"all_yt_extractor\" saved as \"yt_extractor.feather\" (4.1 MB) (<1s)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# ----------------------------\n",
    "# Specify Model and data\n",
    "# ----------------------------\n",
    "Model = CCMTLFr\n",
    "\n",
    "ckpt_name = 'MTL-14'\n",
    "ckpt_paths = [path for path in os.listdir(f'{CHECKPOINT_TEMP_DIR}')\n",
    "              if path.startswith(ckpt_name+'_')]\n",
    "print(f'N checkpoint found: {len(ckpt_paths)}')\n",
    "\n",
    "# load data\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '6y', # key!\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Extract\n",
    "# ----------------------------\n",
    "def extract(model, dataloader):\n",
    "    # Extract y, x using model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        transcriptid = []\n",
    "        features = []\n",
    "        t_car = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            tid, car, f = model.forward(batch)\n",
    "            transcriptid.append(tid)\n",
    "            t_car.append(car)\n",
    "            features.append(f)\n",
    "\n",
    "        transcriptid = dt.Frame(transcriptid=torch.cat(transcriptid).numpy())\n",
    "        t_car = dt.Frame(t_car=torch.cat(t_car).numpy())\n",
    "        features = dt.Frame(torch.cat(features, dim=0).numpy())\n",
    "        features.names = [n.replace('C', 'feature') for n in features.names]\n",
    "\n",
    "        targets = dt.cbind([transcriptid, t_car, features])\n",
    "\n",
    "        t, x = dmatrices(f't_car ~ {\"+\".join(features.names)}',\n",
    "                         data=targets,\n",
    "                         return_type='dataframe')\n",
    "        \n",
    "        return transcriptid['transcriptid'].to_list()[0], t, x\n",
    "\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "yt_extractor = []\n",
    "for yqtr in tqdm(split_df.yqtr):\n",
    "    \n",
    "    if yqtr<'2012-q4':\n",
    "        continue\n",
    "        \n",
    "    # load train/test data\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    datamodule = CCDataModule(**data_hparams)\n",
    "    datamodule.setup()\n",
    "    \n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "    \n",
    "    # load model\n",
    "    ckpt_path = [path for path in ckpt_paths \n",
    "                 if path.startswith(f'{ckpt_name}_{yqtr}')]\n",
    "    assert len(ckpt_path)==1, f'Multiple or no checkpoint found for \"{ckpt_name}_{yqtr}\"'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "    \n",
    "    model = Model.load_from_checkpoint(f'{CHECKPOINT_TEMP_DIR}/{ckpt_path}')\n",
    "    model.eval()\n",
    "    \n",
    "    # extract\n",
    "    import statsmodels.api as sm\n",
    "    from patsy import dmatrices\n",
    "\n",
    "    _, t_train, x_train = extract(model, train_dataloader)\n",
    "    transcriptid, t_test, x_test = extract(model, test_dataloader)\n",
    "        \n",
    "    # Fit OLS on Train\n",
    "    fitted = sm.OLS(t_train, x_train).fit()\n",
    "    \n",
    "    # Apply OLS on Test\n",
    "    y_test = fitted.predict(x_test).to_list()\n",
    "    t_test = t_test['t_car'].to_list()\n",
    "    \n",
    "    df = dt.Frame(transcriptid=transcriptid,\n",
    "                  t_car=t_test,\n",
    "                  y_car=y_test)\n",
    "    df[:, update(model_name=ckpt_name+'_extractor',\n",
    "                 window_size=data_hparams['window_size'],\n",
    "                 yqtr=yqtr)]\n",
    "    \n",
    "    yt_extractor.append(df)\n",
    "    \n",
    "yt_extractor = dt.rbind(yt_extractor)\n",
    "\n",
    "# Combine\n",
    "ld('yt_extractor', 'old_yt_extractor', force=True)\n",
    "all_yt_extractor = dt.rbind([yt_extractor, old_yt_extractor])\n",
    "\n",
    "sv('all_yt_extractor', 'yt_extractor')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cRT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ----------------------------\n",
    "# Specify Model and data\n",
    "# ----------------------------\n",
    "class cRT(CC):\n",
    "    def __init__(self, learning_rate, extractor):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "        self.extractor = extractor\n",
    "        self.fc_1 = nn.Linear(16,1)\n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "        \n",
    "    def shared_step(self, batch):\n",
    "        transcriptid, t_car, x = self.extractor(batch)\n",
    "        y = self.dropout_1(F.relu(self.fc_1(x)))\n",
    "        return transcriptid, t_car, y\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        transcriptid, t_car, y = self.shared_step(batch) \n",
    "        return transcriptid, t_car, y\n",
    "        \n",
    "    def training_step(self, batch, idx):\n",
    "        transcriptid, t_car, y = self.shared_step(batch)\n",
    "        return {'y':y, 't':t_car, 'transcriptid':transcriptid}\n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        transcriptid = outputs['transcriptid']\n",
    "        y = outputs['y']\n",
    "        t = outputs['t']\n",
    "        \n",
    "        loss = self.mse_loss(y,t)\n",
    "        return {'loss':loss}\n",
    "        \n",
    "\n",
    "ckpt_name = 'MTL-11'\n",
    "ckpt_paths = [path for path in os.listdir(f'{CHECKPOINT_TEMP_DIR}')\n",
    "              if path.startswith(ckpt_name+'_')]\n",
    "print(f'N checkpoint found: {len(ckpt_paths)}')\n",
    "\n",
    "# load data\n",
    "data_hparams = {\n",
    "    'targets_name': 'targets_final', # key!\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size':64,\n",
    "    'test_batch_size':64,\n",
    "    \n",
    "    'text_in_dataset': False,\n",
    "    'window_size': '6y', # key!\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Extract\n",
    "# ----------------------------\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        transcriptid = []\n",
    "        y_car = []\n",
    "        t_car = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            tid, t, y = model.forward(batch)\n",
    "            transcriptid.append(tid)\n",
    "            t_car.append(t)\n",
    "            y_car.append(y)\n",
    "            \n",
    "        transcriptid = dt.Frame(transcriptid=torch.cat(transcriptid).numpy())\n",
    "        t_car = dt.Frame(t_car=torch.cat(t_car).numpy())\n",
    "        y_car = dt.Frame(torch.cat(y_car, dim=0).numpy())\n",
    "\n",
    "        targets = dt.cbind([transcriptid, t_car, features])\n",
    "\n",
    "        \n",
    "        return targets\n",
    "\n",
    "split_df = load_split_df(data_hparams['window_size'])\n",
    "\n",
    "yt_extractor = []\n",
    "for yqtr in tqdm(split_df.yqtr):\n",
    "    \n",
    "    if yqtr<'2012-q4':\n",
    "        continue\n",
    "        \n",
    "    # load model\n",
    "    ckpt_path = [path for path in ckpt_paths \n",
    "                 if path.startswith(f'{ckpt_name}_{yqtr}')]\n",
    "    assert len(ckpt_path)==1, \\\n",
    "           f'Multiple or no checkpoint found for \"{ckpt_name}_{yqtr}\"'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "    \n",
    "    extractor = CCMTLFr.load_from_checkpoint(f'{CHECKPOINT_TEMP_DIR}/{ckpt_path}')\n",
    "    extractor.eval()\n",
    "        \n",
    "    # load train/test data\n",
    "    data_hparams.update({'yqtr': yqtr})\n",
    "\n",
    "    datamodule = CCDataModule(**data_hparams)\n",
    "    datamodule.setup()\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "    \n",
    "    # Retrain classifier\n",
    "    classrt = cRT(learning_rate=1e-4,\n",
    "                  extractor=extractor)\n",
    "    logger = CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=data_hparams['yqtr'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "    logger.experiment.log_parameters({'note':'cRT-test'})\n",
    "    trainer = pl.Trainer(gpus=[1], accelerator='dp',\n",
    "                         max_epochs=20)\n",
    "    trainer.fit(classrt, datamodule)\n",
    "    \n",
    "    # predict\n",
    "    df = predict(classrt, test_dataloader)\n",
    "        \n",
    "    df[:, update(model_name=ckpt_name+'_fcextractor',\n",
    "                 window_size=data_hparams['window_size'],\n",
    "                 yqtr=yqtr)]\n",
    "    \n",
    "    yt_extractor.append(df)\n",
    "    \n",
    "yt_extractor = dt.rbind(yt_extractor)\n",
    "\n",
    "# Combine\n",
    "# ld('yt_extractor', 'old_yt_extractor', force=True)\n",
    "# all_yt_extractor = dt.rbind([yt_extractor, old_yt_extractor])\n",
    "\n",
    "# sv('all_yt_extractor', 'yt_extractor')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
