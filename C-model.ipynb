{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: .\n",
      "DATA_DIR: ./data\n",
      "CHECKPOINT_DIR: d:/checkpoints/earnings-call\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 11.00 (GB)\n",
      "GPU 1:  0.00/ 11.00 (GB)\n",
      "\n",
      "CPU count (physical): 16\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import comet_ml\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import sentence_transformers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from spacy.lang.en import English\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer, RobertaTokenizer, RobertaModel, XLNetTokenizer, XLNetModel\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = 'd:/checkpoints/earnings-call'\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');\n",
    "    \n",
    "cpu = torch.device('cpu');\n",
    "n_cpu = int(mp.cpu_count()/2);\n",
    "\n",
    "print(f'\\nCPU count (physical): {n_cpu}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=cpu:\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name):\n",
    "    if 'targets_df' not in globals():\n",
    "        print(f'Loading targets...@{Now()}')\n",
    "        globals()['targets_df'] = pd.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_type):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_type}.pt\")\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    globals()['split_df'] = split_df.loc[split_df.roll_type==roll_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, window_i, model_hparams, train_hparams):\n",
    "    \n",
    "    # set window\n",
    "    model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "    \n",
    "    # init model\n",
    "    model = Model(model_hparams)\n",
    "\n",
    "    # get model type\n",
    "    train_hparams['task_type'] = model.task_type\n",
    "    train_hparams['feature_type'] = model.feature_type\n",
    "    train_hparams['model_type'] = model.model_type\n",
    "    train_hparams['attn_type'] = model.attn_type\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{train_hparams['model_type']}_{model_hparams['window']}_\"\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        monitor='val_loss',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=train_hparams['save_top_k'],\n",
    "        period=train_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=model_hparams['window'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_hparams['early_stop_patience'],\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=-1, \n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         early_stop_callback=early_stop_callback,\n",
    "                         overfit_batches=train_hparams['overfit_batches'], \n",
    "                         row_log_interval=train_hparams['row_log_interval'],\n",
    "                         val_check_interval=train_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=2, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=train_hparams['min_epochs'],\n",
    "                         max_epochs=train_hparams['max_epochs'], \n",
    "                         max_steps=train_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # delete unused hparam\n",
    "    if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "    if model.feature_type=='fin-ratio': \n",
    "        model_hparams.pop('max_seq_len',None)\n",
    "        model_hparams.pop('n_layers_encoder',None)\n",
    "        model_hparams.pop('n_head_encoder',None)\n",
    "        model_hparams.pop('d_model',None)\n",
    "        model_hparams.pop('dff',None)\n",
    "    if model.feature_type=='text': \n",
    "        model_hparams.pop('normalize_layer',None)\n",
    "        model_hparams.pop('normalize_batch',None)\n",
    "    if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "    # add n_model_params\n",
    "    train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(train_hparams)\n",
    "\n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # train the model\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `val` and `train` are of same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, preembeddings, targets_df, split_df, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "        '''\n",
    "\n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        docid = targets.docid\n",
    "        \n",
    "        sue = targets.sue\n",
    "        sest = targets.sest\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        \n",
    "        alpha = targets.alpha\n",
    "        volatility = targets.volatility\n",
    "        mcap = targets.mcap/1e6\n",
    "        bm = targets.bm\n",
    "        roa = targets.roa\n",
    "        debt_asset = targets.debt_asset\n",
    "        numest = targets.numest\n",
    "        smedest = targets.smedest\n",
    "        sstdest = targets.sstdest\n",
    "        car_m1_m1 = targets.car_m1_m1\n",
    "        car_m2_m2 = targets.car_m2_m2\n",
    "        car_m30_m3 = targets.car_m30_m3\n",
    "        volume = targets.volume\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "            \n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, alpha, car_m1_m1, car_m2_m2, car_m30_m3, \\\n",
    "                   sest, sue, numest, sstdest, smedest, \\\n",
    "                   mcap, roa, bm, debt_asset, volatility, volume\n",
    "        else:\n",
    "            return docid, \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        \n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "\n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        return {'val_loss': mse, 'log': {'val_rmse': rmse}}\n",
    "    \n",
    "    # test step\n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "\n",
    "        return {'test_loss': mse, 'log': {'test_rmse': rmse}, 'progress_bar':{'test_rmse': rmse}}\n",
    "    \n",
    "    # Dataset\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.hparams.window, split_type='train', text_in_dataset=self.text_in_dataset,\n",
    "                                       roll_type=self.hparams.roll_type, print_window=True, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "        self.val_dataset = CCDataset(self.hparams.window, split_type='val', text_in_dataset=self.text_in_dataset,\n",
    "                                     roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "        self.test_dataset = CCDataset(self.hparams.window, split_type='test', text_in_dataset=self.text_in_dataset, \n",
    "                                      roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, transcriptid, embeddings, alpha, car_m1_m1, car_m2_m2, car_m30_m3, \\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = zip(*data)\n",
    "            \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "\n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(transcriptid, dtype=torch.float32), \\\n",
    "               embeddings.float(), mask, \\\n",
    "               torch.tensor(alpha, dtype=torch.float32), torch.tensor(car_m1_m1, dtype=torch.float32), \\\n",
    "               torch.tensor(car_m2_m2, dtype=torch.float32), torch.tensor(car_m30_m3, dtype=torch.float32), \\\n",
    "               torch.tensor(sest, dtype=torch.float32), torch.tensor(sue, dtype=torch.float32), \\\n",
    "               torch.tensor(numest, dtype=torch.float32), torch.tensor(sstdest, dtype=torch.float32), \\\n",
    "               torch.tensor(smedest, dtype=torch.float32), torch.tensor(mcap, dtype=torch.float32), \\\n",
    "               torch.tensor(roa, dtype=torch.float32), torch.tensor(bm, dtype=torch.float32), \\\n",
    "               torch.tensor(debt_asset, dtype=torch.float32), torch.tensor(volatility, dtype=torch.float32), \\\n",
    "               torch.tensor(volume, dtype=torch.float32), torch.tensor(inflow, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32)\n",
    "        \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, hparams):\n",
    "        hparams = Namespace(**hparams)\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # attibutes\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'fin-ratio'\n",
    "        self.model_type = 'mlp'\n",
    "        self.attn_type = 'dotprod'\n",
    "        \n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout1 = nn.Dropout(hparams.dropout)\n",
    "        self.dropout2 = nn.Dropout(hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.linear1 = nn.Linear(15, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "        \n",
    "        # batch normalization\n",
    "        if hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(15)\n",
    "            \n",
    "        # layer normalization\n",
    "        if hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(15)\n",
    "            \n",
    "    # forward\n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            inp = self.batch_norm(inp)\n",
    "            \n",
    "        # layer norm\n",
    "        if self.hparams.normalize_layer:\n",
    "            inp = self.layer_norm(inp)\n",
    "        \n",
    "        x_car = self.dropout1(F.relu(self.linear1(inp)))\n",
    "        x_car = self.dropout2(F.relu(self.linear2(x_car)))\n",
    "        y_car = self.linear3(x_car) # (N, 1)\n",
    "        \n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}    \n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/00b6ff7b94fe4400ae1c611a55d3f00b\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | dropout1   | Dropout     | 0     \n",
      "1 | dropout2   | Dropout     | 0     \n",
      "2 | linear1    | Linear      | 1 K   \n",
      "3 | linear2    | Linear      | 4 K   \n",
      "4 | linear3    | Linear      | 65    \n",
      "5 | batch_norm | BatchNorm1d | 30    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (3y) \n",
      "(train: 2008-01-01 to 2010-12-31) (test: 2011-01-01 to 2011-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f7f967ee27482594f45fa673daa895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 240.36601 (best 240.36601), saving model to d:/checkpoints/earnings-call\\mlp_roll-01_epoch=0.ckpt as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 237.84189 (best 237.84189), saving model to d:/checkpoints/earnings-call\\mlp_roll-01_epoch=1.ckpt as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss reached 233.98634 (best 233.98634), saving model to d:/checkpoints/earnings-call\\mlp_roll-01_epoch=2.ckpt as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: Still uploading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (3y) \n",
      "(train: 2008-01-01 to 2010-12-31) (test: 2011-01-01 to 2011-03-31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/00b6ff7b94fe4400ae1c611a55d3f00b\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1172476f4747e9aad256f32c4e18db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_loss': tensor(107.5705, device='cuda:0'),\n",
      " 'test_rmse': tensor(10.3716, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm', # key!\n",
    "    'normalize_batch': True, # key!\n",
    "    'normalize_layer': False, # key!\n",
    "    'normalize_target': True, # key!\n",
    "    'roll_type': '3y',\n",
    "    'batch_size': 128, # 128\n",
    "    'val_batch_size':2,\n",
    "    'learning_rate': 5e-4,\n",
    "    'task_weight': 1,\n",
    "    'dropout': 0.5,\n",
    "} # optional\n",
    "\n",
    "# train hparams\n",
    "train_hparams = {\n",
    "    # checkpoint & log\n",
    "    'note': '3y-MLP-car', # key!\n",
    "    'row_log_interval': 1,\n",
    "    'save_top_k': 3,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3, # (6)\n",
    "    'max_epochs': 3, # defaults: 15\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 3, # (6)\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "# you have to do this because CCDataset requires it\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df))[0:1]:\n",
    "\n",
    "    # train one window\n",
    "    train_one(Model, window_i, model_hparams, train_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true toc-hr-collapsed=true toc-nb-collapsed=true",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCGRU\n",
    "class CCGRU(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # set model types\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'univariate'\n",
    "        self.model_type = 'gru'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # layers\n",
    "        self.gru_expert = nn.GRU(hparams.d_model, hparams.rnn_hidden_size, num_layers=4, batch_first=True,\n",
    "                                 dropout=0.1, bidirectional=True)\n",
    "        self.dropout_expert = nn.Dropout(hparams.dropout)\n",
    "        self.linear_car = nn.Linear(hparams.rnn_hidden_size*2, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, valid_seq_len):\n",
    "        # Note: inp is [N, S, E] and **already** been packed\n",
    "        self.gru_expert.flatten_parameters()\n",
    "        \n",
    "        # if S is longer than `max_seq_len`, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        valid_seq_len[valid_seq_len>self.hparams.max_seq_len] = self.hparams.max_seq_len # (N,)\n",
    "        \n",
    "        # RNN layers\n",
    "        inp = pack_padded_sequence(inp, valid_seq_len, batch_first=True, enforce_sorted=False)\n",
    "        x_expert = pad_packed_sequence(self.gru_expert(inp)[0], batch_first=True)[0][:,-1,:] # (N, E)\n",
    "        \n",
    "        # final FC layers\n",
    "        y_car = self.linear_car(x_expert) # (N, E)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "            \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}        \n",
    "    \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility = batch\n",
    "        \n",
    "        # get valid seq_len\n",
    "        valid_seq_len = torch.sum(~mask, -1)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, valid_seq_len) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over 24 windows\n",
    "load_split_df()\n",
    "load_targets()\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key\n",
    "    'batch_size': 8, # key\n",
    "    'val_batch_size': 1, # key\n",
    "    \n",
    "    'max_seq_len': 1024, \n",
    "    'learning_rate': 3e-4,\n",
    "    'task_weight': 1,\n",
    "\n",
    "    'n_layers_encoder': 6,\n",
    "    'n_head_encoder': 8, # optional\n",
    "    'd_model': 1024,\n",
    "    'rnn_hidden_size': 64,\n",
    "    'final_tdim': 1024, # optional\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} # optional\n",
    "\n",
    "# train hparams\n",
    "train_hparams = {\n",
    "    # checkpoint & log\n",
    "    'note': 'temp',\n",
    "    'checkpoint_path': 'D:\\Checkpoints\\earnings-call',\n",
    "    'row_log_interval': 1,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.25,\n",
    "\n",
    "    # data size\n",
    "    'overfit_pct': 1,\n",
    "    'min_epochs': 0,\n",
    "    'max_epochs': 1,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 5,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt(train_hparams['checkpoint_path'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df)):\n",
    "    # load preembeddings\n",
    "    load_preembeddings(model_hparams['preembedding_type'])\n",
    "\n",
    "    # train one window\n",
    "    train_one(CCGRU, window_i, model_hparams, train_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL-text-MLP\n",
    "class CCTransformerSTLTxt(CC):\n",
    "        \n",
    "    def __init__(self, hparams, preembeddings, targets_df, split_df):\n",
    "        super().__init__(hparams, preembeddings, targets_df, split_df)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # specify model type\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'text'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.model_type = 'transformer'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(hparams.d_model, hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(hparams.d_model, hparams.n_head_encoder, hparams.dff, hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for SUE, CAR, SELEAD, SEST\n",
    "        self.attn_layers_car = nn.Linear(hparams.d_model, 1)\n",
    "        self.attn_dropout = nn.Dropout(hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(hparams.d_model, hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(hparams.d_model, hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(hparams.final_tdim, 1)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(hparams.dropout)\n",
    "        self.dropout_2 = nn.Dropout(hparams.dropout)\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, inp, src_key_padding_mask):\n",
    "        bsz, embed_dim = inp.size(0), inp.size(2)\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        inp = inp.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(inp) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        x_attn = self.attn_dropout(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        y_car = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # final linear layer\n",
    "        y_car = self.dropout_1(F.relu(self.linear_car_1(y_car)))\n",
    "        y_car = self.dropout_2(F.relu(self.linear_car_2(y_car)))\n",
    "        y_car = self.linear_car_3(y_car) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL-text-fr-MLP\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # specify model type\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'text + fin-ratio'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.model_type = 'transformer'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(hparams.d_model, hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(hparams.d_model, hparams.n_head_encoder, hparams.dff, hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for SUE, CAR, SELEAD, SEST\n",
    "        self.attn_layers_car = nn.Linear(hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(hparams.d_model, hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(hparams.d_model, hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(hparams.final_tdim+self.n_covariate, hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_4 = nn.Linear(hparams.final_tdim+self.n_covariate, hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_5 = nn.Linear(hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.final_dropout_1 = nn.Dropout(hparams.dropout)\n",
    "        self.final_dropout_2 = nn.Dropout(hparams.dropout)\n",
    "        self.final_dropout_3 = nn.Dropout(hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, src_key_padding_mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                smedest, mcap, roa, bm, debt_asset, volatility, volume):\n",
    "        \n",
    "        bsz, embed_dim = inp.size(0), inp.size(2)\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        inp = inp.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(inp) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # mix with covariate\n",
    "        x_expert = self.final_dropout_1(F.relu(self.linear_car_1(x_expert))) # (N, E)\n",
    "        x_expert = F.relu(self.linear_car_2(x_expert)) # (N, final_tdim)\n",
    "        \n",
    "        fin_ratio = torch.cat([alpha.unsqueeze(-1), car_m1_m1.unsqueeze(-1), car_m2_m2.unsqueeze(-1),\n",
    "                               car_m30_m3.unsqueeze(-1), sest.unsqueeze(-1), sue.unsqueeze(-1), numest.unsqueeze(-1),\n",
    "                               sstdest.unsqueeze(-1), smedest.unsqueeze(-1), mcap.unsqueeze(-1), roa.unsqueeze(-1), bm.unsqueeze(-1),\n",
    "                               debt_asset.unsqueeze(-1), volatility.unsqueeze(-1), volume.unsqueeze(-1)], dim=-1) # (N, X)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratio = self.batch_norm(fin_ratio)\n",
    "        \n",
    "        x_final = torch.cat([x_expert, fin_ratio], dim=-1) # (N, X + final_tdim) where X is the number of covariate (n_covariate)\n",
    "\n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            x_final = self.layer_norm(x_final)\n",
    "            \n",
    "        # final FC\n",
    "        x_final = self.final_dropout_2(F.relu(self.linear_car_3(x_final))) # (N, X + final_tdim)\n",
    "        x_final = self.final_dropout_3(F.relu(self.linear_car_4(x_final))) # (N, X + final_tdim)\n",
    "        y_car = self.linear_car_5(x_final) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest,\\\n",
    "                             mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL-test\n",
    "class CCTransformerSTLTest(CC):\n",
    "    def __init__(self, hparams, preembeddings, targets_df, split_df):\n",
    "        super().__init__(hparams, preembeddings, targets_df, split_df)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # specify model type\n",
    "        self.task_type = 'single'\n",
    "        self.feature_type = 'text + fin-ratio'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.model_type = 'transformer'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        self.final_linear = nn.Linear(self.n_covariate, 1)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                smedest, mcap, roa, bm, debt_asset, volatility, volume):\n",
    "        \n",
    "        \n",
    "        fin_ratio = torch.cat([alpha.unsqueeze(-1), car_m1_m1.unsqueeze(-1), car_m2_m2.unsqueeze(-1),\n",
    "                               car_m30_m3.unsqueeze(-1), sest.unsqueeze(-1), sue.unsqueeze(-1), numest.unsqueeze(-1),\n",
    "                               sstdest.unsqueeze(-1), smedest.unsqueeze(-1), mcap.unsqueeze(-1), roa.unsqueeze(-1), bm.unsqueeze(-1),\n",
    "                               debt_asset.unsqueeze(-1), volatility.unsqueeze(-1), volume.unsqueeze(-1)], dim=-1) # (N, X)\n",
    "        \n",
    "            \n",
    "        y_car = self.final_linear(fin_ratio) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, transcriptid, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, transcriptid, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest,\\\n",
    "                             mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTL-text-fr-Inf\n",
    "class CCTransformerMTLInf(CC):\n",
    "    def __init__(self, hparams, preembeddings, targets_df, split_df):\n",
    "        super().__init__(hparams, preembeddings, targets_df, split_df)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # specify model type\n",
    "        self.task_type = 'mtl'\n",
    "        self.feature_type = 'text + fin-ratio + (inf + rev)'\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.model_type = 'transformer'\n",
    "        self.text_in_dataset = True if self.feature_type!='fin-ratio' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(hparams.d_model, hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(hparams.d_model, hparams.n_head_encoder, hparams.dff, hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(hparams.d_model, hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(hparams.d_model, hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(hparams.final_tdim+self.n_covariate, hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_4 = nn.Linear(hparams.final_tdim+self.n_covariate, hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_5 = nn.Linear(hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        self.linear_inflow = nn.Linear(hparams.final_tdim, 1)\n",
    "        # self.linear_revision = nn.Linear(hparam.final_tdim, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.final_dropout_1 = nn.Dropout(hparams.dropout)\n",
    "        self.final_dropout_2 = nn.Dropout(hparams.dropout)\n",
    "        self.final_dropout_3 = nn.Dropout(hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, inp, src_key_padding_mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                smedest, mcap, roa, bm, debt_asset, volatility, volume):\n",
    "        \n",
    "        bsz, embed_dim = inp.size(0), inp.size(2)\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        inp = inp[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        inp = inp.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(inp) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # multiply with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # mix with covariate\n",
    "        x_expert = self.final_dropout_1(F.relu(self.linear_car_1(x_expert))) # (N, E)\n",
    "        x_expert = F.relu(self.linear_car_2(x_expert)) # (N, final_tdim)\n",
    "        \n",
    "        fin_ratio = torch.cat([alpha.unsqueeze(-1), car_m1_m1.unsqueeze(-1), car_m2_m2.unsqueeze(-1),\n",
    "                               car_m30_m3.unsqueeze(-1), sest.unsqueeze(-1), sue.unsqueeze(-1), numest.unsqueeze(-1),\n",
    "                               sstdest.unsqueeze(-1), smedest.unsqueeze(-1), mcap.unsqueeze(-1), roa.unsqueeze(-1), bm.unsqueeze(-1),\n",
    "                               debt_asset.unsqueeze(-1), volatility.unsqueeze(-1), volume.unsqueeze(-1)], dim=-1) # (N, X)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratio = self.batch_norm(fin_ratio)\n",
    "        \n",
    "        x_car = torch.cat([x_expert, fin_ratio], dim=-1) # (N, X + final_tdim) where X is the number of covariate (n_covariate)\n",
    "\n",
    "            \n",
    "        # final FC\n",
    "        y_inflow = self.linear_inflow(x_expert)\n",
    "        # y_revision = self.linear_revision(x_expert)\n",
    "        \n",
    "        x_car = self.final_dropout_2(F.relu(self.linear_car_3(x_car))) # (N, X + final_tdim)\n",
    "        y_car = self.linear_car_5(x_car) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_inflow\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                             smedest, mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss = loss_car + loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "        sest, sue, numest, sstdest, smedest, \\\n",
    "        mcap, roa, bm, debt_asset, volatility, volume, inflow, revision = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = sue.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest,\\\n",
    "                             mcap, roa, bm, debt_asset, volatility, volume) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss = loss_car + loss_inflow\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose Model\n",
    "Model = CCTransformerMTLInf\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text', # key!\n",
    "    'roll_type': '3y',  # key!\n",
    "    'batch_size': 20,\n",
    "    'val_batch_size': 4,\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate': 3e-4,\n",
    "    'task_weight': 1,\n",
    "    'normalize_layer': False, # key!\n",
    "    'normalize_batch': True, # key!\n",
    "\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 1024,\n",
    "    'final_tdim': 1024, \n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} \n",
    "\n",
    "train_hparams = {\n",
    "    # log\n",
    "    'note': 'temp',\n",
    "    'row_log_interval': 1,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2,\n",
    "\n",
    "    # data size\n",
    "    'overfit_pct': 1,\n",
    "    'min_epochs': 3,\n",
    "    'max_epochs': 1,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 5,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df))[:1]:\n",
    "\n",
    "    # train one window\n",
    "    train_one(Model, window_i, model_hparams, train_hparams, preembeddings, targets_df, split_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test on one batch\n",
    "def test_step_fr(model, batch):\n",
    "    # get input\n",
    "    docid, car, inp = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(inp).item() # (N, 1)\n",
    "\n",
    "    return y_car, docid[0]\n",
    "    \n",
    "    \n",
    "def test_step_text(model, batch):\n",
    "    car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "    sest, sue, numest, sstdest, smedest, \\\n",
    "    mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(embeddings, mask).item() # (N, 1)\n",
    "    transcriptid = transcriptid.int().item()\n",
    "    docid = targets_df.loc[targets_df.transcriptid==transcriptid].docid.iloc[0]\n",
    "\n",
    "    return y_car, docid\n",
    "\n",
    "def test_step_text_fr(model, batch):\n",
    "    car, transcriptid, embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3,\\\n",
    "    sest, sue, numest, sstdest, smedest, \\\n",
    "    mcap, roa, bm, debt_asset, volatility, volume = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(embeddings, mask, alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, \\\n",
    "                  smedest, mcap, roa, bm, debt_asset, volatility, volume).item() # (N, 1)\n",
    "    transcriptid = transcriptid.int().item()\n",
    "    docid = targets_df.loc[targets_df.transcriptid==transcriptid].docid.iloc[0]\n",
    "\n",
    "    return y_car, docid\n",
    "\n",
    "# test on one window\n",
    "def test_one_window(model, dataloader):\n",
    "    # select test_step\n",
    "    if type(model) in [CCMLP]:\n",
    "        test_step = test_step_fr\n",
    "    elif type(model) in [CCTransformerSTLTxtFr]:\n",
    "        test_step = test_step_text_fr\n",
    "    # elif type(model) in [CCTransformerSTLTxtFr]:\n",
    "    #    test_step = test_step_text_fr\n",
    "    \n",
    "    ys = []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        ys.append(test_step(model, batch))\n",
    "        # if i>=2: break\n",
    "    return pd.DataFrame(ys, columns=['y_car', 'docid'])\n",
    "\n",
    "# get prediction\n",
    "def predict_car():\n",
    "    global hparams\n",
    "    hparams = Namespace(**hparams)\n",
    "\n",
    "    # get roll_type\n",
    "    hparams.roll_type = hparams.ckpt_folder.split('/')[-1].split('-')[0]\n",
    "    \n",
    "    # load split_df\n",
    "    load_split_df(hparams.roll_type)\n",
    "\n",
    "    # load targets_df\n",
    "    load_targets(hparams.targets_name)\n",
    "\n",
    "    # load preembedding\n",
    "    load_preembeddings(hparams.preembedding_type)\n",
    "    \n",
    "    car_prediction = []\n",
    "    for i, name in enumerate(sorted(os.listdir(f'D:/Checkpoints/earnings-call/{hparams.ckpt_folder}'))):\n",
    "        # if i>=1: break\n",
    "            \n",
    "        if name.endswith('.ckpt'):\n",
    "\n",
    "            # get window\n",
    "            window = re.search(r'roll-\\d{2}', name).group()\n",
    "\n",
    "            # load model\n",
    "            model = hparams.Model.load_from_checkpoint(f'D:/Checkpoints/earnings-call/{hparams.ckpt_folder}/{name}')\n",
    "\n",
    "            # get testloader\n",
    "            model.prepare_data()\n",
    "            test_dataloader = model.test_dataloader()\n",
    "            model.freeze()\n",
    "            \n",
    "            # predict\n",
    "            y = test_one_window(model, test_dataloader)\n",
    "            y['window'] = window\n",
    "            y['roll_type'] = hparams.roll_type\n",
    "\n",
    "            # append to ys\n",
    "            car_prediction.append(y)\n",
    "    car_prediction = pd.concat(car_prediction)\n",
    "    \n",
    "    car_prediction.reset_index().to_feather(f'data/{hparams.save_name}.feather')\n",
    "    return car_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (3y) \n",
      "(train: 2008-01-01 to 2010-12-31) (test: 2011-01-01 to 2011-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a808e708ee043f1b10cff671fdea0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-02 (3y) \n",
      "(train: 2008-04-01 to 2011-03-31) (test: 2011-04-01 to 2011-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52977f528795414c8fc7a595186b93cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=516.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-03 (3y) \n",
      "(train: 2008-07-01 to 2011-06-30) (test: 2011-07-01 to 2011-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5cb7a3a8041dba151fe588abdbbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=510.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-04 (3y) \n",
      "(train: 2008-10-01 to 2011-09-30) (test: 2011-10-01 to 2011-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115a87f14f374afea2e7e23d02594208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-05 (3y) \n",
      "(train: 2009-01-01 to 2011-12-31) (test: 2012-01-01 to 2012-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebaf70235f64d7aac02980813471503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=479.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-06 (3y) \n",
      "(train: 2009-04-01 to 2012-03-31) (test: 2012-04-01 to 2012-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf71d0e622e24a508a9a72ad7f41559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=510.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-07 (3y) \n",
      "(train: 2009-07-01 to 2012-06-30) (test: 2012-07-01 to 2012-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd2a91e5d2648c291a4d02873235509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=497.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-08 (3y) \n",
      "(train: 2009-10-01 to 2012-09-30) (test: 2012-10-01 to 2012-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e1db12916b4c25a993ec5f19fe8099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=462.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-09 (3y) \n",
      "(train: 2010-01-01 to 2012-12-31) (test: 2013-01-01 to 2013-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25058b7eed04ca8b95ff8cea55cbb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=460.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-10 (3y) \n",
      "(train: 2010-04-01 to 2013-03-31) (test: 2013-04-01 to 2013-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4214550478d243dc8b041aaca98602ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=489.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-11 (3y) \n",
      "(train: 2010-07-01 to 2013-06-30) (test: 2013-07-01 to 2013-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65d5e7b76ff4bbdb4ceaba1e3a8e42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=501.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-12 (3y) \n",
      "(train: 2010-10-01 to 2013-09-30) (test: 2013-10-01 to 2013-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0639d6b2fa34a46869c53696634c339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=479.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-13 (3y) \n",
      "(train: 2011-01-01 to 2013-12-31) (test: 2014-01-01 to 2014-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd5b65225c4420db21e4a82ad5b0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=466.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-14 (3y) \n",
      "(train: 2011-04-01 to 2014-03-31) (test: 2014-04-01 to 2014-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80756ea834e64a3d9c170aef98529b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=499.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-15 (3y) \n",
      "(train: 2011-07-01 to 2014-06-30) (test: 2014-07-01 to 2014-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e382007f7e4c29957c0bc2f8f7a629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=496.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-16 (3y) \n",
      "(train: 2011-10-01 to 2014-09-30) (test: 2014-10-01 to 2014-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a544d9747244d8ba0a686f9352c901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-17 (3y) \n",
      "(train: 2012-01-01 to 2014-12-31) (test: 2015-01-01 to 2015-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86714bc1cf1e47d496ef798f3b04a85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=478.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-18 (3y) \n",
      "(train: 2012-04-01 to 2015-03-31) (test: 2015-04-01 to 2015-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea38d20726f241eaa4fd54adb400a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-19 (3y) \n",
      "(train: 2012-07-01 to 2015-06-30) (test: 2015-07-01 to 2015-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da46c76275f47f287804e4586697190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=484.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-20 (3y) \n",
      "(train: 2012-10-01 to 2015-09-30) (test: 2015-10-01 to 2015-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb99a335938b4ccfaa8101114b4e4fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=471.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-21 (3y) \n",
      "(train: 2013-01-01 to 2015-12-31) (test: 2016-01-01 to 2016-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62e763dfbc84d9b8f53277576faf8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-22 (3y) \n",
      "(train: 2013-04-01 to 2016-03-31) (test: 2016-04-01 to 2016-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56da6fe8891f49cebbd21637dd5efe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-23 (3y) \n",
      "(train: 2013-07-01 to 2016-06-30) (test: 2016-07-01 to 2016-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2489b4b54449daaa0fa793d2d4cf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=468.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-24 (3y) \n",
      "(train: 2013-10-01 to 2016-09-30) (test: 2016-10-01 to 2016-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4654aa5a94b4a40b775895cb82f9861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=466.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-25 (3y) \n",
      "(train: 2014-01-01 to 2016-12-31) (test: 2017-01-01 to 2017-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f02462fe00a446a86d704c0992dbdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=457.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-26 (3y) \n",
      "(train: 2014-04-01 to 2017-03-31) (test: 2017-04-01 to 2017-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1dd7979f2c400a84709ebed851cbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-27 (3y) \n",
      "(train: 2014-07-01 to 2017-06-30) (test: 2017-07-01 to 2017-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8768f37a1ab0472798a748fa368e27eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=452.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-28 (3y) \n",
      "(train: 2014-10-01 to 2017-09-30) (test: 2017-10-01 to 2017-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edf412ac16e4fbe92c66a75d7872306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=454.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-29 (3y) \n",
      "(train: 2015-01-01 to 2017-12-31) (test: 2018-01-01 to 2018-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fadcad34574ec9837faaae62fa0d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=432.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-30 (3y) \n",
      "(train: 2015-04-01 to 2018-03-31) (test: 2018-04-01 to 2018-06-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4829865dbb493bbd365c2c8e1b3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=461.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-31 (3y) \n",
      "(train: 2015-07-01 to 2018-06-30) (test: 2018-07-01 to 2018-09-30)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb2f8afece473a84a61a1d6e843f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current window: roll-32 (3y) \n",
      "(train: 2015-10-01 to 2018-09-30) (test: 2018-10-01 to 2018-12-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f0738b0e534220964927700ff2078b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=436.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    'Model': CCTransformerSTLTxtFr, # key!\n",
    "    'ckpt_folder': '3y-TSFM-txt-fr', # key!\n",
    "    'save_name': 'car_pred_tsfm_txt_fr', # key!\n",
    "\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_text',\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded',\n",
    "}\n",
    "\n",
    "# get car_ranking_predict\n",
    "car_pred_mlp = predict_car()\n",
    "\n",
    "# car_pred_mlp.reset_index().to_feather('data/car_pred_mlp.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_car</th>\n",
       "      <th>docid</th>\n",
       "      <th>window</th>\n",
       "      <th>roll_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>15.298223</td>\n",
       "      <td>031122-2011-01-10</td>\n",
       "      <td>roll-01</td>\n",
       "      <td>3y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_car              docid   window roll_type\n",
       "224  15.298223  031122-2011-01-10  roll-01        3y"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_pred_mlp.loc[car_pred_mlp.docid=='031122-2011-01-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- modify experiment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'Saved', 'code': 200, 'data': None, 'sdkErrorCode': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ((comet_ml.api.Metric('test_rmse')!=None) &\n",
    "         (comet_ml.api.Parameter('note')!='3y-MLP-car_norm'))\n",
    "\n",
    "exps = comet_ml.api.API().query('amiao', 'earnings-call', query, archived=False)\n",
    "\n",
    "for exp in exps:\n",
    "    exp.log_parameter('normalize_target', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- download comet log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# exps = comet_ml.api.API().query('amiao', 'earnings-call', comet_ml.api.Metric('test_rmse') != None, archived=False)\n",
    "\n",
    "exps = comet_ml.api.API().query('amiao', 'earnings-call', comet_ml.api.Metric('test_rmse') != None, archived=False)\n",
    "\n",
    "log_comet = []\n",
    "for exp in exps:\n",
    "    # get parameter\n",
    "    log = {param['name']:param['valueCurrent'] for param in exp.get_parameters_summary()}\n",
    "    \n",
    "    # get metrics\n",
    "    log['test_rmse'] = exp.get_metrics('test_rmse')[0]['metricValue']\n",
    "    \n",
    "    # get metadat\n",
    "    log = {**log, **exp.get_metadata()}\n",
    "    \n",
    "    # delete useless params\n",
    "    for key in ['checkpoint_path']:\n",
    "        log.pop(\"key\", None)\n",
    "    log_comet.append(log)\n",
    "    \n",
    "log_comet = pd.DataFrame(log_comet)\n",
    "log_comet.to_feather('data/comet_log.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever.\n",
    "\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)\n",
    "\n",
    "class FastDataLoader(torch.utils.data.dataloader.DataLoader):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true toc-nb-collapsed=true"
   },
   "source": [
    "### `val` is after `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, valid_transcriptids=None, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "        '''\n",
    "\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        \n",
    "        # decalre data as globals so don't   need to create/reload\n",
    "        global preembeddings, targets_df, split_df\n",
    "        \n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, val_start, val_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (val: {val_start} to {val_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        val_start = datetime.strptime(val_start, '%Y-%m-%d').date()\n",
    "        val_end = datetime.strptime(val_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.tolist()\n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(val_start, val_end)].transcriptid.tolist()\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "        \n",
    "        # self attributes\n",
    "        self.targets_df = targets_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.transform = transform\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # inputs: preembeddings\n",
    "        embeddings = self.preembeddings[transcriptid]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        sue = targets.sue\n",
    "        sest = targets.sest\n",
    "        car_0_30 = targets.car_0_30\n",
    "        \n",
    "        alpha = targets.alpha\n",
    "        volatility = targets.volatility\n",
    "        mcap = targets.mcap/1e6\n",
    "        bm = targets.bm\n",
    "        roa = targets.roa\n",
    "        debt_asset = targets.debt_asset\n",
    "        numest = targets.numest\n",
    "        smedest = targets.smedest\n",
    "        sstdest = targets.sstdest\n",
    "        car_m1_m1 = targets.car_m1_m1\n",
    "        car_m2_m2 = targets.car_m2_m2\n",
    "        car_m30_m3 = targets.car_m30_m3\n",
    "        volume = targets.volume\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            return car_0_30, transcriptid, embeddings, alpha, car_m1_m1, car_m2_m2, car_m30_m3, \\\n",
    "                   sest, sue, numest, sstdest, smedest, \\\n",
    "                   mcap, roa, bm, debt_asset, volatility, volume\n",
    "        else:\n",
    "            return torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
