{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: C:/Users/rossz/OneDrive/CC\n",
      "DATA_DIR: C:/Users/rossz/OneDrive/CC/data\n",
      "\n",
      "GPU enabled: GeForce GTX 1080 Ti\n",
      "\n",
      "GPU Memory allocated/cached (GB): 0.0/0.0\n",
      "GPU Memory used/total (GB):  0.2848/ 11.0\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import logging\n",
    "import pynvml\n",
    "from spacy.lang.en import English\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence\n",
    "from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = 'C:/Users/rossz/OneDrive/CC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;\n",
    "\n",
    "# helper to show GPU memory\n",
    "def log_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f'GPU Memory allocated/cached (GB): {torch.cuda.memory_allocated()/1024**3:.4}/{torch.cuda.memory_cached()/1024**3:.4}')\n",
    "    print(f'GPU Memory used/total (GB): {meminfo.used/1024**3: .4}/{meminfo.total/1024**3: .4}')\n",
    "    pynvml.nvmlShutdown()\n",
    "    \n",
    "# set device name: 'cuda' or 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Show if GPU enabled\n",
    "if device.type == 'cuda':\n",
    "    print(f'\\nGPU enabled: {torch.cuda.get_device_name(0)}\\n');\n",
    "    log_gpu_memory()\n",
    "else: \n",
    "    print('GPU NOT enabled')\n",
    "pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Doc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read-in-`df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "# cc_5y: past 5 years (2014-2018), 70,114 obs\n",
    "cc = pd.read_feather(f'C:/Users/rossz/OneDrive/CC/data/cc_5y.feather')\n",
    "print(f'num of cc: {len(cc)}')\n",
    "md = cc['md'].to_list() # (n_cc,) -> str\n",
    "qa = cc['qa'].to_list() # (n_cc,) -> str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load `spaCy` and `Transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# spacy model\n",
    "nlp = English()  \n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated/cached (GB): 0.5144/0.5371\n",
      "GPU Memory used/total (GB):  1.182/ 11.0\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "# BERT model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking')\n",
    "bert_model = BertModel.from_pretrained('bert-large-cased-whole-word-masking')\n",
    "'''\n",
    "\n",
    "# GPT-2\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_model.eval();\n",
    "gpt_model.to('cuda');\n",
    "pass;\n",
    "log_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `doc_to_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_tokens_v1(doc:str, nlp, tokenizer, max_seq_len):\n",
    "    '''Split doc to sequences, then to tokens\n",
    "    v1: a sequence is defined as a natural \"setence\" given by spaCy SBD\n",
    "    \n",
    "    Args:\n",
    "        doc: a document string\n",
    "        nlp: spaCy model to split sentences\n",
    "        \n",
    "    input: a document as a string\n",
    "    \n",
    "    output: a list of tokens: \n",
    "        [[tokens_of_seq_1], [tokens_of_seq_2],...]\n",
    "    '''\n",
    "    \n",
    "    # split into sentences\n",
    "    # sents: a list,\n",
    "    #     [sent_1_as_string, sent_2_as_string,...]\n",
    "    sents = [sent.text for sent in nlp(doc).sents] \n",
    "    \n",
    "    # tokenize sentence\n",
    "    # sents_tokenized: a list,\n",
    "    #     [[tokens_of_seq_1], [tokens_of_seq_2],...]\n",
    "    sents_tokenized = [tokenizer.encode(sent) for sent in sents]   \n",
    "    \n",
    "    # if seq_len exceeds `max_seq_len`, \n",
    "    # split them into multiple sequences\n",
    "    sents_tokenized = [torch.tensor(subsent) for sent in sents_tokenized for subsent in list(chunks(sent, max_seq_len))]\n",
    "    \n",
    "    return sents_tokenized\n",
    "\n",
    "\n",
    "\n",
    "def doc_to_tokens_v2(doc:str, tokenizer, max_seq_len):\n",
    "    '''Split doc to sequences, then to tokens\n",
    "    \n",
    "    v2: a sequence's length is fixed to `max_seq_len`\n",
    "    \n",
    "    input: a document as a string\n",
    "    \n",
    "    output: a list of tokens: \n",
    "        [[tokens_of_seq_1], [tokens_of_seq_2],...]\n",
    "    '''\n",
    "    \n",
    "    # split into sentences\n",
    "    \n",
    "    # tokenize sentence\n",
    "    # doc_tokenized: a list,\n",
    "    #     [token_1, token_2,..., token_end_of_doc]\n",
    "    doc_tokenized = tokenizer.encode(doc)\n",
    "    \n",
    "    # chunk doc_tokenized into multiple sequences\n",
    "#     sents_tokenized = [torch.tensor(subsent) for sent in sents_tokenized for subsent in list(chunks(sent, max_seq_len))]\n",
    "    sents_tokenized = [torch.tensor(sent) \n",
    "                       for sent in list(chunks(doc_tokenized, max_seq_len))]\n",
    "    # print(len(sents_tokenized))\n",
    "    \n",
    "    \n",
    "    return sents_tokenized\n",
    "\n",
    "# sents_tokenized = doc_to_tokens_v2(md[0], gpt_tokenizer, 25)\n",
    "# for _ in range(3):\n",
    "#     print(sents_tokenized[_])\n",
    "\n",
    "# sents_tokenized = doc_to_tokens_v1(md[0], nlp, gpt_tokenizer, 25)\n",
    "# for _ in range(3):\n",
    "#     print(sents_tokenized[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tokens_to_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_emb(sents_tokenized, max_n_seq_per_batch, model):\n",
    "    '''\n",
    "    Args:\n",
    "        sent_tokenized: a list of sentence tokens. \n",
    "            e.g. [[0, 3], \n",
    "                  [3], \n",
    "                  [3, 13, 134]]\n",
    "            \n",
    "        max_n_seq_per_batch: e.g., for the above example, if \n",
    "            n_seq_per_batch is 2, then the output will be: \n",
    "            [[[0, 3], [3]],\n",
    "             [[3, 13, 134]]]\n",
    "    '''\n",
    "    \n",
    "    # split into batchs\n",
    "    # print(f'inp n_seq: {len(sents_tokenized)}')\n",
    "    sents_tokenized_as_batch = list(\n",
    "        chunks(sents_tokenized, max_n_seq_per_batch))\n",
    "    # print(f'n_batch_in_doc: {len(sents_tokenized_as_batch)}')\n",
    "    \n",
    "    # output place holder\n",
    "    emb_avgpool = []\n",
    "    emb_maxpool = []\n",
    "    \n",
    "    # for every batch, generate emb\n",
    "    for batch in sents_tokenized_as_batch:\n",
    "        # batch: list [seq_1_in_batch, seq_2_in_batch,... seq_n_in_batch]\n",
    "        \n",
    "        # n_seq: current n_seq in the batch, most of them is\n",
    "        # max_n_seq_per_batch, only the last will the less\n",
    "        n_seq = len(batch)\n",
    "        \n",
    "        # valid_seq_len: (n_seq, 1)\n",
    "        valid_seq_len = torch.tensor([sent.shape[0] for sent in batch]).to(device)\n",
    "        # print(valid_seq_len)\n",
    "\n",
    "        # pad setences to max length\n",
    "        # (n_seq_per_batch, max_seq_len)\n",
    "        sents_padded_tokenized = pad_sequence(\n",
    "            batch, batch_first=True).to(device)\n",
    "        # print(sents_padded_tokenized)\n",
    "\n",
    "        # create mask for padded sentences\n",
    "        # mask: (n_seq_per_batch, max_seq_len)\n",
    "        mask = (sents_padded_tokenized != 0).int().float().to(device)\n",
    "        # print(mask)\n",
    "\n",
    "        # get sentence emb\n",
    "        # (n_seq_in_doc, max_seq_len, d_model)\n",
    "        sents_emb = model(sents_padded_tokenized,\n",
    "                          attention_mask = mask)[0]\n",
    "        d_model = sents_emb.shape[-1]\n",
    "        # print(sents_emb[:,:,:4])\n",
    "\n",
    "        # create a placeholder to store the output\n",
    "        # MUST explicitly newly created tensor to its device!\n",
    "        avgpool = torch.zeros(n_seq, d_model).to(device)\n",
    "        maxpool = torch.zeros(n_seq, d_model).to(device)\n",
    "        \n",
    "        # for every doc in a batch, do mask average pooling\n",
    "        for i, end in enumerate(valid_seq_len):\n",
    "            avgpool[i] = torch.mean(sents_emb[i, :end], 0)\n",
    "            maxpool[i] = torch.max(sents_emb[i, :end], 0)[0]\n",
    "        # print(avgpool[:,:4])\n",
    "        # print(maxpool[:,:4])\n",
    "        \n",
    "        # add to output\n",
    "        emb_avgpool.append(avgpool)\n",
    "        emb_maxpool.append(maxpool)\n",
    "        \n",
    "    # final output\n",
    "    emb_avgpool = torch.cat(emb_avgpool, 0)\n",
    "    emb_maxpool = torch.cat(emb_maxpool, 0)\n",
    "    \n",
    "    return emb_avgpool, emb_maxpool\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     tokens_to_emb(\n",
    "#         sents_tokenized[:5],\n",
    "#         max_n_seq_per_batch=2,\n",
    "#         model=gpt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `doc_to_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_emb(\n",
    "    docs, save_name, \n",
    "    tokenizer, model, \n",
    "    max_seq_len, max_n_seq_per_batch, \n",
    "    start_i=0,\n",
    "    nlp=None):\n",
    "    \n",
    "    log_gpu_memory()\n",
    "    \n",
    "    # final: contain ALL doc\n",
    "    # temp: a bunch of doc, for intermediate saving\n",
    "    doc_emb_avg_final = []\n",
    "    doc_emb_avg_temp = []\n",
    "\n",
    "    doc_emb_max_final = []\n",
    "    doc_emb_max_temp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, doc in enumerate(tqdm(docs)):\n",
    "#         for i in tqdm(range(start_i, len(docs))):\n",
    "#             doc = docs[i]\n",
    "            if (doc is None) or (doc == ''):\n",
    "                doc_emb_avg_final.append(None)\n",
    "                doc_emb_avg_temp.append(None)\n",
    "                doc_emb_max_final.append(None)\n",
    "                doc_emb_max_temp.append(None)\n",
    "            else:\n",
    "                '''\n",
    "                tokens = doc_to_tokens_v1(\n",
    "                    doc, nlp, tokenizer, max_seq_len)\n",
    "                '''\n",
    "                tokens = doc_to_tokens_v2(\n",
    "                    doc, tokenizer, max_seq_len)\n",
    "                \n",
    "                emb_avg, emb_max = tokens_to_emb(\n",
    "                    tokens, max_n_seq_per_batch, model)\n",
    "\n",
    "                # must send result to 'cpu', otherwise\n",
    "                # they'll accumulate in GPU memory!\n",
    "                emb_avg_cpu = emb_avg.to('cpu')\n",
    "                emb_max_cpu = emb_max.to('cpu')\n",
    "\n",
    "                doc_emb_avg_final.append(emb_avg_cpu)\n",
    "                doc_emb_max_final.append(emb_max_cpu)\n",
    "\n",
    "                doc_emb_avg_temp.append(emb_avg_cpu)\n",
    "                doc_emb_max_temp.append(emb_max_cpu)\n",
    "\n",
    "                # delete intermediate result to save GPU memory\n",
    "                del emb_avg, emb_max\n",
    "\n",
    "            # save every n doc\n",
    "            if (i % 5000 == 4999) or (i == len(docs)):\n",
    "                print(f'Save at i = {i}')\n",
    "                sv_dir_avg = f'{DATA_DIR}/{save_name}_avg_{str(i).zfill(6)}.pt'\n",
    "                sv_dir_max = f'{DATA_DIR}/{save_name}_max_{str(i).zfill(6)}.pt'\n",
    "                torch.save(doc_emb_avg_temp, sv_dir_avg)\n",
    "                torch.save(doc_emb_max_temp, sv_dir_max)\n",
    "                doc_emb_avg_temp = []\n",
    "                doc_emb_max_temp = []                \n",
    "                \n",
    "        # when job is done, save again\n",
    "        print('Saving final results...')\n",
    "        sv_dir_avg = f'{DATA_DIR}/{save_name}_avg_final.pt'\n",
    "        sv_dir_max = f'{DATA_DIR}/{save_name}_max_final.pt'\n",
    "        torch.save(doc_emb_avg_final, sv_dir_avg)\n",
    "        torch.save(doc_emb_max_final, sv_dir_max)\n",
    "        print('Saving done!')\n",
    "        return doc_emb_avg_final, doc_emb_max_final\n",
    "                \n",
    "    log_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated/cached (GB): 0.5144/0.5371\n",
      "GPU Memory used/total (GB):  1.415/ 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▍                                | 4999/70114 [12:53<2:34:59,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████▉                              | 9999/70114 [25:34<2:50:23,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▎                          | 14999/70114 [39:00<2:34:44,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████▋                        | 19999/70114 [51:57<2:16:40,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 19999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████▍                    | 24998/70114 [1:04:44<2:25:40,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 24999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████▋                  | 29999/70114 [1:17:55<2:27:49,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 29999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████▉                | 34999/70114 [1:31:00<1:15:51,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 34999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████▎             | 39998/70114 [1:43:48<1:42:31,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 39999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████▌           | 44999/70114 [1:57:29<1:14:52,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 44999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████▏         | 49999/70114 [2:10:32<55:11,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 49999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████▋       | 54999/70114 [2:23:25<47:06,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 54999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████     | 59999/70114 [2:37:05<30:59,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 59999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████▌  | 64999/70114 [2:50:01<14:01,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 64999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████▉| 69999/70114 [3:03:51<00:21,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 69999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 70114/70114 [3:04:10<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final results...\n",
      "Saving done!\n",
      "Wall time: 3h 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# suppress \"seq too long warning\"\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "md_emb_avg, md_emb_max = doc_to_emb(\n",
    "    md, 'md_emb',\n",
    "    gpt_tokenizer, gpt_model, \n",
    "    max_seq_len=64, max_n_seq_per_batch=1024)\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated/cached (GB): 0.5144/0.5371\n",
      "GPU Memory used/total (GB):  1.182/ 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▍                                | 4999/70114 [16:54<3:36:04,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████▉                              | 9999/70114 [33:57<3:05:26,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▎                          | 14999/70114 [51:02<3:35:09,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████▏                      | 19999/70114 [1:09:13<3:36:21,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 19999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████▍                    | 24998/70114 [1:26:26<3:02:04,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 24999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████▋                  | 29999/70114 [1:43:17<3:27:12,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 29999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████▉                | 34999/70114 [2:01:52<2:05:41,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 34999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████▎             | 39999/70114 [2:19:10<1:22:58,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 39999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████▌           | 44999/70114 [2:37:45<1:32:55,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 44999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████▊         | 49999/70114 [2:55:27<1:35:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 49999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████▋       | 54999/70114 [3:13:33<45:09,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 54999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████     | 59997/70114 [3:32:27<46:14,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 59999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████▌  | 64999/70114 [3:49:51<15:11,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 64999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████▉| 69999/70114 [4:09:01<00:36,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at i = 69999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 70114/70114 [4:09:24<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final results...\n",
      "Saving done!\n",
      "Wall time: 4h 10min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# suppress \"seq too long warning\"\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "qa_emb_avg, qa_emb_max = doc_to_emb(\n",
    "    qa, 'qa_emb',\n",
    "    gpt_tokenizer, gpt_model, \n",
    "    max_seq_len=64, max_n_seq_per_batch=1024)\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len of qa must match md\n",
    "# torch.load(f'{DATA_DIR}/qa_emb_avg_final.pt')\n",
    "# md_emb_avg = torch.load(f'{DATA_DIR}/md_emb_avg_final.pt')\n",
    "\n",
    "assert len(md_emb_avg) == len(qa_emb_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare a toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# laod parsed MD and QA\n",
    "qa_avgpool = torch.load(f'{DATA_DIR}/md_emb_avgpool.pt')\n",
    "qa_maxpool = torch.load(f'{DATA_DIR}/md_emb_maxpool.pt')\n",
    "md_avgpool = torch.load(f'{DATA_DIR}/qa_emb_avgpool.pt')\n",
    "md_maxpool = torch.load(f'{DATA_DIR}/qa_emb_maxpool.pt')\n",
    "\n",
    "expert = torch.randn((100,))\n",
    "non_expert = torch.randn((100,))\n",
    "# expert = torch.stack([torch.randn((100,)), torch.zeros(100)], 1)\n",
    "# non_expert = torch.stack([torch.zeros(100), torch.randn((100,))], 1)\n",
    "\n",
    "data = {'qa_avgpool': qa_avgpool,\n",
    "        'qa_maxpool': qa_maxpool,\n",
    "        'md_avgpool': md_avgpool,\n",
    "        'md_maxpool': md_maxpool,\n",
    "        'expert': expert,\n",
    "        'non_expert': non_expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_avgpool)\n",
    "len(qa_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCDataset(Dataset):\n",
    "    '''Conference Call Dataset'''\n",
    "    \n",
    "    def __init__(self, data, \n",
    "                 output,\n",
    "                 transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            data: a dict containing all the input\n",
    "                Note: we didn't use DataFrame because it's a bad idea\n",
    "                to store tensors in a DataFrame\n",
    "            output: which text we'll use. Could be one of the following:\n",
    "                - md_avgpool\n",
    "                - md_maxpool\n",
    "                - qa_avgpool\n",
    "                - qa_maxpool\n",
    "                - all: (not implemented)\n",
    "                \n",
    "        Returns:\n",
    "            text\n",
    "            expert\n",
    "            non_expert\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.output = output\n",
    "        self.transform = transform\n",
    "        \n",
    "        # delete any None in the output field\n",
    "        for i in range(len(data['expert'])):\n",
    "            if data[output][i] is None:\n",
    "                del self.data[output][i]\n",
    "                del self.data['expert'][i]\n",
    "                del self.data['non_expert'][i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(data['expert']))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''parse cc on the fly'''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        text = self.data[self.output][idx]\n",
    "        expert = self.data['expert'][idx]\n",
    "        non_expert = self.data['non_expert'][idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "            \n",
    "        return text, expert, non_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: torch.Size([190, 768])\n",
      "expert: torch.Size([])\n",
      "non_expert: torch.Size([])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "ccds = CCDataset(data, output='qa_avgpool')\n",
    "print(f'text: {ccds[50][0].shape}')\n",
    "print(f'expert: {ccds[50][1].shape}')\n",
    "print(f'non_expert: {ccds[50][2].shape}')\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccds_train_len = int(len(ccds) * 0.8)\n",
    "ccds_valid_len = int(len(ccds) * 0.1)\n",
    "ccds_test_len = len(ccds) - ccds_train_len - ccds_valid_len\n",
    "\n",
    "torch.manual_seed(42); # must reset random seed!\n",
    "ccds_train, ccds_valid, ccds_test = random_split(ccds, [ccds_train_len, ccds_valid_len, ccds_test_len])\n",
    "pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must create a NEW `DataLoader` in EVERY epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data):\n",
    "    '''create mini-batch\n",
    "    \n",
    "    Retures:\n",
    "        text: tensor, (batch_size, max_seq_len, d_model)\n",
    "        expert: tensor, (batch_size,)\n",
    "        non_expert: tensor, (batch_size,)\n",
    "        length: list, valide length for each padded seq\n",
    "    '''\n",
    "    \n",
    "    # sort a data list by seq_len (descending)\n",
    "    data.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    text, expert, non_expert = zip(*data)\n",
    "    \n",
    "    # expert/non_expert: (batch_size, 2)\n",
    "    expert_tgt = torch.zeros((BATCH_SIZE, N_TARGET))\n",
    "    expert_tgt[:,0] = torch.stack(expert).squeeze()\n",
    "    \n",
    "    non_expert_tgt = torch.zeros((BATCH_SIZE, N_TARGET))\n",
    "    non_expert_tgt[:, 1] = torch.stack(non_expert).squeeze()\n",
    "    \n",
    "    assert (expert_tgt * non_expert_tgt).sum() == 0\n",
    "    \n",
    "    # collate text\n",
    "    lengths = [t.shape[-2] for t in text]\n",
    "    text = pad_sequence(text, batch_first=True) # (n_batch, max_seq_len, d)\n",
    "    text = torch.transpose(text, 0, 1) # (max_seq_len, n_batch, d)\n",
    "    \n",
    "    # create mask: (n_batch, max_seq_len)\n",
    "    mask = torch.ones((text.shape[1], text.shape[0]))\n",
    "    for i, length in enumerate(lengths):\n",
    "        mask[i, :length] = 0\n",
    "    mask = mask == 1\n",
    "    \n",
    "    return text, expert_tgt, non_expert_tgt, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.text.is_pinned()=False\n",
      "text: torch.Size([298, 3, 768])\n",
      "expert: torch.Size([3, 1])\n",
      "non_expert: torch.Size([3, 1])\n",
      "mask: torch.Size([3, 298])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "ccdl = DataLoader(ccds, batch_size=3, shuffle=False, \n",
    "                  num_workers=0,\n",
    "                  collate_fn=generate_batch)\n",
    "\n",
    "for i, batch in enumerate(ccdl):\n",
    "#     print(batch[0])\n",
    "    print(f'batch.text.is_pinned()={batch[0].is_pinned()}')\n",
    "    print(f'text: {batch[0].shape}')\n",
    "    print(f'expert: {batch[1].shape}')\n",
    "    print(f'non_expert: {batch[2].shape}')\n",
    "    print(f'mask: {batch[3].shape}')\n",
    "    print('------------')\n",
    "    \n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PositionEnc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output x: (seq_len_of_x, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaskPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskAvgPool(nn.Module):\n",
    "    '''\n",
    "    Given a (max_seq_len, batch_size, d_model) `input` tensor and a\n",
    "            (batch, max_seq_len) `mask`,\n",
    "    pool into (batch_size, d_model)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(MaskAvgPool, self).__init__()\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        # inp x: (max_seq_len, batch_size, d_model)\n",
    "        # inp mask: (batch_size, max_seq_len)\n",
    "        _, batch_size, d_model = x.shape\n",
    "        \n",
    "        \n",
    "        # compute valid length of each sequence\n",
    "        # valid_seq_len: (batch_size,)\n",
    "        valid_seq_len = torch.sum(mask==False, -1)\n",
    "\n",
    "        \n",
    "        # create a placeholder to store the output\n",
    "        # MUST explicitly newly created tensor to its device!\n",
    "        pooled = torch.zeros(batch_size, d_model).to(device)\n",
    "        \n",
    "        # for every doc in a batch, do mask average pooling\n",
    "        for i, end in enumerate(valid_seq_len):\n",
    "            pooled[i] = torch.mean(x[:end, i], 0)\n",
    "            \n",
    "        return pooled\n",
    "\n",
    "    \n",
    "class MaskMaxPool(nn.Module):\n",
    "    '''\n",
    "    Given a (max_seq_len, batch_size, d_model) `input` tensor and a\n",
    "            (batch, max_seq_len) `mask`,\n",
    "    pool into (batch_size, d_model)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(MaskMaxPool, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # inp x: (max_seq_len, batch_size, d_model)\n",
    "        # inp mask: (batch_size, max_seq_len)\n",
    "        _, batch_size, d_model = x.shape\n",
    "        \n",
    "        # compute valid length of each sequence\n",
    "        # valid_seq_len: (batch_size,)\n",
    "        valid_seq_len = torch.sum(mask==False, -1).to(device)\n",
    "        \n",
    "        # create a placeholder to store the output\n",
    "        pooled = torch.zeros(batch_size, d_model)\n",
    "        \n",
    "        # for every doc in a batch, do mask average pooling\n",
    "        for i, end in enumerate(valid_seq_len):\n",
    "            pooled[i] = torch.max(text[:end, i], 0)[0]\n",
    "            \n",
    "        return pooled    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, \n",
    "        n_layers_inp, n_layers_y1, n_layers_y2,\n",
    "        n_head_inp=8, n_head_y1=8, n_head_y2=8, \n",
    "        dff=2048, attn_dropout=0.1, dropout=0.5):\n",
    "        \n",
    "        super(CC, self).__init__()\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(d_model, attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, y1, y2\n",
    "        inp_encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model, n_head_inp, dff, attn_dropout)\n",
    "        y1_encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model, n_head_y1, dff, attn_dropout)\n",
    "        y2_encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model, n_head_y1, dff, attn_dropout)\n",
    "        \n",
    "        self.encoder_inp = nn.TransformerEncoder(\n",
    "            inp_encoder_layers, n_layers_inp)\n",
    "        self.encoder_y1 = nn.TransformerEncoder(\n",
    "            y1_encoder_layers, n_layers_y1)\n",
    "        self.encoder_y2 = nn.TransformerEncoder(\n",
    "            y2_encoder_layers, n_layers_y2)\n",
    "        \n",
    "        # pooling layers\n",
    "        self.mask_avgpool = MaskAvgPool().to(device)\n",
    "        self.mask_maxpool = MaskMaxPool().to(device)\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout_x = nn.Dropout(dropout)\n",
    "        self.dropout_y1 = nn.Dropout(dropout)\n",
    "        self.dropout_y1_2 = nn.Dropout(dropout)\n",
    "        self.dropout_y2 = nn.Dropout(dropout)\n",
    "        self.dropout_y2_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # final layer\n",
    "        self.final_y1 = nn.Linear(d_model, 2)\n",
    "        self.final_y2 = nn.Linear(d_model, 2)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # inp mask: (batch_size, seq_len)\n",
    "        # inp x: (seq_len, batch_size, d_model)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(x)\n",
    "        \n",
    "        # encode input\n",
    "        # x: (seq_len, batch_size, d_model)\n",
    "        x = self.encoder_inp(\n",
    "            x, src_key_padding_mask=mask)\n",
    "        x = self.dropout_x(x)\n",
    "        \n",
    "        # split heads\n",
    "        # final y1/y2: (seq_len, batch_size, d_model)\n",
    "        y1 = self.encoder_y1(\n",
    "            x, src_key_padding_mask=mask)\n",
    "        y2 = self.encoder_y2(\n",
    "            x, src_key_padding_mask=mask)\n",
    "        y1 = self.dropout_y1(y1)\n",
    "        y2 = self.dropout_y2(y2)\n",
    "        \n",
    "        # pool outcome\n",
    "        # y1/y2: (batch_size, d_model)\n",
    "        y1 = self.mask_avgpool(y1, mask)\n",
    "        y2 = self.mask_avgpool(y2, mask)\n",
    "        y1 = self.dropout_y1_2(y1)\n",
    "        y2 = self.dropout_y2_2(y2)\n",
    "        \n",
    "        # regularizer\n",
    "        dist = torch.dist(y1, y2)\n",
    "        \n",
    "        # final layer\n",
    "        # (batch_size, 2)\n",
    "        y1 = self.final_y1(y1)\n",
    "        y2 = self.final_y2(y2)\n",
    "        \n",
    "        return y1, y2, dist\n",
    "    \n",
    "# cc = CC(\n",
    "#     d_model=768, \n",
    "#     n_layers_inp=2, n_layers_y1=2, n_layers_y2=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_th_dist not supported on CPUType for Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-372c3d1609cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m y = torch.tensor([[8, 8],\n\u001b[0;32m      4\u001b[0m                   [1, 2]])\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: _th_dist not supported on CPUType for Long"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                  [5, 8.]])\n",
    "y = torch.tensor([[8, 8],\n",
    "                  [1, 2.]])\n",
    "torch.dist(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n",
      "---------------\n",
      "torch.Size([3, 1]) torch.Size([3, 1])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i, (text, expert, non_expert, mask) in enumerate(ccdl):\n",
    "    text = text.to('cuda')\n",
    "    mask = mask.to('cuda')\n",
    "    \n",
    "    y1, y2 = cc(text, mask)\n",
    "    print(f'y1.shape, y2.shape, dist.shape:')\n",
    "    print(y1.shape, y2.shape)\n",
    "    print('---------------')\n",
    "    \n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every task has one criterion\n",
    "criterion = [nn.MSELoss(), nn.MSELoss()]\n",
    "optimizer = torch.optim.Adam(cc.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.random((3, 2))\n",
    "t = torch.random((3, 2))\n",
    "nn.MSELoss(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [00:01<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  time in 0 mins, 1 secs\n",
      "  Loss: 0.049 (train)\n",
      "  Loss: 0.397 (valid)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [00:01<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  |  time in 0 mins, 1 secs\n",
      "  Loss: 0.039 (train)\n",
      "  Loss: 0.365 (valid)\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(ccds_train):\n",
    "    # For each epoch, create a new DataLoader\n",
    "    n_batch_in_one_epoch = len(ccds_train)//BATCH_SIZE\n",
    "    train_loss = 0.0\n",
    "    ccdl_train = DataLoader(\n",
    "        ccds_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=generate_batch)\n",
    "        \n",
    "    # train one epoch\n",
    "    for i, (text, expert, non_expert, mask) in enumerate(\n",
    "        tqdm(ccdl_train, total=n_batch_in_one_epoch)):\n",
    "        # send to GPU\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        expert = expert.to(device)\n",
    "        non_expert = non_expert.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y1, y2, dist = cc(text, mask)\n",
    "        loss_y1 = criterion[0](y1, expert)\n",
    "        loss_y2 = criterion[1](y2, non_expert)\n",
    "        loss = loss_y1 + loss_y2 - LAMBDA * dist\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        if i % 5 == 0:\n",
    "            # print(f'\\t{i+1: 5d} loss: {train_loss/100:.3f}')\n",
    "            train_loss = 0.0\n",
    "          \n",
    "    # adjust learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # return mean loss of the epoch\n",
    "    return train_loss / ccds_train_len\n",
    "\n",
    "def evaluate(ccds_eval):\n",
    "    loss = 0.0\n",
    "    \n",
    "    # create DataLoader\n",
    "    ccdl_eval = DataLoader(\n",
    "        ccds_eval,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=generate_batch)\n",
    "    \n",
    "    # evaluate\n",
    "    for i, (text, expert, non_expert, mask) in enumerate(ccdl_eval):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        expert = expert.to(device)\n",
    "        non_expert = non_expert.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y1, y2, dist = cc(text, mask)\n",
    "            loss_y1 = criterion[0](y1, expert)\n",
    "            loss_y2 = criterion[1](y2, non_expert)\n",
    "            loss = loss_y1 + loss_y2 - LAMBDA * dist\n",
    "            loss += loss.item()\n",
    "            \n",
    "    # return mean loss\n",
    "    return loss / len(ccds_eval)\n",
    "\n",
    "\n",
    "# Let's train!\n",
    "N_EPOCHS = 2\n",
    "N_TARGET = 2\n",
    "BATCH_SIZE = 10\n",
    "LAMBDA = 1\n",
    "\n",
    "\n",
    "cc.train();\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_one_epoch(ccds_train)\n",
    "    valid_loss = evaluate(ccds_valid)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs // 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print(f'Epoch: {epoch+1}  |  time in {mins} mins, {secs} secs')\n",
    "    print(f'  Loss: {train_loss:.3f} (train)')\n",
    "    print(f'  Loss: {valid_loss:.3f} (valid)')\n",
    "cc.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "torch.save(cc, f'{DATA_DIR}/cc.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# set to eval mode to disable dropout/normalization\n",
    "cc = torch.load(f'{DATA_DIR}/cc.pt')\n",
    "cc.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `gpu-memory-log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "import pynvml\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def _get_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            tensor = obj\n",
    "        else:\n",
    "            continue\n",
    "        if tensor.is_cuda:\n",
    "            yield tensor\n",
    "\n",
    "def _write_log(f, write_str):\n",
    "    print(write_str)\n",
    "    f.write(\"%s\\n\" % write_str)\n",
    "\n",
    "def gpu_memory_log(gpu_log_file=\"gpu_mem.log\", device=0):\n",
    "    stack_layer = 1\n",
    "    func_name = sys._getframe(stack_layer).f_code.co_name\n",
    "    file_name = sys._getframe(stack_layer).f_code.co_filename\n",
    "    line = sys._getframe(stack_layer).f_lineno\n",
    "    now_time = datetime.datetime.now()\n",
    "    log_format = 'LINE:%s, FUNC:%s, FILE:%s, TIME:%s, CONTENT:%s'\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(device)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "    with open(gpu_log_file, 'a+') as f:\n",
    "        write_str = log_format % (line, func_name, file_name, now_time, \"\")\n",
    "        _write_log(f, write_str)\n",
    "\n",
    "        ts_list = [tensor.size() for tensor in _get_tensors()]\n",
    "        new_tensor_sizes = {(type(x), \n",
    "                             tuple(x.size()), \n",
    "                             ts_list.count(x.size()), \n",
    "                             np.prod(np.array(x.size()))*4/1024**2)\n",
    "                             for x in _get_tensors()}\n",
    "        for t, s, n, m in new_tensor_sizes:\n",
    "            write_str = '[tensor: %s * Size:%s | Memory: %s M | %s]' %(str(n), str(s), str(m*n)[:6], str(t))\n",
    "            _write_log(f, write_str)\n",
    "\n",
    "        write_str = \"memory_allocated:%f Mb\" % float(torch.cuda.memory_allocated()/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"max_memory_allocated:%f Mb\" % float(torch.cuda.max_memory_allocated()/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"memory_cached:%f Mb\" % float(torch.cuda.memory_cached()/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"max_memory_cached:%f Mb\" % float(torch.cuda.max_memory_cached()/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"Used Memory:%f Mb\" % float(meminfo.used/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"Free Memory:%f Mb\" % float(meminfo.free/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "        write_str = \"Total Memory:%f Mb\" % float(meminfo.total/1024**2)\n",
    "        _write_log(f, write_str)\n",
    "\n",
    "    pynvml.nvmlShutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
