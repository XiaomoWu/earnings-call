{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: .\n",
      "DATA_DIR: ./data\n",
      "CHECKPOINT_DIR: d:/checkpoints/earnings-call\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import comet_ml\n",
    "import copy\n",
    "import datatable as dt\n",
    "import gc\n",
    "# import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "# import spacy\n",
    "# import sentence_transformers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "# import pyarrow.feather as feather\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from datatable import f, update\n",
    "# from spacy.lang.en import English\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from sklearn import preprocessing\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = 'd:/checkpoints/earnings-call'\n",
    "CHECKPOINT_TEMP_DIR = f'{ROOT_DIR}/checkpoint/earnings-call/temp'\n",
    "\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "# torch.backends.cudnn.deterministic = False;\n",
    "# torch.backends.cudnn.benchmark = True;\n",
    "# torch.backends.cudnn.enabled = True\n",
    "\n",
    "    \n",
    "# cpu = torch.device('cpu');\n",
    "# n_cpu = int(mp.cpu_count()/2);\n",
    "\n",
    "# print(f'\\nCPU count (physical): {n_cpu}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=cpu:\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name):\n",
    "    if 'targets_df' not in globals():\n",
    "        print(f'Loading targets...@{Now()}')\n",
    "        globals()['targets_df'] = pd.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_type):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_type}.pt\")\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    globals()['split_df'] = split_df.loc[split_df.roll_type==roll_type]\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, roll_type, window):\n",
    "    '''\n",
    "    Given roll_type and window, find the corresponding ols_rmse from `bench_fr.feather`, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, _, _, _ = tuple(split_df.loc[(split_df.window==window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "    \n",
    "    bench_fr = pd.read_feather('data/bench_fr.feather')\n",
    "\n",
    "    ols_rmse_norm = bench_fr.loc[(bench_fr.roll_type=='3y') & (bench_fr.test_start==pd.to_datetime(test_start))].test_rmse_fr_norm.to_list()[0]\n",
    "    logger.experiment.log_parameter('ols_rmse_norm', ols_rmse_norm)\n",
    "    \n",
    "def log_test_start(logger, roll_type, window):\n",
    "    '''\n",
    "    Given window, find the corresponding star/end date of the training/test periods, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, _, _, _ = tuple(split_df.loc[(split_df.window==window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)\n",
    "    \n",
    "class Now:\n",
    "    '''return current datetime, but has a more pretty format\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.current_datetime = datetime.now()\n",
    "    def __repr__(self):\n",
    "        return self.current_datetime.strftime('%H:%M:%S')    \n",
    "    \n",
    "def sv(objname:str, svname:str=None, svtype='pkl', path='./data', log=True, feather_compression='lz4'):\n",
    "    '''\n",
    "    log: output success messsage at the end\n",
    "    '''\n",
    "    starttime = time.perf_counter()\n",
    "    assert isinstance(objname, str), 'Please provide object name as a str!'\n",
    "    if svname is None:\n",
    "        svname = objname\n",
    "\n",
    "    # if obj is a datatable, \n",
    "    # 1) change svtype to 'feather', and \n",
    "    # 2) convert it to pandas\n",
    "    obj = globals()[objname]\n",
    "    \n",
    "    # write to disk\n",
    "    if svtype=='feather':\n",
    "        assert isinstance(obj, (dt.Frame, pd.DataFrame)), 'Error! Trying to save an object of type in [dt.Frame, pd.DataFrame] as feather.'\n",
    "        save_path = f\"{path}/{svname}.feather\"\n",
    "\n",
    "        if isinstance(obj, dt.Frame):\n",
    "            obj = obj.to_pandas()\n",
    "        feather.write_feather(obj, save_path, compression=feather_compression)\n",
    "\n",
    "    else:\n",
    "        save_path = f\"{path}/{svname}.pkl\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "    # print operation result\n",
    "    endtime = time.perf_counter()\n",
    "    \n",
    "    if log is True:\n",
    "        elipsetime = endtime-starttime\n",
    "        if svname == objname:\n",
    "            print(f'-{objname}- saved ({pretty_time_delta(elipsetime)})')\n",
    "        else:\n",
    "            print(f'-{objname}- saved as -{svname}- ({pretty_time_delta(elipsetime)})')\n",
    "\n",
    "def pretty_time_delta(seconds):\n",
    "    seconds = int(seconds)\n",
    "    days, seconds = divmod(seconds, 86400)\n",
    "    hours, seconds = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    if days > 0:\n",
    "        return f'{days}days {hours}h {minutes}m {seconds}s'\n",
    "    elif hours > 0:\n",
    "        return f'{hours}h {minutes}m {seconds}s'\n",
    "    elif minutes > 0:\n",
    "        return f'{minutes}m {seconds}s'\n",
    "    else:\n",
    "        return f'{seconds}s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, preembeddings, targets_df, split_df, gpus, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train', 'val', or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "            gpus: gpus used, should be a list. ex, [0,1]\n",
    "        '''\n",
    "\n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _, yqtr, is_test = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            # print(f'Dateset -> N train: {len(transcriptids)}')\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "            # print(f'Dataset -> N val: {len(transcriptids)}')\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "            # print(f'Dataset -> N test: {len(transcriptids)}')\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "            \n",
    "        # remove last few samples from `valid_preemb_keys` so that it's divisible by the number of gpus\n",
    "        if split_type in ['train', 'val']:\n",
    "            max_valid_preemb_keys_len = len(self.valid_preemb_keys)//len(gpus)*len(gpus)\n",
    "            self.valid_preemb_keys = list(self.valid_preemb_keys)[:(max_valid_preemb_keys_len)]\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        docid = targets.docid\n",
    "\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        # using the normalized features\n",
    "        similarity = targets.similarity_bigram_norm\n",
    "        sentiment = targets.qa_positive_sent_norm\n",
    "        sue = targets.sue_norm\n",
    "        sest = targets.sest_norm        \n",
    "        alpha = targets.alpha_norm\n",
    "        volatility = targets.volatility_norm\n",
    "        mcap = targets.mcap_norm\n",
    "        bm = targets.bm_norm\n",
    "        roa = targets.roa_norm\n",
    "        debt_asset = targets.debt_asset_norm\n",
    "        numest = targets.numest_norm\n",
    "        smedest = targets.smedest_norm\n",
    "        sstdest = targets.sstdest_norm\n",
    "        car_m1_m1 = targets.car_m1_m1_norm\n",
    "        car_m2_m2 = targets.car_m2_m2_norm\n",
    "        car_m30_m3 = targets.car_m30_m3_norm\n",
    "        volume = targets.volume_norm\n",
    "\n",
    "        # using the unnormalized features\n",
    "#         similarity = targets.similarity_bigram\n",
    "#         sentiment = targets.qa_positive_sent\n",
    "#         sue = targets.sue\n",
    "#         sest = targets.sest        \n",
    "#         alpha = targets.alpha\n",
    "#         volatility = targets.volatility\n",
    "#         mcap = targets.mcap\n",
    "#         bm = targets.bm\n",
    "#         roa = targets.roa\n",
    "#         debt_asset = targets.debt_asset\n",
    "#         numest = targets.numest\n",
    "#         smedest = targets.smedest\n",
    "#         sstdest = targets.sstdest\n",
    "#         car_m1_m1 = targets.car_m1_m1\n",
    "#         car_m2_m2 = targets.car_m2_m2\n",
    "#         car_m30_m3 = targets.car_m30_m3\n",
    "#         volume = targets.volume\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "            \n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return (docid, \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        self.hparams = Namespace(**hparams)\n",
    "        \n",
    "    # Dataset\n",
    "    def step(self):\n",
    "        # first, read the preembedding, targests, and split_df\n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.hparams.window, split_type='train', text_in_dataset=self.hparams.text_in_dataset,\n",
    "                                       roll_type=self.hparams.roll_type, print_window=True,\n",
    "                                       preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df, \n",
    "                                       gpus=self.hparams.gpus)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.hparams.window, split_type='val', text_in_dataset=self.hparams.text_in_dataset,\n",
    "                                     roll_type=self.hparams.roll_type, print_window=False,\n",
    "                                     preembeddings=self.preembeddings,\n",
    "                                     targets_df=self.targets_df, split_df=self.split_df, \n",
    "                                     gpus=self.hparams.gpus)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.hparams.window, split_type='test', text_in_dataset=self.hparams.text_in_dataset, \n",
    "                                      roll_type=self.hparams.roll_type, print_window=False,\n",
    "                                      preembeddings=self.preembeddings,\n",
    "                                      targets_df=self.targets_df, split_df=self.split_df, \n",
    "                                      gpus=self.hparams.gpus)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.hparams.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, \n",
    "                          shuffle=True, drop_last=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.hparams.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, num_workers=0, \n",
    "                          pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.hparams.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Model: FeatureMixer\n",
    "class FeatureMixerLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        '''\n",
    "        d_model: the dimension in the FC layers. For text, usually be 1024; for fin-ratios, should be 15\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.size = [bsz, d_model]\n",
    "        '''\n",
    "        x = self.dropout(self.batch_norm(x))\n",
    "        y = F.relu(self.fc(x))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class FrMixerLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        '''\n",
    "        d_model: the dimension in the FC layers. For text, usually be 1024; for fin-ratios, should be 15\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.size = [bsz, d_model]\n",
    "        '''\n",
    "        x = self.dropout(self.batch_norm(x))\n",
    "        y = F.relu(self.fc(x))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class FeatureMixer(nn.Module):\n",
    "    def __init__(self, featuremixer_layer, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([copy.deepcopy(featuremixer_layer) for i in range(n_layers)])\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "            \n",
    "        return output\n",
    "\n",
    "        \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        self.hparams = Namespace(**hparams)\n",
    "        \n",
    "        # check: batch_size//len(gpus)\n",
    "        assert self.hparams.batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`batch_size` must be divisible by `len(gpus)`. Currently batch_size={self.hparams.batch_size}, gpus={self.hparams.gpus}'\n",
    "        # check: val_batch_size//len(gpus)\n",
    "        assert self.hparams.val_batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={self.hparams.val_batch_size}, gpus={self.hparams.gpus}'\n",
    "        \n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "\n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        self.log('val_rmse', rmse, on_step=False)\n",
    "        \n",
    "        \n",
    "        if 'val_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['val_loss_car'] for x in outputs]).mean())\n",
    "            self.log('val_rmse_car', rmse_car, on_step=False)\n",
    "            \n",
    "        if 'val_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['val_loss_inflow'] for x in outputs]).mean())\n",
    "            self.log('val_rmse_inflow', rmse_inflow, on_step=False)\n",
    "\n",
    "        if 'val_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['val_loss_revision'] for x in outputs]).mean())\n",
    "            self.log('val_rmse_revision', rmse_revision, on_step=False)\n",
    "\n",
    "        \n",
    "    \n",
    "    # test step\n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        self.log('test_rmse', rmse, on_step=False)\n",
    "        \n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_car', rmse_car, on_step=False)\n",
    "            \n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_inflow', rmse_inflow, on_step=False)\n",
    "\n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            self.log('test_rmse_revision', rmse_revision, on_step=False)\n",
    "    \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, window_i, model_hparams, train_hparams):\n",
    "    \n",
    "    # set window\n",
    "    model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "    \n",
    "    # init model\n",
    "    model = Model(model_hparams)\n",
    "\n",
    "    # get model type\n",
    "    model_hparams['model_type'] = model.model_type\n",
    "    model_hparams['target_type'] = model.target_type\n",
    "    model_hparams['feature_type'] = model.feature_type\n",
    "    model_hparams['normalize_target'] = model.normalize_target\n",
    "    model_hparams['attn_type'] = model.attn_type\n",
    "    if hasattr(model, 'emb_share'):\n",
    "        model_hparams['emb_share'] = model.emb_share\n",
    "    \n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{train_hparams['note']}_{model_hparams['window']}_\".replace('*',  '')\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=False,\n",
    "        mode='min',\n",
    "        monitor='val_loss',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=train_hparams['save_top_k'],\n",
    "        period=train_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=model_hparams['window'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_hparams['early_stop_patience'],\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=model_hparams['gpus'], \n",
    "                         precision=train_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         early_stop_callback=early_stop_callback,\n",
    "                         overfit_batches=train_hparams['overfit_batches'], \n",
    "                         row_log_interval=train_hparams['row_log_interval'],\n",
    "                         val_check_interval=train_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=1, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=train_hparams['min_epochs'],\n",
    "                         max_epochs=train_hparams['max_epochs'], \n",
    "                         max_steps=train_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # delete unused hparam\n",
    "    if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "    if model.feature_type=='fin-ratio': \n",
    "        model_hparams.pop('max_seq_len',None)\n",
    "        model_hparams.pop('n_layers_encoder',None)\n",
    "        model_hparams.pop('n_head_encoder',None)\n",
    "        model_hparams.pop('d_model',None)\n",
    "        model_hparams.pop('dff',None)\n",
    "    if model.feature_type=='text': \n",
    "        model_hparams.pop('normalize_layer',None)\n",
    "        model_hparams.pop('normalize_batch',None)\n",
    "    if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "    # add n_model_params\n",
    "    train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(train_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, model_hparams['roll_type'], model_hparams['window'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, model_hparams['roll_type'], model_hparams['window'])\n",
    "    \n",
    "    # If run on ASU, upload code explicitly\n",
    "    if train_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # train the model\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # by super().__init__, `self.hparams` will be created\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # attibutes\n",
    "        self.model_type = 'MLP'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'fr+mtxt'\n",
    "        self.normalize_target = True\n",
    "        self.attn_type = 'dotprod'\n",
    "        \n",
    "        # self.text_in_dataset = True if self.feature_type not in ['fr', 'fr+mtxt'] else False \n",
    "        \n",
    "        # dropout layers\n",
    "        # self.dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.fc_1 = nn.Linear(17, 32)\n",
    "        self.fc_2 = nn.Linear(32, 1)\n",
    "        #self.fc_3 = nn.Linear(32, 1)\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, fin_ratios, manual_txt):\n",
    "       \n",
    "        x = torch.cat([fin_ratios, manual_txt], dim=-1) # (N, 2+15)\n",
    "\n",
    "        x_car = F.relu(self.fc_1(x))\n",
    "        # x_car = F.relu(self.fc_1(x_car))\n",
    "        y_car = self.fc_2(x_car) # (N, 1)\n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, manual_txt, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self(fin_ratio, manual_txt) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, manual_txt, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self(fin_ratio, manual_txt) # (N, 1)\n",
    "        \n",
    "        # print(f'y_car from val: {y_car}')\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}    \n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, manual_txt, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio, manual_txt) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading targets...@21:05:52\n",
      "Loading finished. @21:05:55\n",
      "Loading preembeddings...@21:05:55\n",
      "Loading finished. @21:05:55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a13d48d750>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_type': 'sm', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_sentiment_text_norm', # key!\n",
    "    'gpus': [0,1], # key\n",
    "    'seed': 42,    # key\n",
    "    'roll_type': '3y',\n",
    "    'batch_size': 32,\n",
    "    'val_batch_size':32,\n",
    "    'learning_rate': 5e-4,\n",
    "    'task_weight': 1,\n",
    "    'dropout': 0.5,\n",
    "    'text_in_dataset': False\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "train_hparams = {\n",
    "    # checkpoint & log\n",
    "    # last: MLP-14\n",
    "    'machine': 'yu-workstation', # key!\n",
    "    'note': f\"MLP-15,(car~fr+mtxt),hidden=32,hiddenLayer=1,fc_dropout=no,NormCAR=yes,bsz={model_hparams['batch_size']},seed={model_hparams['seed']},log(mcap)=yes,lr={model_hparams['learning_rate']:.2g}\", # key!\n",
    "    'log_every_n_steps': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 10, \n",
    "    'max_epochs': 50, \n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 6,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "# refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "# you have to do this because CCDataset requires it\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "np.random.seed(model_hparams['seed'])\n",
    "torch.manual_seed(model_hparams['seed'])\n",
    "\n",
    "# for window_i in range(len(split_df)):\n",
    "\n",
    "#     # train one window\n",
    "#     train_one(Model, window_i, model_hparams, train_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Checkpoint directory d:/checkpoints/earnings-call exists and is not empty. With save_top_k=1, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "CometLogger will be initialized in online mode\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    }
   ],
   "source": [
    "# loop one\n",
    "window_i = 0\n",
    "    \n",
    "    \n",
    "# set window\n",
    "model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "\n",
    "# init model\n",
    "model = Model(model_hparams)\n",
    "\n",
    "# get model attributes from the Model class\n",
    "model_hparams['model_type'] = model.model_type\n",
    "model_hparams['target_type'] = model.target_type\n",
    "model_hparams['feature_type'] = model.feature_type\n",
    "model_hparams['normalize_target'] = model.normalize_target\n",
    "model_hparams['attn_type'] = model.attn_type\n",
    "if hasattr(model, 'emb_share'):\n",
    "    model_hparams['emb_share'] = model.emb_share\n",
    "\n",
    "# checkpoint\n",
    "ckpt_prefix = f\"{train_hparams['note']}_{model_hparams['window']}_\".replace('*',  '')\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    verbose=False,\n",
    "    mode='min',\n",
    "    monitor='val_loss',\n",
    "    filepath=CHECKPOINT_DIR,\n",
    "    prefix=ckpt_prefix,\n",
    "    save_top_k=train_hparams['save_top_k'],\n",
    "    period=train_hparams['checkpoint_period'])\n",
    "\n",
    "# logger\n",
    "logger = pl.loggers.CometLogger(\n",
    "    api_key=COMET_API_KEY,\n",
    "    save_dir='/data/logs',\n",
    "    project_name='earnings-call',\n",
    "    experiment_name=model_hparams['window'],\n",
    "    workspace='amiao',\n",
    "    display_summary_level=0)\n",
    "\n",
    "# early stop\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=train_hparams['early_stop_patience'],\n",
    "    verbose=False,\n",
    "    mode='min')\n",
    "\n",
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "                     gpus=[0,1], \n",
    "                     precision=train_hparams['precision'],\n",
    "                     checkpoint_callback=checkpoint_callback, \n",
    "                     callbacks=[early_stop_callback],\n",
    "                     overfit_batches=train_hparams['overfit_batches'], \n",
    "                     log_every_n_steps=train_hparams['log_every_n_steps'],\n",
    "                     val_check_interval=train_hparams['val_check_interval'], \n",
    "                     progress_bar_refresh_rate=1, \n",
    "                     accelerator='dp', \n",
    "                     accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                     min_epochs=train_hparams['min_epochs'],\n",
    "                     max_epochs=train_hparams['max_epochs'], \n",
    "                     max_steps=train_hparams['max_steps'], \n",
    "                     # logger=logger\n",
    "                    )\n",
    "\n",
    "\n",
    "# delete unused hparam\n",
    "if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "if model.feature_type=='fin-ratio': \n",
    "    model_hparams.pop('max_seq_len',None)\n",
    "    model_hparams.pop('n_layers_encoder',None)\n",
    "    model_hparams.pop('n_head_encoder',None)\n",
    "    model_hparams.pop('d_model',None)\n",
    "    model_hparams.pop('dff',None)\n",
    "if model.feature_type=='text': \n",
    "    model_hparams.pop('normalize_layer',None)\n",
    "    model_hparams.pop('normalize_batch',None)\n",
    "if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "# add n_model_params\n",
    "train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# upload hparams\n",
    "# logger.experiment.log_parameters(model_hparams)\n",
    "# logger.experiment.log_parameters(train_hparams)\n",
    "\n",
    "# upload ols_rmse (for reference)\n",
    "# log_ols_rmse(logger, model_hparams['roll_type'], model_hparams['window'])\n",
    "\n",
    "# upload test_start\n",
    "# log_test_start(logger, model_hparams['roll_type'], model_hparams['window'])\n",
    "\n",
    "# If run on ASU, upload code explicitly\n",
    "if train_hparams['machine'] == 'ASU':\n",
    "    codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "    assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "    logger.experiment.log_asset(codefile[0])\n",
    "\n",
    "\n",
    "# refresh GPU memory\n",
    "# refresh_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CCDataModule(model_hparams)\n",
    "datamodule.step()\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and test\n",
    "# try:\n",
    "#     # train the model\n",
    "#     trainer.fit(model)\n",
    "\n",
    "#     # test on the best model\n",
    "#     trainer.test(ckpt_path='best')\n",
    "\n",
    "# except RuntimeError as e:\n",
    "#     raise e\n",
    "# finally:\n",
    "#     del model, trainer\n",
    "#     refresh_cuda_memory()\n",
    "#     logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = torch.nn.Linear(128, 256)\n",
    "        self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "        print(x)\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download only\n",
    "        MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
    "        MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # transform\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "        MNIST(os.getcwd(), train=True, download=False, transform=transform)\n",
    "        MNIST(os.getcwd(), train=False, download=False, transform=transform)\n",
    "\n",
    "        # train/val split\n",
    "        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "\n",
    "        # assign to use in dataloaders\n",
    "        self.train_dataset = mnist_train\n",
    "        self.val_dataset = mnist_val\n",
    "        self.test_dataset = mnist_test\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "# prepare transforms standard to MNIST\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# data\n",
    "mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "\n",
    "# dm = MNISTDataModule()\n",
    "# dm.prepare_data()\n",
    "# dm.setup('fit')\n",
    "\n",
    "model = LitMNIST()\n",
    "trainer = pl.Trainer(gpus=-1, accelerator='dp')\n",
    "trainer.fit(model, mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}