{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import sentence_transformers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from spacy.lang.en import English\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = 'd:/checkpoints/earnings-call'\n",
    "CHECKPOINT_TEMP_DIR = f'{ROOT_DIR}/checkpoint/earnings-call/temp'\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# helper functions\n",
    "class Now:\n",
    "    '''return current datetime, but has a more pretty format\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_datetime = datetime.now()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.current_datetime.strftime('%H:%M:%S')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `val` and `train` are of same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, preembeddings, targets_df, split_df, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "        '''\n",
    "\n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        docid = targets.docid\n",
    "        \n",
    "        sue = targets.sue\n",
    "        sest = targets.sest\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        alpha = targets.alpha\n",
    "        volatility = targets.volatility\n",
    "        mcap = targets.mcap/1e6\n",
    "        bm = targets.bm\n",
    "        roa = targets.roa\n",
    "        debt_asset = targets.debt_asset\n",
    "        numest = targets.numest\n",
    "        smedest = targets.smedest\n",
    "        sstdest = targets.sstdest\n",
    "        car_m1_m1 = targets.car_m1_m1\n",
    "        car_m2_m2 = targets.car_m2_m2\n",
    "        car_m30_m3 = targets.car_m30_m3\n",
    "        volume = targets.volume\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "            \n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return docid, \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, window_i, model_hparams, train_hparams):\n",
    "    \n",
    "    # set window\n",
    "    model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "    \n",
    "    # init model\n",
    "    model = Model(model_hparams)\n",
    "\n",
    "    # get model type\n",
    "    model_hparams['model_type'] = model.model_type\n",
    "    model_hparams['target_type'] = model.target_type\n",
    "    model_hparams['feature_type'] = model.feature_type\n",
    "    model_hparams['normalize_target'] = model.normalize_target\n",
    "    model_hparams['attn_type'] = model.attn_type\n",
    "    if hasattr(model, 'emb_share'):\n",
    "        model_hparams['emb_share'] = model.emb_share\n",
    "    \n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{model_hparams['model_type']}_{model_hparams['window']}_\"\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        monitor='val_loss',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=train_hparams['save_top_k'],\n",
    "        period=train_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=model_hparams['window'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_hparams['early_stop_patience'],\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=-1, \n",
    "                         precision=train_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         early_stop_callback=early_stop_callback,\n",
    "                         overfit_batches=train_hparams['overfit_batches'], \n",
    "                         row_log_interval=train_hparams['row_log_interval'],\n",
    "                         val_check_interval=train_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=5, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=train_hparams['min_epochs'],\n",
    "                         max_epochs=train_hparams['max_epochs'], \n",
    "                         max_steps=train_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # delete unused hparam\n",
    "    if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "    if model.feature_type=='fin-ratio': \n",
    "        model_hparams.pop('max_seq_len',None)\n",
    "        model_hparams.pop('n_layers_encoder',None)\n",
    "        model_hparams.pop('n_head_encoder',None)\n",
    "        model_hparams.pop('d_model',None)\n",
    "        model_hparams.pop('dff',None)\n",
    "    if model.feature_type=='text': \n",
    "        model_hparams.pop('normalize_layer',None)\n",
    "        model_hparams.pop('normalize_batch',None)\n",
    "    if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "    # add n_model_params\n",
    "    train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(train_hparams)\n",
    "\n",
    "    # If run on ASU, upload code explicitly\n",
    "    if train_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    # refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # train the model\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        # refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = Namespace(**hparams)\n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        \n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "\n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'val_rmse': rmse}\n",
    "        \n",
    "        if 'val_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['val_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_car'] = rmse_car\n",
    "            \n",
    "        if 'val_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['val_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_inflow'] = rmse_inflow\n",
    "\n",
    "        if 'val_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['val_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_revision'] = rmse_revision\n",
    "\n",
    "        return {'val_loss': mse, 'log': log_dict}\n",
    "    \n",
    "    # test step\n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'test_rmse': rmse}\n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_car'] = rmse_car\n",
    "\n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_inflow'] = rmse_inflow\n",
    "            \n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_revision'] = rmse_revision\n",
    "            \n",
    "        return {'test_loss': mse, 'log': log_dict, 'progress_bar':log_dict}\n",
    "    \n",
    "    # Dataset\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.hparams.window, split_type='train', text_in_dataset=self.text_in_dataset,\n",
    "                                       roll_type=self.hparams.roll_type, print_window=True, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "        self.val_dataset = CCDataset(self.hparams.window, split_type='val', text_in_dataset=self.text_in_dataset,\n",
    "                                     roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "        self.test_dataset = CCDataset(self.hparams.window, split_type='test', text_in_dataset=self.text_in_dataset, \n",
    "                                      roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df)\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)\n",
    "        \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers: load targets\n",
    "def load_targets(targets_name):\n",
    "    if 'targets_df' not in globals():\n",
    "        print(f'Loading targets...@{Now()}')\n",
    "        globals()['targets_df'] = pd.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_type):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_type}.pt\")\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    globals()['split_df'] = split_df.loc[split_df.roll_type==roll_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading targets...@19:24:37\n",
      "Loading finished. @19:24:44\n",
      "Loading preembeddings...@19:24:44\n",
      "Loading finished. @19:25:02\n"
     ]
    }
   ],
   "source": [
    "model_hparams = {\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm', # key!\n",
    "    'roll_type': '3y',  # key!\n",
    "}    \n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MTL, hardshare) x*car + (1-x)*inf ~ txt + fr\n",
    "class CCTransformerMTLInfHard(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car+inf'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.emb_share = 'hard'\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.linear_car_1 = nn.Linear(self.hparams.d_model, self.hparams.d_model)\n",
    "        self.linear_car_2 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        self.linear_car_3 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_4 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.linear_car_5 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        self.linear_inflow = nn.Linear(self.hparams.final_tdim, 1)\n",
    "        # self.linear_revision = nn.Linear(hparam.final_tdim, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.final_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        self.final_dropout_3 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(self.hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        bsz, embed_dim = embeddings.size(0), embeddings.size(2)\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # multiply with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # mix with covariate\n",
    "        x_expert = self.final_dropout_1(F.relu(self.linear_car_1(x_expert))) # (N, E)\n",
    "        x_expert = F.relu(self.linear_car_2(x_expert)) # (N, final_tdim)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratio = self.batch_norm(fin_ratios)\n",
    "        \n",
    "        x_car = torch.cat([x_expert, fin_ratios], dim=-1) # (N, X + final_tdim) where X is the number of covariate (n_covariate)\n",
    "\n",
    "        # ouput y\n",
    "        y_inflow = self.linear_inflow(x_expert)\n",
    "        \n",
    "        x_car = self.final_dropout_2(F.relu(self.linear_car_3(x_car))) # (N, X + final_tdim)\n",
    "        y_car = self.linear_car_5(x_car) # (N,1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car, y_inflow\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        \n",
    "        assert self.hparams.car_weight+self.hparams.inflow_weight==1, 'car_weight + inflow_weight != 1'\n",
    "        \n",
    "        loss = self.hparams.car_weight*loss_car + self.hparams.inflow_weight*loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss, 'val_loss_car': loss_car, 'val_loss_inflow': loss_inflow}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car, y_inflow = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        loss_inflow = self.mse_loss(y_inflow, inflow_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        loss = loss_car + loss_inflow\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss, 'test_loss_car': loss_car, 'test_loss_inflow': loss_inflow}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL-text-fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.normalize_target = True\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for SUE, CAR, SELEAD, SEST\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, self.hparams.final_tdim+self.n_covariate)\n",
    "        self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(self.hparams.final_tdim+self.n_covariate)\n",
    "            \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        bsz, embed_dim = embeddings.size(0), embeddings.size(2)\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # batch norm fin-ratios\n",
    "        if self.hparams.normalize_batch:\n",
    "            fin_ratios = self.batch_norm(fin_ratios)\n",
    "        \n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_covariate)\n",
    "\n",
    "        # final FC\n",
    "        x_final = self.fc_dropout_1(F.relu(self.fc_1(x_final))) # (N, E+X)\n",
    "        y_car = self.fc_2(x_final) # (N, 1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # get batch size\n",
    "        bsz = fin_ratios.size(0)\n",
    "\n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class CCMLP(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # by super().__init__, `self.hparams` will be created\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # attibutes\n",
    "        self.model_type = 'MLP'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'fr'\n",
    "        self.normalize_target = True\n",
    "        self.attn_type = 'dotprod'\n",
    "        \n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout1 = nn.Dropout(self.hparams.dropout)\n",
    "        self.dropout2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # fc layers\n",
    "        self.linear1 = nn.Linear(15, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            self.batch_norm = nn.BatchNorm1d(15)\n",
    "            \n",
    "        # layer normalization\n",
    "        if self.hparams.normalize_layer:\n",
    "            self.layer_norm = nn.LayerNorm(15)\n",
    "            \n",
    "    # forward\n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # batch normalization\n",
    "        if self.hparams.normalize_batch:\n",
    "            inp = self.batch_norm(inp)\n",
    "            \n",
    "        # layer norm\n",
    "        if self.hparams.normalize_layer:\n",
    "            inp = self.layer_norm(inp)\n",
    "        \n",
    "        x_car = self.dropout1(F.relu(self.linear1(inp)))\n",
    "        x_car = self.dropout2(F.relu(self.linear2(x_car)))\n",
    "        y_car = self.linear3(x_car) # (N, 1)\n",
    "        \n",
    "        \n",
    "        return y_car\n",
    "    \n",
    "        \n",
    "    # train step\n",
    "    def training_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'val_loss': loss_car}    \n",
    "        \n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        \n",
    "        _, car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(fin_ratio) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)) # ()\n",
    "        \n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# choose Model\n",
    "Model = CCMLP\n",
    "\n",
    "# model hparams\n",
    "model_hparams = {\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm', # key!\n",
    "    'normalize_batch': True, # key!\n",
    "    'normalize_layer': False, # key!\n",
    "    'roll_type': '3y',\n",
    "    'batch_size': 128, # 128\n",
    "    'val_batch_size':2,\n",
    "    'learning_rate': 5e-4,\n",
    "    'task_weight': 1,\n",
    "    'dropout': 0.5,\n",
    "}\n",
    "\n",
    "# train hparams\n",
    "train_hparams = {\n",
    "    # checkpoint & log\n",
    "    'note': 'xxxx', # key!\n",
    "    'row_log_interval': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 1.0,\n",
    "    'machine': 'ASU',\n",
    "\n",
    "    # data size\n",
    "    'precision': 32, # key!\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 1, \n",
    "    'max_epochs': 1, \n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 1}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "# refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "# you have to do this because CCDataset requires it\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df))[5:6]:\n",
    "\n",
    "    # train one window\n",
    "    train_one(Model, window_i, model_hparams, train_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CCTransformerMTLInfHard.load_from_checkpoint(\"D:\\Checkpoints\\earnings-call\\MTL-01-3y-(0.5car+0.5inf~txt+fr)-hardshare-norm\\TSFM_roll-01_epoch=9_v0.ckpt\")\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x18b3a7bf108>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (3y) \n",
      "(train: 2008-01-01 to 2010-12-31) (test: 2011-01-01 to 2011-03-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e3a4eb322d452b8da14061ff39f1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-7cf2a2bc72ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcar_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_ratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "def extractor_hook(m, i, o):\n",
    "    features.append(o)\n",
    "    \n",
    "dict(model.named_children()).get('linear_car_2').register_forward_hook(extractor_hook)\n",
    "\n",
    "model.prepare_data()\n",
    "dl = model.test_dataloader()\n",
    "\n",
    "for i, batch in enumerate(tqdm(dl)):\n",
    "    if i > 100: break\n",
    "    \n",
    "    _, car, car_norm, fin_ratios = batch\n",
    "\n",
    "    # forward\n",
    "    y_car = model(fin_ratios) # (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
