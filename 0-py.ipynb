{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from argparse import Namespace\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.targets_df = pd.read_feather('data/f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm.feather')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.targets_df.iloc[idx]\n",
    "        \n",
    "        # t\n",
    "        car_0_30 = line.car_0_30\n",
    "        car_0_30_norm = line.car_0_30_norm\n",
    "        \n",
    "        # x\n",
    "        sue = line.sue\n",
    "        sest = line.sest\n",
    "        alpha = line.alpha\n",
    "        volatility = line.volatility\n",
    "        mcap = line.mcap/1e6\n",
    "        bm = line.bm\n",
    "        roa = line.roa\n",
    "        debt_asset = line.debt_asset\n",
    "        numest = line.numest\n",
    "        smedest = line.smedest\n",
    "        sstdest = line.sstdest\n",
    "        car_m1_m1 = line.car_m1_m1\n",
    "        car_m2_m2 = line.car_m2_m2\n",
    "        car_m30_m3 = line.car_m30_m3\n",
    "        volume = line.volume\n",
    "\n",
    "        return torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "               torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "               torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TestDataset(data_path='data/f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        hparams = Namespace(**hparams)\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        # only one layer\n",
    "        self.hidden_1 = nn.Linear(15, 64)\n",
    "        self.hidden_2 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        full_dataset = TestDataset(self.hparams.data_path)\n",
    "        n_dataset = len(full_dataset)\n",
    "        n_train = int(n_dataset*0.8)\n",
    "        n_val = n_dataset - n_train\n",
    "        \n",
    "        train_idx = range(n_train)\n",
    "        val_idx = range(n_train, len(full_dataset))\n",
    "        \n",
    "        assert len(train_idx)+len(val_idx)==len(full_dataset)\n",
    "        \n",
    "        print(f'n_train={n_train}, n_val={n_val}')\n",
    "        \n",
    "        self.train_dataset = Subset(full_dataset, train_idx)\n",
    "        self.val_dataset = Subset(full_dataset, val_idx)\n",
    "        self.test_dataset = self.train_dataset\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=True)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, shuffle=False, drop_last=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    def forward(self, fin_ratio):\n",
    "        x = self.hidden_1(fin_ratio)\n",
    "        x = self.hidden_2(x)\n",
    "        y = self.output(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   \n",
    "    \n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        y_car = self.forward(fin_ratio)\n",
    "        loss = self.loss(y_car, car.unsqueeze(-1))\n",
    "        \n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        y_car = self.forward(fin_ratio)\n",
    "        loss = self.loss(y_car, car.unsqueeze(-1))\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, fin_ratio = batch\n",
    "        \n",
    "        y_car = self.forward(fin_ratio)\n",
    "        loss = self.loss(y_car, car.unsqueeze(-1))\n",
    "        \n",
    "        return {'test_loss': loss}\n",
    "    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        return {'val_loss': mse, 'log': {'val_rmse': rmse}}   \n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "\n",
    "        return {'test_loss': mse, 'log': {'test_rmse': rmse}, 'progress_bar':{'test_rmse': rmse}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0,1]\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/36ddc6b9ef1c4fd6a63c187f587cb21e\n",
      "\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | hidden_1 | Linear | 1 K   \n",
      "1 | hidden_2 | Linear | 4 K   \n",
      "2 | output   | Linear | 65    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train=17410, n_val=4353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35e6418a4a1410db51605d5bfcac5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 156.67194 (best 156.67194), saving model to d:/Checkpoints/earnings-call/test_ckpt\\car_epoch=0.ckpt as top 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 153.81688 (best 153.81688), saving model to d:/Checkpoints/earnings-call/test_ckpt\\car_epoch=1.ckpt as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hparams\n",
    "model_hparams = {\n",
    "    'data_path': 'data/f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm.feather',\n",
    "    'learning_rate': 1e-3,\n",
    "    'val_batch_size': 16, \n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "train_hparams = {\n",
    "    'max_epochs': 2,\n",
    "    'note': 'car',\n",
    "    'save_top_k': 5,\n",
    "    'ckpt_period': 1, # save every epoch\n",
    "    'ckpt_path': 'd:/Checkpoints/earnings-call/test_ckpt'\n",
    "}\n",
    "\n",
    "# logger\n",
    "logger = pl.loggers.CometLogger(\n",
    "    api_key=os.getenv('COMET_API_KEY'),\n",
    "    project_name='earnings-call',\n",
    "    workspace='amiao',\n",
    "    display_summary_level=0)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    monitor='val_loss',\n",
    "    filepath=train_hparams['ckpt_path'],\n",
    "    prefix=f\"{train_hparams['note']}_\",\n",
    "    save_top_k=train_hparams['save_top_k'],\n",
    "    period=train_hparams['ckpt_period'])\n",
    "\n",
    "    # logger\n",
    "\n",
    "# trainner\n",
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    gpus=-1, \n",
    "    progress_bar_refresh_rate=2, \n",
    "    distributed_backend='dp', \n",
    "    max_epochs=train_hparams['max_epochs'], \n",
    "    logger=logger)\n",
    "\n",
    "# init model\n",
    "model = Test(model_hparams)\n",
    "\n",
    "# upload logger\n",
    "logger.experiment.log_parameters(model_hparams)\n",
    "logger.experiment.log_parameters(train_hparams)\n",
    "\n",
    "# fit\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd:/Checkpoints/earnings-call/test_ckpt\\\\car_epoch=1.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4a46c43bbc41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, test_dataloaders, ckpt_path)\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'best'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m                 \u001b[0mckpt_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\core\\saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, tags_csv, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# add the hparams from csv file to checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\utilities\\cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrive\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# no scheme or with a drive letter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd:/Checkpoints/earnings-call/test_ckpt\\\\car_epoch=1.ckpt'"
     ]
    }
   ],
   "source": [
    "trainer.test(ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
