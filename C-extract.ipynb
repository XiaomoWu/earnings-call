{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: .\n",
      "DATA_DIR: ./data\n",
      "CHECKPOINT_DIR: d:/checkpoints/earnings-call\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 11.00 (GB)\n",
      "GPU 1:  0.00/ 11.00 (GB)\n",
      "\n",
      "CPU count (physical): 16\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import comet_ml\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import spacy\n",
    "import sentence_transformers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from spacy.lang.en import English\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = 'd:/checkpoints/earnings-call'\n",
    "CHECKPOINT_TEMP_DIR = f'{ROOT_DIR}/checkpoint/earnings-call/temp'\n",
    "\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# COMET API KEY\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');\n",
    "    \n",
    "cpu = torch.device('cpu');\n",
    "n_cpu = int(mp.cpu_count()/2);\n",
    "\n",
    "print(f'\\nCPU count (physical): {n_cpu}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=cpu:\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name):\n",
    "    if 'targets_df' not in globals():\n",
    "        print(f'Loading targets...@{Now()}')\n",
    "        globals()['targets_df'] = pd.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_type):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_type}.pt\")\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    globals()['split_df'] = split_df.loc[split_df.roll_type==roll_type]\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, window):\n",
    "    '''\n",
    "    Given window, find the corresponding ols_rmse from `bench_fr.feather`, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    bench_fr = pd.read_feather('data/bench_fr.feather')\n",
    "\n",
    "    ols_rmse_norm = bench_fr.loc[bench_fr.window==window].test_rmse_norm.to_list()[0]\n",
    "    logger.experiment.log_parameter('ols_rmse_norm', ols_rmse_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "m = re.search('earnings-call/(\\w{3}-\\d{2})', 'D:/Checkpoints/earnings-call/STL-08,(car~txt),fc=1,NormCAR=yes,BN=no,bsz=28,lr=3e-4')\n",
    "len(m.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, window_i, model_hparams, train_hparams):\n",
    "    \n",
    "    # set window\n",
    "    model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "    \n",
    "    # get encoder ID (e.g.: STL-08)\n",
    "    m = re.search('earnings-call/(\\w{3}-\\d{2})', model_hparams['ckpt_dir'])\n",
    "    assert len(m.groups())==1, \"Can't find text encoder ID (e.g., STL-08)\"\n",
    "    model_hparams['encoder'] = m.groups()[0]\n",
    "    \n",
    "    # init model\n",
    "    model = Model(model_hparams)\n",
    "\n",
    "    # get model type\n",
    "    model_hparams['model_type'] = model.model_type\n",
    "    model_hparams['target_type'] = model.target_type\n",
    "    model_hparams['feature_type'] = model.feature_type\n",
    "    model_hparams['normalize_target'] = model.normalize_target\n",
    "    \n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{train_hparams['note']}_{model_hparams['window']}_\".replace('*',  '')\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=False,\n",
    "        mode='min',\n",
    "        monitor='val_loss',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=train_hparams['save_top_k'],\n",
    "        period=train_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=model_hparams['window'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_hparams['early_stop_patience'],\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=model_hparams['gpus'], \n",
    "                         precision=train_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         early_stop_callback=early_stop_callback,\n",
    "                         overfit_batches=train_hparams['overfit_batches'], \n",
    "                         row_log_interval=train_hparams['row_log_interval'],\n",
    "                         val_check_interval=train_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=1, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=train_hparams['min_epochs'],\n",
    "                         max_epochs=train_hparams['max_epochs'], \n",
    "                         max_steps=train_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # delete unused hparam\n",
    "    if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "    if model.feature_type=='fin-ratio': \n",
    "        model_hparams.pop('max_seq_len',None)\n",
    "        model_hparams.pop('n_layers_encoder',None)\n",
    "        model_hparams.pop('n_head_encoder',None)\n",
    "        model_hparams.pop('d_model',None)\n",
    "        model_hparams.pop('dff',None)\n",
    "    if model.feature_type=='text': \n",
    "        model_hparams.pop('normalize_layer',None)\n",
    "        model_hparams.pop('normalize_batch',None)\n",
    "    if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "    # add n_model_params\n",
    "    train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(train_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, model_hparams['window'])\n",
    "    \n",
    "    # upload test_r2\n",
    "    \n",
    "\n",
    "    # If run on ASU, upload code explicitly\n",
    "    if train_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # train the model\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `val` and `train` are of same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, preembeddings, targets_df, split_df, gpus, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "            gpus: gpus used, should be a list. ex, [0,1]\n",
    "        '''\n",
    "\n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "            \n",
    "        # remove last few samples from `valid_preemb_keys` so that it's divisible by the number of gpus\n",
    "        max_valid_preemb_keys_len = len(self.valid_preemb_keys)//len(gpus)*len(gpus)\n",
    "        self.valid_preemb_keys = list(self.valid_preemb_keys)[:(max_valid_preemb_keys_len)]\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        docid = targets.docid\n",
    "        \n",
    "        sue = targets.sue_norm\n",
    "        sest = targets.sest_norm\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        alpha = targets.alpha_norm\n",
    "        volatility = targets.volatility_norm\n",
    "        mcap = targets.mcap_norm\n",
    "        bm = targets.bm_norm\n",
    "        roa = targets.roa_norm\n",
    "        debt_asset = targets.debt_asset_norm\n",
    "        numest = targets.numest_norm\n",
    "        smedest = targets.smedest_norm\n",
    "        sstdest = targets.sstdest_norm\n",
    "        car_m1_m1 = targets.car_m1_m1_norm\n",
    "        car_m2_m2 = targets.car_m2_m2_norm\n",
    "        car_m30_m3 = targets.car_m30_m3_norm\n",
    "        volume = targets.volume_norm\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "            \n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return docid, \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        self.hparams = Namespace(**hparams)\n",
    "        \n",
    "        # check: batch_size//len(gpus)\n",
    "        assert self.hparams.batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`batch_size` must be divisible by `len(gpus)`. Currently batch_size={self.hparams.batch_size}, gpus={self.hparams.gpus}'\n",
    "        # check: val_batch_size//len(gpus)\n",
    "        assert self.hparams.val_batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`val_batch_size` must be divisible by `len(gpus)`. Currently batch_size={self.hparams.val_batch_size}, gpus={self.hparams.gpus}'\n",
    "        \n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "\n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'val_rmse': rmse}\n",
    "        \n",
    "        if 'val_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['val_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_car'] = rmse_car\n",
    "            \n",
    "        if 'val_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['val_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_inflow'] = rmse_inflow\n",
    "\n",
    "        if 'val_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['val_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_revision'] = rmse_revision\n",
    "\n",
    "        return {'val_loss': mse, 'log': log_dict}\n",
    "    \n",
    "    # test step\n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'test_rmse': rmse}\n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_car'] = rmse_car\n",
    "\n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_inflow'] = rmse_inflow\n",
    "            \n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_revision'] = rmse_revision\n",
    "            \n",
    "        return {'test_loss': mse, 'log': log_dict, 'progress_bar':log_dict}\n",
    "    \n",
    "    # Dataset\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.hparams.window, split_type='train', text_in_dataset=self.text_in_dataset,\n",
    "                                       roll_type=self.hparams.roll_type, print_window=True, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.hparams.window, split_type='val', text_in_dataset=self.text_in_dataset,\n",
    "                                     roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.hparams.window, split_type='test', text_in_dataset=self.text_in_dataset, \n",
    "                                      roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)\n",
    "        \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "ys = pd.DataFrame()\n",
    "features = {}\n",
    "\n",
    "for model in models[:1]:\n",
    "\n",
    "    # freeze model\n",
    "    model.freeze()\n",
    "\n",
    "    # create test dataloader\n",
    "    model.prepare_data()\n",
    "    dl = model.test_dataloader()\n",
    "\n",
    "    # hook: feature extractor\n",
    "    def feature_hook(m, i, o):\n",
    "        feature.append(o.data.squeeze())\n",
    "        \n",
    "    model._modules.get('identity_1').register_forward_hook(feature_hook);\n",
    "\n",
    "\n",
    "    # forward: make prediction & extract features\n",
    "    y = []\n",
    "    feature = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        if i > 100: break\n",
    "\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "\n",
    "        # get docid\n",
    "        transcriptid = transcriptid.int().item()\n",
    "        docid = targets_df.loc[targets_df.transcriptid==transcriptid].docid.iloc[0]\n",
    "\n",
    "        # forward\n",
    "        y_car = model(embeddings, mask, fin_ratios) # (N, 1)\n",
    "        y.append((docid, y_car.item())) \n",
    "\n",
    "    # convert `y` to DataFrame\n",
    "    y = pd.DataFrame(y, columns=['docid', 'car'])\n",
    "    # add docid for `features`\n",
    "    feature = {k: v for k, v in zip(y.docid, feature)}\n",
    "        \n",
    "        \n",
    "    # gether results\n",
    "    ys = pd.concat([ys, y])\n",
    "    features.update(feature)\n",
    "\n",
    "# reset_index for `ys`\n",
    "ys = ys.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt\n",
    "class ExtSTLTxt(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # check: hparams.car_weight == 1\n",
    "        assert self.hparams.car_weight == 1, f'car_weight must be 1. Currently car_weight={self.hparams.car_weight}'\n",
    "\n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'txt'\n",
    "        self.normalize_target = True\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for SUE, CAR, SELEAD, SEST\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model, self.hparams.d_model)\n",
    "        self.fc_2 = nn.Linear(self.hparams.d_model, 1)\n",
    "        \n",
    "        # identity (for feature extraction)\n",
    "        self.identity_1 = nn.Identity()\n",
    "        \n",
    "        \n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # copy x_expert so that it's the output of a layer\n",
    "        x_expert = self.identity_1(x_expert)\n",
    "        \n",
    "        # final output\n",
    "        return x_expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfer(CC):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSF'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.normalize_target = True\n",
    "        \n",
    "        # num of fin_ratios\n",
    "        self.n_fr = 15\n",
    "        \n",
    "        # initialize txt encoder\n",
    "        self.txt_encoder = self.init_encoder()\n",
    "        self.txt_encoder.freeze() # (N,E)\n",
    "        \n",
    "        # fc\n",
    "        self.fc_1 = nn.Linear(self.hparams.d_model+self.hparams.n_fr, 1)\n",
    "        \n",
    "    # init text encoder (load ckpt)\n",
    "    def init_encoder(self):\n",
    "        ckpt_dir = self.hparams.ckpt_dir\n",
    "        window = self.hparams.window\n",
    "        encoder = self.hparams.encoder # e.g., \"STL-08\"\n",
    "        \n",
    "        ckpt_path = [path for path in os.listdir(ckpt_dir) if re.search(f'{encoder}.+{window}.+\\\\.ckpt', path)]\n",
    "        assert len(ckpt_path)==1, f\"Can't find checkpoint for encoder={encoder} and window={window}\"\n",
    "        ckpt_path = ckpt_path[0]\n",
    "        \n",
    "        model = self.hparams.extract_model.load_from_checkpoint(ckpt_path)\n",
    "        model.freeze()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        # encode text\n",
    "        x_expert = self.txt_encoder(embeddings, src_key_padding_mask)\n",
    "        \n",
    "        # concat with fin_ratios\n",
    "        x_expert = torch.cat([x_expert, fin_ratios], dim=-1)\n",
    "        \n",
    "        # make prediction\n",
    "        y_car = self.fc_1(x_expert)\n",
    "        \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose Model\n",
    "Model = Transfer # key\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'encoder_model': ExtSTLTxt,\n",
    "    'ckpt_dir': 'D:/Checkpoints/earnings-call/STL-08,(car~txt),fc=1,NormCAR=yes,BN=no,bsz=28,lr=3e-4', # key! \n",
    "    \n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_text_norm', # key!\n",
    "    'roll_type': '3y',  # key!\n",
    "    'gpus': [0,1],\n",
    "    \n",
    "    # task weight\n",
    "    'car_weight': 1,   # Key!\n",
    "    'inflow_weight': 0, # key!\n",
    "    \n",
    "    'batch_size': 28,\n",
    "    'val_batch_size': 64,\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':1e-4, \n",
    "    'task_weight': 1,\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 1024,\n",
    "    'final_tdim': 1024, \n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} \n",
    "\n",
    "train_hparams = {\n",
    "    # log\n",
    "    'machine': 'yu-workstation',  # key!\n",
    "    'note': f\"TSF-01,(txt~car),NormCAR=yes,bsz={model_hparams['batch_size']},lr={model_hparams['learning_rate']:.2g}\", # key!\n",
    "    'row_log_interval': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32,\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3,\n",
    "    'max_epochs': 20,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 7,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "for window_i in range(len(split_df)):\n",
    "\n",
    "    # train one window\n",
    "    train_one(Model, window_i, model_hparams, train_hparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
