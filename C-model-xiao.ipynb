{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: .\n",
      "DATA_DIR: ./data\n",
      "CHECKPOINT_DIR: ./checkpoint/earnings-call\n",
      "\n",
      "2 GPUs found:\n",
      "    GeForce RTX 2080 Ti (cuda0)\n",
      "    GeForce RTX 2080 Ti (cuda1)\n",
      "\n",
      "GPU memory:\n",
      "GPU 0:  0.00/ 11.00 (GB)\n",
      "GPU 1:  0.00/ 11.00 (GB)\n",
      "\n",
      "CPU count (physical): 16\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import collections\n",
    "import functools\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import requests\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import comet_ml\n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import comet_ml\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import shutil\n",
    "import sklearn\n",
    "import torch\n",
    "from collections import OrderedDict, defaultdict\n",
    "from argparse import Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm.auto import trange, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "# helper functions\n",
    "class Now:\n",
    "    '''return current datetime, but has a more pretty format\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_datetime = datetime.now()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.current_datetime.strftime('%H:%M:%S')    \n",
    "\n",
    "# working directory\n",
    "ROOT_DIR = '.'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'\n",
    "CHECKPOINT_DIR = f'{ROOT_DIR}/checkpoint/earnings-call'\n",
    "CHECKPOINT_TEMP_DIR = f'{ROOT_DIR}/checkpoint/earnings-call/temp'\n",
    "print(f'ROOT_DIR: {ROOT_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'CHECKPOINT_DIR: {CHECKPOINT_DIR}')\n",
    "\n",
    "# Comet API key\n",
    "COMET_API_KEY = 'tOoHzzV1S039683RxEr2Hl9PX'\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);\n",
    "torch.backends.cudnn.deterministic = False;\n",
    "torch.backends.cudnn.benchmark = True;\n",
    "\n",
    "# set device 'cuda' or 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    n_cuda = torch.cuda.device_count();\n",
    "    \n",
    "    def log_gpu_memory(verbose=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            for _ in range(n_cuda):\n",
    "                print(f'GPU {_}:')\n",
    "                print(f'{torch.cuda.memory_summary(_, abbreviated=True)}')\n",
    "        else:\n",
    "            for _ in range(n_cuda):\n",
    "                memory_total = torch.cuda.get_device_properties(_).total_memory/(1024**3)\n",
    "                memory_allocated = torch.cuda.memory_allocated(_)/(1024**3)\n",
    "                print(f'GPU {_}: {memory_allocated: .2f}/{memory_total: .2f} (GB)')\n",
    "            \n",
    "    print(f'\\n{n_cuda} GPUs found:');\n",
    "    for _ in range(n_cuda):\n",
    "        globals()[f'cuda{_}'] = torch.device(f'cuda:{_}');\n",
    "        print(f'    {torch.cuda.get_device_name(_)} (cuda{_})');\n",
    "        \n",
    "    print('\\nGPU memory:');\n",
    "    log_gpu_memory();\n",
    "else:\n",
    "    print('GPU NOT enabled');\n",
    "    \n",
    "cpu = torch.device('cpu');\n",
    "n_cpu = int(mp.cpu_count()/2);\n",
    "\n",
    "print(f'\\nCPU count (physical): {n_cpu}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: refresh cuda memory\n",
    "def refresh_cuda_memory():\n",
    "    \"\"\"\n",
    "    Re-allocate all cuda memory to help alleviate fragmentation\n",
    "    \"\"\"\n",
    "    # Run a full garbage collect first so any dangling tensors are released\n",
    "    gc.collect()\n",
    "\n",
    "    # Then move all tensors to the CPU\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.device!=cpu:\n",
    "            obj.data = torch.empty(0)\n",
    "            if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
    "                obj.grad.data = torch.empty(0)\n",
    "\n",
    "    # Now empty the cache to flush the allocator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# helper: flush chpt\n",
    "def refresh_ckpt():\n",
    "    '''\n",
    "    move all `.ckpt` files to `/temp`\n",
    "    '''\n",
    "    # create ckpt_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "    # create ckpt_temp_dir if not exists\n",
    "    if not os.path.exists(CHECKPOINT_TEMP_DIR):\n",
    "        os.makedirs(CHECKPOINT_TEMP_DIR)\n",
    "    \n",
    "    for name in os.listdir(CHECKPOINT_DIR):\n",
    "        if name.endswith('.ckpt'):\n",
    "            shutil.move(f'{CHECKPOINT_DIR}/{name}', f'{CHECKPOINT_DIR}/temp/{name}')\n",
    "\n",
    "# helpers: load targets\n",
    "def load_targets(targets_name):\n",
    "    if 'targets_df' not in globals():\n",
    "        print(f'Loading targets...@{Now()}')\n",
    "        globals()['targets_df'] = pd.read_feather(f'{DATA_DIR}/{targets_name}.feather')\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load preembeddings\n",
    "def load_preembeddings(preembedding_type):\n",
    "    if 'preembeddings' not in globals():\n",
    "        print(f'Loading preembeddings...@{Now()}')\n",
    "        globals()['preembeddings'] = torch.load(f\"{DATA_DIR}/embeddings/preembeddings_{preembedding_type}.pt\")\n",
    "        print(f'Loading finished. @{Now()}')\n",
    "        \n",
    "# helpers: load split_df\n",
    "def load_split_df(roll_type):\n",
    "    split_df = pd.read_csv(f'{DATA_DIR}/split_dates.csv')\n",
    "    globals()['split_df'] = split_df.loc[split_df.roll_type==roll_type]\n",
    "    \n",
    "# helper: log_ols_rmse\n",
    "def log_ols_rmse(logger, window):\n",
    "    '''\n",
    "    Given window, find the corresponding ols_rmse from `bench_fr.feather`, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    bench_fr = pd.read_feather('data/bench_fr.feather')\n",
    "\n",
    "    ols_rmse_norm = bench_fr.loc[(bench_fr.roll_type=='3y') & (bench_fr.window==window)].test_rmse_fr_norm.to_list()[0]\n",
    "    logger.experiment.log_parameter('ols_rmse_norm', ols_rmse_norm)\n",
    "    \n",
    "def log_test_start(logger, roll_type, window):\n",
    "    '''\n",
    "    Given window, find the corresponding ols_rmse from `bench_fr.feather`, \n",
    "    then log to Comet\n",
    "    '''\n",
    "    split_df = pd.read_csv(f'data/split_dates.csv')\n",
    "\n",
    "    _, train_start, train_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "    \n",
    "    logger.experiment.log_parameter('train_start', train_start)\n",
    "    logger.experiment.log_parameter('train_end', train_end)\n",
    "    logger.experiment.log_parameter('test_start', test_start)\n",
    "    logger.experiment.log_parameter('test_end', test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop one\n",
    "def train_one(Model, window_i, model_hparams, train_hparams):\n",
    "    \n",
    "    # set window\n",
    "    model_hparams.update({'window': split_df.iloc[window_i].window})\n",
    "    \n",
    "    # init model\n",
    "    model = Model(model_hparams)\n",
    "\n",
    "    # get model type\n",
    "    model_hparams['model_type'] = model.model_type\n",
    "    model_hparams['target_type'] = model.target_type\n",
    "    model_hparams['feature_type'] = model.feature_type\n",
    "    model_hparams['normalize_target'] = model.normalize_target\n",
    "    model_hparams['attn_type'] = model.attn_type\n",
    "    if hasattr(model, 'emb_share'):\n",
    "        model_hparams['emb_share'] = model.emb_share\n",
    "    \n",
    "    # checkpoint\n",
    "    ckpt_prefix = f\"{train_hparams['note']}_{model_hparams['window']}_\".replace('*',  '')\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        verbose=False,\n",
    "        mode='min',\n",
    "        monitor='val_loss',\n",
    "        filepath=CHECKPOINT_DIR,\n",
    "        prefix=ckpt_prefix,\n",
    "        save_top_k=train_hparams['save_top_k'],\n",
    "        period=train_hparams['checkpoint_period'])\n",
    "\n",
    "    # logger\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=COMET_API_KEY,\n",
    "        save_dir='/data/logs',\n",
    "        project_name='earnings-call',\n",
    "        experiment_name=model_hparams['window'],\n",
    "        workspace='amiao',\n",
    "        display_summary_level=0)\n",
    "\n",
    "    # early stop\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=train_hparams['early_stop_patience'],\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "\n",
    "    # trainer\n",
    "    trainer = pl.Trainer(gpus=model_hparams['gpus'], \n",
    "                         precision=train_hparams['precision'],\n",
    "                         checkpoint_callback=checkpoint_callback, \n",
    "                         early_stop_callback=early_stop_callback,\n",
    "                         overfit_batches=train_hparams['overfit_batches'], \n",
    "                         row_log_interval=train_hparams['row_log_interval'],\n",
    "                         val_check_interval=train_hparams['val_check_interval'], \n",
    "                         progress_bar_refresh_rate=1, \n",
    "                         distributed_backend='dp', \n",
    "                         accumulate_grad_batches=train_hparams['accumulate_grad_batches'],\n",
    "                         min_epochs=train_hparams['min_epochs'],\n",
    "                         max_epochs=train_hparams['max_epochs'], \n",
    "                         max_steps=train_hparams['max_steps'], \n",
    "                         logger=logger)\n",
    "\n",
    "    # delete unused hparam\n",
    "    if model.model_type=='mlp': model_hparams.pop('final_tdim',None)\n",
    "    if model.feature_type=='fin-ratio': \n",
    "        model_hparams.pop('max_seq_len',None)\n",
    "        model_hparams.pop('n_layers_encoder',None)\n",
    "        model_hparams.pop('n_head_encoder',None)\n",
    "        model_hparams.pop('d_model',None)\n",
    "        model_hparams.pop('dff',None)\n",
    "    if model.feature_type=='text': \n",
    "        model_hparams.pop('normalize_layer',None)\n",
    "        model_hparams.pop('normalize_batch',None)\n",
    "    if model.attn_type!='mha': model_hparams.pop('n_head_decoder',None)\n",
    "\n",
    "    # add n_model_params\n",
    "    train_hparams['n_model_params'] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # upload hparams\n",
    "    logger.experiment.log_parameters(model_hparams)\n",
    "    logger.experiment.log_parameters(train_hparams)\n",
    "    \n",
    "    # upload ols_rmse (for reference)\n",
    "    log_ols_rmse(logger, model_hparams['window'])\n",
    "    \n",
    "    # upload test_start\n",
    "    log_test_start(logger, model_hparams['roll_type'], model_hparams['window'])\n",
    "    \n",
    "    # If run on ASU, upload code explicitly\n",
    "    if train_hparams['machine'] == 'ASU':\n",
    "        codefile = [name for name in os.listdir('.') if name.endswith('.py')]\n",
    "        assert len(codefile)==1, f'There must be only one `.py` file in the current directory! {len(codefile)} files detected: {codefile}'\n",
    "        logger.experiment.log_asset(codefile[0])\n",
    "    \n",
    "    \n",
    "    # refresh GPU memory\n",
    "    refresh_cuda_memory()\n",
    "\n",
    "    # fit and test\n",
    "    try:\n",
    "        # train the model\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test on the best model\n",
    "        trainer.test(ckpt_path='best')\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        del model, trainer\n",
    "        refresh_cuda_memory()\n",
    "        logger.finalize('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Txt + Fin-ratio\n",
    "class CCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split_window, split_type, text_in_dataset, roll_type, print_window, preembeddings, targets_df, split_df, gpus, valid_transcriptids=None):\n",
    "        '''\n",
    "        Args:\n",
    "            preembeddings (from globals): list of embeddings. Each element is a tensor (S, E) where S is number of sentences in a call\n",
    "            targets_df (from globals): DataFrame of targets variables.\n",
    "            split_df (from globals):\n",
    "            split_window: str. e.g., \"roll-09\"\n",
    "            split_type: str. 'train' or 'test'\n",
    "            text_only: only output CAR and transcripts if true, otherwise also output financial ratios\n",
    "            transcriptids: list. If provided, only the given transcripts will be used in generating the Dataset. `transcriptids` is applied **on top of** `split_window` and `split_type`\n",
    "            gpus: gpus used, should be a list. ex, [0,1]\n",
    "        '''\n",
    "\n",
    "        # get split dates from `split_df`\n",
    "        _, train_start, train_end, test_start, test_end, _ = tuple(split_df.loc[(split_df.window==split_window) & (split_df.roll_type==roll_type)].iloc[0])\n",
    "        # print current window\n",
    "        if print_window:\n",
    "            print(f'Current window: {split_window} ({roll_type}) \\n(train: {train_start} to {train_end}) (test: {test_start} to {test_end})')\n",
    "        \n",
    "        train_start = datetime.strptime(train_start, '%Y-%m-%d').date()\n",
    "        train_end = datetime.strptime(train_end, '%Y-%m-%d').date()\n",
    "        test_start = datetime.strptime(test_start, '%Y-%m-%d').date()\n",
    "        test_end = datetime.strptime(test_end, '%Y-%m-%d').date()\n",
    "        \n",
    "        # select valid transcriptids (preemb_keys) according to split dates \n",
    "        if split_type=='train':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[:int(len(transcriptids)*0.9)]\n",
    "            # print(f'Dateset -> N train: {len(transcriptids)}')\n",
    "            \n",
    "        if split_type=='val':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(train_start, train_end)].transcriptid.sample(frac=1, random_state=42).tolist()\n",
    "            transcriptids = transcriptids[int(len(transcriptids)*0.9):]\n",
    "            # print(f'Dataset -> N val: {len(transcriptids)}')\n",
    "\n",
    "        elif split_type=='test':\n",
    "            transcriptids = targets_df[targets_df.ciq_call_date.between(test_start, test_end)].transcriptid.tolist()\n",
    "            # print(f'Dataset -> N test: {len(transcriptids)}')\n",
    "\n",
    "\n",
    "        self.valid_preemb_keys = set(transcriptids).intersection(set(preembeddings.keys()))\n",
    "        \n",
    "        if valid_transcriptids is not None:\n",
    "            self.valid_preemb_keys = self.valid_preemb_keys.intersection(set(valid_transcriptids))\n",
    "            \n",
    "        # remove last few samples from `valid_preemb_keys` so that it's divisible by the number of gpus\n",
    "        if split_type in ['train', 'val']:\n",
    "            max_valid_preemb_keys_len = len(self.valid_preemb_keys)//len(gpus)*len(gpus)\n",
    "            self.valid_preemb_keys = list(self.valid_preemb_keys)[:(max_valid_preemb_keys_len)]\n",
    "        \n",
    "        # self attributes\n",
    "        self.text_in_dataset = text_in_dataset\n",
    "        if text_in_dataset:\n",
    "            self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.sent_len = sorted([(k, preembeddings[k].shape[0]) for k in self.valid_preemb_keys], key=itemgetter(1))\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.test_start = test_start\n",
    "        self.test_end = test_end\n",
    "        self.n_samples = len(self.sent_len)\n",
    "        self.split_window = split_window\n",
    "        self.split_type = split_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.valid_preemb_keys))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        transcriptid = self.sent_len[idx][0]\n",
    "        targets = self.targets_df[self.targets_df.transcriptid==transcriptid].iloc[0]\n",
    "        \n",
    "        # all of the following targests are\n",
    "        # of type `numpy.float64`\n",
    "        docid = targets.docid\n",
    "        \n",
    "        sue = targets.sue_norm\n",
    "        sest = targets.sest_norm\n",
    "        car_0_30 = targets.car_0_30\n",
    "        car_0_30_norm = targets.car_0_30_norm\n",
    "        revision = targets.revision\n",
    "        revision_norm = targets.revision_norm\n",
    "        inflow = targets.inflow\n",
    "        inflow_norm = targets.inflow_norm\n",
    "        \n",
    "        similarity = targets.similarity_bigram_norm\n",
    "        sentiment = targets.qa_positive_sent_norm\n",
    "        \n",
    "        \n",
    "        alpha = targets.alpha_norm\n",
    "        volatility = targets.volatility_norm\n",
    "        mcap = targets.mcap_norm\n",
    "        bm = targets.bm_norm\n",
    "        roa = targets.roa_norm\n",
    "        debt_asset = targets.debt_asset_norm\n",
    "        numest = targets.numest_norm\n",
    "        smedest = targets.smedest_norm\n",
    "        sstdest = targets.sstdest_norm\n",
    "        car_m1_m1 = targets.car_m1_m1_norm\n",
    "        car_m2_m2 = targets.car_m2_m2_norm\n",
    "        car_m30_m3 = targets.car_m30_m3_norm\n",
    "        volume = targets.volume_norm\n",
    "        \n",
    "        if self.text_in_dataset:\n",
    "            # inputs: preembeddings\n",
    "            embeddings = self.preembeddings[transcriptid]\n",
    "            \n",
    "            return car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "                   transcriptid, embeddings, \\\n",
    "                   [alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume]\n",
    "        else:\n",
    "            return docid, \\\n",
    "                   torch.tensor(car_0_30,dtype=torch.float32), \\\n",
    "                   torch.tensor(car_0_30_norm,dtype=torch.float32), \\\n",
    "                   torch.tensor([similarity, sentiment],dtype=torch.float32),\\\n",
    "                   torch.tensor([alpha, car_m1_m1, car_m2_m2, car_m30_m3, sest, sue, numest, sstdest, smedest, mcap, roa, bm, debt_asset, volatility, volume], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: position encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # (S, N, E)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Model: Base\n",
    "class CC(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.text_in_dataset will be filled during instanciating.\n",
    "        self.hparams = Namespace(**hparams)\n",
    "        \n",
    "        # check: batch_size//len(gpus)\n",
    "        assert self.hparams.batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`batch_size` must be divisible by `N gpus`. Currently batch_size={self.hparams.batch_size}, N gpus={len(self.hparams.gpus)}'\n",
    "        # check: val_batch_size//len(gpus)\n",
    "        assert self.hparams.val_batch_size%len(self.hparams.gpus)==0, \\\n",
    "            f'`val_batch_size` must be divisible by `N gpus`. Currently batch_size={self.hparams.val_batch_size}, N gpus={len(self.hparams.gpus)}'\n",
    "        \n",
    "        global preembeddings, targets_df, split_df\n",
    "        self.preembeddings = preembeddings\n",
    "        self.targets_df = targets_df\n",
    "        self.split_df = split_df\n",
    "\n",
    "    # forward\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # loss\n",
    "    def mse_loss(self, y, t):\n",
    "        return F.mse_loss(y, t)\n",
    "        \n",
    "    # validation step\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'val_rmse': rmse}\n",
    "        \n",
    "        if 'val_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['val_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_car'] = rmse_car\n",
    "            \n",
    "        if 'val_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['val_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_inflow'] = rmse_inflow\n",
    "\n",
    "        if 'val_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['val_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['val_rmse_revision'] = rmse_revision\n",
    "\n",
    "        return {'val_loss': mse, 'log': log_dict}\n",
    "    \n",
    "    # test step\n",
    "    def test_epoch_end(self, outputs):\n",
    "        mse = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        log_dict = {'test_rmse': rmse}\n",
    "        \n",
    "        if 'test_loss_car' in outputs[0]:\n",
    "            rmse_car = torch.sqrt(torch.stack([x['test_loss_car'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_car'] = rmse_car\n",
    "\n",
    "        if 'test_loss_inflow' in outputs[0]:\n",
    "            rmse_inflow = torch.sqrt(torch.stack([x['test_loss_inflow'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_inflow'] = rmse_inflow\n",
    "            \n",
    "        if 'test_loss_revision' in outputs[0]:\n",
    "            rmse_revision = torch.sqrt(torch.stack([x['test_loss_revision'] for x in outputs]).mean())\n",
    "            log_dict['test_rmse_revision'] = rmse_revision\n",
    "            \n",
    "        return {'test_loss': mse, 'log': log_dict, 'progress_bar':log_dict}\n",
    "    \n",
    "    # Dataset\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.train_dataset = CCDataset(self.hparams.window, split_type='train', text_in_dataset=self.text_in_dataset,\n",
    "                                       roll_type=self.hparams.roll_type, print_window=True, preembeddings=self.preembeddings,\n",
    "                                       targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N train = {len(self.train_dataset)}')\n",
    "        \n",
    "        self.val_dataset = CCDataset(self.hparams.window, split_type='val', text_in_dataset=self.text_in_dataset,\n",
    "                                     roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                     targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N val = {len(self.val_dataset)}')\n",
    "        print(f'N train+val = {len(self.train_dataset)+len(self.val_dataset)}')\n",
    "\n",
    "        self.test_dataset = CCDataset(self.hparams.window, split_type='test', text_in_dataset=self.text_in_dataset, \n",
    "                                      roll_type=self.hparams.roll_type, print_window=False, preembeddings=self.preembeddings,\n",
    "                                      targets_df=self.targets_df, split_df=self.split_df, gpus=self.hparams.gpus)\n",
    "        print(f'N test = {len(self.test_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    def train_dataloader(self):\n",
    "        # Caution:\n",
    "        # - If you enable `BatchNorm`, then must set `drop_last=True`.\n",
    "\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, drop_last=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # Caution: \n",
    "        # - To improve the validation speed, I'll set val_batch_size to 4. \n",
    "        # - Must set `drop_last=True`, otherwise the `val_loss` tensors for different batches won't match and hence give you error.\n",
    "        # - Not to set `val_batch_size` too large (e.g., 16), otherwise you'll lose precious validation data points\n",
    "        \n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.val_batch_size, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = self.collate_fn if self.text_in_dataset else None\n",
    "        return DataLoader(self.test_dataset, num_workers=0, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        '''create mini-batch\n",
    "\n",
    "        Retures:\n",
    "            embeddings: tensor, (N, S, E)\n",
    "            mask: tensor, (N, S)\n",
    "            sue,car,selead,sest: tensor, (N,)\n",
    "        '''\n",
    "        \n",
    "        # embeddings: (N, S, E)\n",
    "        car_0_30, car_0_30_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, \\\n",
    "        fin_ratios = zip(*data)\n",
    "        \n",
    "        # pad sequence\n",
    "        # the number of `padding_value` is irrelevant, since we'll \n",
    "        # apply a mask in the Transformer encoder, which will \n",
    "        # eliminate the padded positions.\n",
    "        valid_seq_len = [emb.shape[-2] for emb in embeddings]\n",
    "        embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0) # (N, T, E)\n",
    "\n",
    "        # mask: (N, T)\n",
    "        mask = torch.ones((embeddings.shape[0], embeddings.shape[1]))\n",
    "        for i, length in enumerate(valid_seq_len):\n",
    "            mask[i, :length] = 0\n",
    "        mask = mask == 1\n",
    "        \n",
    "        return torch.tensor(car_0_30, dtype=torch.float32), torch.tensor(car_0_30_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(inflow, dtype=torch.float32), torch.tensor(inflow_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(revision, dtype=torch.float32), torch.tensor(revision_norm, dtype=torch.float32), \\\n",
    "               torch.tensor(transcriptid, dtype=torch.float32), embeddings.float(), mask, \\\n",
    "               torch.tensor(fin_ratios, dtype=torch.float32)\n",
    "        \n",
    "    # optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car ~ txt + fr\n",
    "class CCTransformerSTLTxtFr(CC):\n",
    "    def __init__(self, hparams):\n",
    "        # `self.hparams` will be created by super().__init__\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "        # check: hparams.car_weight == 1\n",
    "        assert self.hparams.car_weight == 1, f'car_weight must be 1. Currently car_weight={self.hparams.car_weight}'\n",
    "        \n",
    "        # specify model type\n",
    "        self.model_type = 'TSFM'\n",
    "        self.target_type = 'car'\n",
    "        self.feature_type = 'txt+fr'\n",
    "        self.normalize_target = True\n",
    "        self.attn_type = 'dotprod'\n",
    "        self.text_in_dataset = True if self.feature_type!='fr' else False \n",
    "\n",
    "        self.n_covariate = 15\n",
    "        \n",
    "        # positional encoding\n",
    "        self.encoder_pos = PositionalEncoding(self.hparams.d_model, self.hparams.attn_dropout)\n",
    "        \n",
    "        # encoder layers for input, expert, nonexpert\n",
    "        encoder_layers_expert = nn.TransformerEncoderLayer(self.hparams.d_model, self.hparams.n_head_encoder, self.hparams.dff, self.hparams.attn_dropout)\n",
    "        \n",
    "        # atten layers for SUE, CAR, SELEAD, SEST\n",
    "        self.attn_layers_car = nn.Linear(self.hparams.d_model, 1)\n",
    "        self.attn_dropout_1 = nn.Dropout(self.hparams.attn_dropout)\n",
    "        \n",
    "        # Build Encoder and Decoder\n",
    "        self.encoder_expert = nn.TransformerEncoder(encoder_layers_expert, self.hparams.n_layers_encoder)\n",
    "        \n",
    "        # linear layer to produce final result\n",
    "        self.txt_fc_1 = nn.Linear(self.hparams.d_model, self.hparams.final_tdim)\n",
    "        self.fc_1 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        # self.fc_2 = nn.Linear(self.hparams.final_tdim+self.n_covariate, 1)\n",
    "        # self.fc_3 = nn.Linear(self.hparams.d_model+self.n_covariate, 1)\n",
    "        \n",
    "        # dropout for final fc layers\n",
    "        # self.txt_dropout_1 = nn.Dropout()\n",
    "        # self.fc_dropout_1 = nn.Dropout(self.hparams.dropout)\n",
    "        # self.fc_dropout_2 = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        # batch normalizer\n",
    "        # self.batch_norm = nn.BatchNorm1d(self.n_covariate)\n",
    "        \n",
    "    # forward\n",
    "    def forward(self, embeddings, src_key_padding_mask, fin_ratios):\n",
    "        \n",
    "        # if S is longer than max_seq_len, cut\n",
    "        embeddings = embeddings[:,:self.hparams.max_seq_len,] # (N, S, E)\n",
    "        src_key_padding_mask = src_key_padding_mask[:,:self.hparams.max_seq_len] # (N, S)\n",
    "        \n",
    "        embeddings = embeddings.transpose(0, 1) # (S, N, E)\n",
    "        \n",
    "        # positional encoding\n",
    "        x = self.encoder_pos(embeddings) # (S, N, E)\n",
    "        \n",
    "        # encode\n",
    "        x_expert = self.encoder_expert(x, src_key_padding_mask=src_key_padding_mask).transpose(0,1) # (N, S, E)\n",
    "        \n",
    "        # decode with attn\n",
    "        x_attn = self.attn_dropout_1(F.softmax(self.attn_layers_car(x_expert), dim=1)) # (N, S, 1)\n",
    "        x_expert = torch.bmm(x_expert.transpose(-1,-2), x_attn).squeeze(-1) # (N, E)\n",
    "        \n",
    "        # project text embedding to a lower dimension\n",
    "        x_expert = self.txt_fc_1(x_expert)\n",
    "        # x_expert = self.txt_dropout_1(x_expert)\n",
    "        \n",
    "        # concate `x_final` with `fin_ratios`\n",
    "        x_final = torch.cat([x_expert, fin_ratios], dim=-1) # (N, E+X) where X is the number of covariate (n_covariate)\n",
    "        \n",
    "        # final FC\n",
    "        # x_final = self.fc_dropout_1(F.relu(self.fc_1(x_expert))) # (N, E+X)\n",
    "        y_car = self.fc_1(x_final) # (N, 1)\n",
    "        \n",
    "        # final output\n",
    "        return y_car\n",
    "    \n",
    "    # traning step\n",
    "    def training_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "        \n",
    "        # logging\n",
    "        return {'loss': loss_car, 'log': {'train_loss': loss_car}}\n",
    "        \n",
    "    # validation step\n",
    "    def validation_step(self, batch, idx):\n",
    "\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'val_loss': loss_car}\n",
    "\n",
    "    # test step\n",
    "    def test_step(self, batch, idx):\n",
    "        car, car_norm, inflow, inflow_norm, revision, revision_norm, \\\n",
    "        transcriptid, embeddings, mask, \\\n",
    "        fin_ratios = batch\n",
    "        \n",
    "        # forward\n",
    "        y_car = self.forward(embeddings, mask, fin_ratios) # (N, 1)\n",
    "\n",
    "        # compute loss\n",
    "        loss_car = self.mse_loss(y_car, car_norm.unsqueeze(-1)).unsqueeze(-1) # (1,)\n",
    "\n",
    "        # logging\n",
    "        return {'test_loss': loss_car}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2117177f3b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET INFO: old comet version (3.1.12) detected. current: 3.1.13 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/amiao/earnings-call/26a73e87a0524cf48009382e4fd09e3f\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | encoder_pos     | PositionalEncoding | 0     \n",
      "1 | attn_layers_car | Linear             | 1 K   \n",
      "2 | attn_dropout_1  | Dropout            | 0     \n",
      "3 | encoder_expert  | TransformerEncoder | 33 M  \n",
      "4 | txt_fc_1        | Linear             | 8 K   \n",
      "5 | fc_1            | Linear             | 24    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current window: roll-01 (6y) \n",
      "(train: 2008-01-01 to 2013-12-31) (test: 2014-01-01 to 2014-03-31)\n",
      "N train = 10608\n",
      "N val = 1186\n",
      "N train+val = 11794\n",
      "N test = 466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42737730ab2466e9ce9fd935631673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[1;31m# -----------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;31m# ------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    647\u001b[0m                 \u001b[1;31m# track total loss for logging (avoid mem leaks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_closure_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8bc326ac449d>\u001b[0m in \u001b[0;36mtrain_one\u001b[1;34m(Model, window_i, model_hparams, train_hparams)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_dp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdp_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\distrib_parts.py\u001b[0m in \u001b[0;36mdp_train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[1;31m# CORE TRAINING LOOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_teardown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"success\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pytorch_lightning\\loggers\\comet.py\u001b[0m in \u001b[0;36mfinalize\u001b[1;34m(self, status)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\comet_ml\\experiment.py\u001b[0m in \u001b[0;36mend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_notebook_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\comet_ml\\__init__.py\u001b[0m in \u001b[0;36m_on_end\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \"\"\"\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0msuccessful_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\comet_ml\\experiment.py\u001b[0m in \u001b[0;36m_on_end\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    561\u001b[0m                 )\n\u001b[1;32m--> 562\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1059\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-571bc477f257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# train one window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mtrain_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_hparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_hparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-8bc326ac449d>\u001b[0m in \u001b[0;36mtrain_one\u001b[1;34m(Model, window_i, model_hparams, train_hparams)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mrefresh_cuda_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-54e944eef35b>\u001b[0m in \u001b[0;36mrefresh_cuda_memory\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Then move all tensors to the CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# choose Model\n",
    "Model = CCTransformerSTLTxtFr\n",
    "\n",
    "# hparams\n",
    "model_hparams = {\n",
    "    'seed': 42, # key!\n",
    "    'preembedding_type': 'all_sbert_roberta_nlistsb_encoded', # key!\n",
    "    'targets_name': 'f_sue_keydevid_car_finratio_vol_transcriptid_sim_inflow_revision_sentiment_text_norm', # key!\n",
    "    'roll_type': '6y',  # key!\n",
    "    'gpus': [0,1,2],\n",
    "    \n",
    "    # task weight\n",
    "    'car_weight': 1,      # Key!\n",
    "    'inflow_weight': 0,   # key!\n",
    "    'revision_weight': 0, # key!\n",
    "    \n",
    "    'batch_size': 20,     # key!\n",
    "    'val_batch_size': 20,\n",
    "    'max_seq_len': 768, \n",
    "    'learning_rate':3e-4, # key!\n",
    "    'task_weight': 1,\n",
    "    'n_layers_encoder': 4,\n",
    "    'n_head_encoder': 8, \n",
    "    'd_model': 1024,\n",
    "    'final_tdim': 8, # key!\n",
    "    'dff': 2048,\n",
    "    'attn_dropout': 0.1,\n",
    "    'dropout': 0.5,\n",
    "    'n_head_decoder': 8} \n",
    "\n",
    "train_hparams = {\n",
    "    # log\n",
    "    'machine': 'ASU',  # key!\n",
    "    'note': f\"STL-24,(car~fr+mtxt {model_hparams['roll_type']}),txtfc=1({model_hparams['final_tdim']}),fc=0,txtdropout=no,fc_dropout=no,NormCAR=yes,bsz={model_hparams['batch_size']},seed={model_hparams['seed']},log(mcap)=yes,lr={model_hparams['learning_rate']:.2g}\", # key!\n",
    "    'row_log_interval': 10,\n",
    "    'save_top_k': 1,\n",
    "    'val_check_interval': 0.2,\n",
    "\n",
    "    # data size\n",
    "    'precision': 32,\n",
    "    'overfit_batches': 0.0,\n",
    "    'min_epochs': 3,\n",
    "    'max_epochs': 20,\n",
    "    'max_steps': None,\n",
    "    'accumulate_grad_batches': 1,\n",
    "\n",
    "    # Caution:\n",
    "    # The check of patience depends on **how often you compute your val_loss** (`val_check_interval`). \n",
    "    # Say you check val every N baches, then `early_stop_callback` will compare to your latest N **baches**.\n",
    "    # If you compute val_loss every N **epoches**, then `early_stop_callback` will compare to the latest N **epochs**.\n",
    "    'early_stop_patience': 8,\n",
    "\n",
    "    # Caution:\n",
    "    # If set to 1, then save ckpt every 1 epoch\n",
    "    # If set to 0, then save ckpt on every val!!! (if val improves)\n",
    "    'checkpoint_period': 0}\n",
    "\n",
    "# delete all existing .ckpt files\n",
    "refresh_ckpt()\n",
    "\n",
    "# load split_df\n",
    "load_split_df(model_hparams['roll_type'])\n",
    "    \n",
    "# load targets_df\n",
    "load_targets(model_hparams['targets_name'])\n",
    "\n",
    "# load preembeddings\n",
    "load_preembeddings(model_hparams['preembedding_type'])\n",
    "    \n",
    "# loop over 24!\n",
    "np.random.seed(model_hparams['seed'])\n",
    "torch.manual_seed(model_hparams['seed'])\n",
    "\n",
    "for window_i in range(len(split_df)):\n",
    "\n",
    "    # train one window\n",
    "    train_one(Model, window_i, model_hparams, train_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
