{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "transient": {
       "display_id": "datatable:css"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datatable as dt\n",
    "import spacy\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from datatable import f\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Doc to \"text tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"text_component_sp500.feather\" (978.0 MB) loaded as \"text_component\" (2s)\n"
     ]
    }
   ],
   "source": [
    "# Select componentid that belongs to MD and QA\n",
    "ld('text_component_sp500', ldname='text_component')\n",
    "text_component = dt.Frame(text_component)\n",
    "\n",
    "componentids_md = text_component[(f.transcriptcomponenttypeid==2) & (f.speakertypeid==2), f.transcriptcomponentid].to_list()[0]\n",
    "\n",
    "componentids_qa = text_component[((f.transcriptcomponenttypeid==3) | (f.transcriptcomponenttypeid==4)) & ((f.speakertypeid==2)|(f.speakertypeid==3)), f.transcriptcomponentid].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DocBin from disk\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "docs = list(DocBin(store_user_data=True).from_disk('data/doc_sp500.spacy').get_docs(nlp.vocab))\n",
    "\n",
    "# Register extension for Span\n",
    "Span.set_extension('transcriptid', default=None, force=True)\n",
    "Span.set_extension('componentid', default=None, force=True)\n",
    "Span.set_extension('componentorder', default=None, force=True)\n",
    "Span.set_extension('componenttypeid', default=None, force=True)\n",
    "Span.set_extension('speakerid', default=None, force=True)\n",
    "Span.set_extension('speakertypeid', default=None, force=True)\n",
    "Span.set_extension('is_component', default=False, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Doc to list of str\n",
    "\n",
    "# Filtering Rule:\n",
    "# - only keep lemma\n",
    "# - no stop words\n",
    "# - no punctuation\n",
    "# - no \"like number\" \n",
    "texttoken_md = []\n",
    "for doc in docs:\n",
    "    txttok = []\n",
    "    for span in doc.spans['components']:\n",
    "        if span._.componentid in componentids_md:\n",
    "            txttok.extend([t.lemma_ for t in span \n",
    "                           if ((not t.is_punct) & (not t.is_stop) & (not t.like_num))])\n",
    "    texttoken_md.append(txttok)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 36,   0,   0, ...,   0,   0,   1],\n       [ 48,   0,   0, ...,   0,   0,   0],\n       [ 64,   0,   0, ...,   0,   0,   0],\n       ...,\n       [ 67,   0,   0, ...,   0,   0,   0],\n       [467,   0,   2, ...,   0,   1,   0],\n       [ 95,   0,   0, ...,   1,   0,   0]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=lambda x: x,\n",
    "                             tokenizer=lambda x: x)\n",
    "x = vectorizer.fit_transform(texttoken_md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}