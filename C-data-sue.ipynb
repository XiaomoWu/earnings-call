{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/yu/OneDrive/CC\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "WORK_DIR = \"/home/yu/OneDrive/CC\"\n",
    "DATA_DIR = f\"{WORK_DIR}/data\"\n",
    "WRDS_DOWNLOAD_DIR = f'{DATA_DIR}/WRDS-download'\n",
    "\n",
    "os.chdir(WORK_DIR)\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wrds\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "###################\n",
    "# Connect to WRDS #\n",
    "###################\n",
    "conn=wrds.Connection(wrds_username='xiaomowu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook stores all supporting Python code. Mainly `ICLINK` and `SUE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# `ICLINK` (IBES+CRSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ICLINK` links `IBES` analyst estimates to `CRSP`\n",
    "\n",
    "> ICLINK: Link CRSP and IBES        \n",
    "> June 2019                         \n",
    "> Qingyi (Freda) Song Drechsler\n",
    "\n",
    "\n",
    "This program replicates the SAS macro ICLINK to create a linking table between CRSP and IBES Output is a score reflecting the quality of the link\n",
    "Score = 0 (best link) to Score = 6 (worst link)\n",
    "\n",
    "More explanation on score system:\n",
    "- 0: BEST match: using (cusip, cusip dates and company names)       \n",
    "         or (exchange ticker, company names and 6-digit cusip)     \n",
    "- 1: Cusips and cusip dates match but company names do not match    \n",
    "- 2: Cusips and company names match but cusip dates do not match    \n",
    "- 3: Cusips match but cusip dates and company names do not match    \n",
    "- 4: tickers and 6-digit cusips match but company names do not match   \n",
    "- 5: tickers and company names match but 6-digit cusips do not match        \n",
    "- 6: tickers match but company names and 6-digit cusips do not match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link by CUSIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Step 1: Link by CUSIP #\n",
    "#########################\n",
    "\n",
    "# 1.1 IBES: Get the list of IBES Tickers for US firms in IBES\n",
    "_ibes1 = conn.raw_sql(\"\"\"\n",
    "                      select ticker, cusip, cname, sdates from ibes.id\n",
    "                      where usfirm=1 and cusip != ''\n",
    "                      \"\"\")\n",
    "\n",
    "# Create first and last 'start dates' for a given cusip\n",
    "# Use agg min and max to find the first and last date per group\n",
    "# then rename to fdate and ldate respectively\n",
    "\n",
    "_ibes1_date = _ibes1.groupby(['ticker','cusip']).sdates.agg(['min', 'max'])\\\n",
    ".reset_index().rename(columns={'min':'fdate', 'max':'ldate'})\n",
    "\n",
    "# merge fdate ldate back to _ibes1 data\n",
    "_ibes2 = pd.merge(_ibes1, _ibes1_date,how='left', on =['ticker','cusip'])\n",
    "_ibes2 = _ibes2.sort_values(by=['ticker','cusip','sdates'])\n",
    "\n",
    "# keep only the most recent company name\n",
    "# determined by having sdates = ldate\n",
    "_ibes2 = _ibes2.loc[_ibes2.sdates == _ibes2.ldate].drop(['sdates'], axis=1)\n",
    "\n",
    "# 1.2 CRSP: Get all permno-ncusip combinations\n",
    "_crsp1 = conn.raw_sql(\"\"\"\n",
    "                      select permno, ncusip, comnam, namedt, nameenddt\n",
    "                      from crsp.stocknames\n",
    "                      where ncusip != ''\n",
    "                      \"\"\")\n",
    "\n",
    "# first namedt\n",
    "_crsp1_fnamedt = _crsp1.groupby(['permno','ncusip']).namedt.min().reset_index()\n",
    "\n",
    "# last nameenddt\n",
    "_crsp1_lnameenddt = _crsp1.groupby(['permno','ncusip']).nameenddt.max().reset_index()\n",
    "\n",
    "# merge both \n",
    "_crsp1_dtrange = pd.merge(_crsp1_fnamedt, _crsp1_lnameenddt, \\\n",
    "                          on = ['permno','ncusip'], how='inner')\n",
    "\n",
    "# replace namedt and nameenddt with the version from the dtrange\n",
    "_crsp1 = _crsp1.drop(['namedt'],axis=1).rename(columns={'nameenddt':'enddt'})\n",
    "_crsp2 = pd.merge(_crsp1, _crsp1_dtrange, on =['permno','ncusip'], how='inner')\n",
    "\n",
    "# keep only most recent company name\n",
    "_crsp2 = _crsp2.loc[_crsp2.enddt ==_crsp2.nameenddt].drop(['enddt'], axis=1)\n",
    "\n",
    "# 1.3 Create CUSIP Link Table\n",
    "\n",
    "# Link by full cusip, company names and dates\n",
    "_link1_1 = pd.merge(_ibes2, _crsp2, how='inner', left_on='cusip', right_on='ncusip')\\\n",
    ".sort_values(['ticker','permno','ldate'])\n",
    "\n",
    "# Keep link with most recent company name\n",
    "_link1_1_tmp = _link1_1.groupby(['ticker','permno']).ldate.max().reset_index()\n",
    "_link1_2 = pd.merge(_link1_1, _link1_1_tmp, how='inner', on =['ticker', 'permno', 'ldate'])\n",
    "\n",
    "\n",
    "# Calculate name matching ratio using FuzzyWuzzy\n",
    "\n",
    "# Note: fuzz ratio = 100 -> match perfectly\n",
    "#       fuzz ratio = 0   -> do not match at all\n",
    "\n",
    "# Comment: token_set_ratio is more flexible in matching the strings:\n",
    "# fuzz.token_set_ratio('AMAZON.COM INC',  'AMAZON COM INC')\n",
    "# returns value of 100\n",
    "\n",
    "# fuzz.ratio('AMAZON.COM INC',  'AMAZON COM INC')\n",
    "# returns value of 93\n",
    "\n",
    "_link1_2['name_ratio'] = _link1_2.apply(lambda x: fuzz.token_set_ratio(x.comnam, x.cname), axis=1)\n",
    "\n",
    "# Note on parameters:\n",
    "# The following parameters are chosen to mimic the SAS macro %iclink\n",
    "# In %iclink, name_dist < 30 is assigned score = 0\n",
    "# where name_dist=30 is roughly 90% percentile in total distribution\n",
    "# and higher name_dist means more different names.\n",
    "# In name_ratio, I mimic this by choosing 10% percentile as cutoff to assign\n",
    "# score = 0\n",
    "\n",
    "# 10% percentile of the company name distance\n",
    "name_ratio_p10 = _link1_2.name_ratio.quantile(0.10)\n",
    "\n",
    "# Function to assign score for companies matched by:\n",
    "# full cusip and passing name_ratio\n",
    "# or meeting date range requirement\n",
    "\n",
    "def score1(row):\n",
    "    if (row['fdate']<=row['nameenddt']) & (row['ldate']>=row['namedt']) & (row['name_ratio'] >= name_ratio_p10):\n",
    "        score = 0\n",
    "    elif (row['fdate']<=row['nameenddt']) & (row['ldate']>=row['namedt']):\n",
    "        score = 1\n",
    "    elif row['name_ratio'] >= name_ratio_p10:\n",
    "        score = 2\n",
    "    else:\n",
    "        score = 3\n",
    "    return score\n",
    "\n",
    "# assign size portfolio\n",
    "_link1_2['score']=_link1_2.apply(score1, axis=1)\n",
    "_link1_2 = _link1_2[['ticker','permno','cname','comnam','name_ratio','score']]\n",
    "_link1_2 = _link1_2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link by Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Step 2: Link by TICKER #\n",
    "##########################\n",
    "\n",
    "# Find links for the remaining unmatched cases using Exchange Ticker \n",
    "\n",
    "# Identify remaining unmatched cases \n",
    "_nomatch1 = pd.merge(_ibes2[['ticker']], _link1_2[['permno','ticker']], on='ticker', how='left')\n",
    "_nomatch1 = _nomatch1.loc[_nomatch1.permno.isnull()].drop(['permno'], axis=1).drop_duplicates()\n",
    "\n",
    "# Add IBES identifying information\n",
    "\n",
    "ibesid = conn.raw_sql(\"\"\" select ticker, cname, oftic, sdates, cusip from ibes.id \"\"\")\n",
    "ibesid = ibesid.loc[ibesid.oftic.notna()]\n",
    "\n",
    "_nomatch2 = pd.merge(_nomatch1, ibesid, how='inner', on=['ticker'])\n",
    "\n",
    "# Create first and last 'start dates' for Exchange Tickers\n",
    "# Label date range variables and keep only most recent company name\n",
    "\n",
    "_nomatch3 = _nomatch2.groupby(['ticker', 'oftic']).sdates.agg(['min', 'max'])\\\n",
    ".reset_index().rename(columns={'min':'fdate', 'max':'ldate'})\n",
    "\n",
    "_nomatch3 = pd.merge(_nomatch2, _nomatch3, how='left', on=['ticker','oftic'])\n",
    "\n",
    "_nomatch3 = _nomatch3.loc[_nomatch3.sdates == _nomatch3.ldate]\n",
    "\n",
    "# Get entire list of CRSP stocks with Exchange Ticker information\n",
    "\n",
    "_crsp_n1 = conn.raw_sql(\"\"\" select ticker, comnam, permno, ncusip, namedt, nameenddt\n",
    "                            from crsp.stocknames \"\"\")\n",
    "\n",
    "_crsp_n1 = _crsp_n1.loc[_crsp_n1.ticker.notna()].sort_values(by=['permno','ticker','namedt'])\n",
    "\n",
    "# Arrange effective dates for link by Exchange Ticker\n",
    "\n",
    "_crsp_n1_namedt = _crsp_n1.groupby(['permno','ticker']).namedt.min().reset_index().rename(columns={'min':'namedt'})\n",
    "_crsp_n1_nameenddt = _crsp_n1.groupby(['permno','ticker']).nameenddt.max().reset_index().rename(columns={'max':'nameenddt'})\n",
    "\n",
    "_crsp_n1_dt = pd.merge(_crsp_n1_namedt, _crsp_n1_nameenddt, how = 'inner', on=['permno','ticker'])\n",
    "\n",
    "_crsp_n1 = _crsp_n1.rename(columns={'namedt': 'namedt_ind', 'nameenddt':'nameenddt_ind'})\n",
    "\n",
    "_crsp_n2 = pd.merge(_crsp_n1, _crsp_n1_dt, how ='left', on = ['permno','ticker'])\n",
    "\n",
    "_crsp_n2 = _crsp_n2.rename(columns={'ticker':'crsp_ticker'})\n",
    "_crsp_n2 = _crsp_n2.loc[_crsp_n2.nameenddt_ind == _crsp_n2.nameenddt].drop(['namedt_ind', 'nameenddt_ind'], axis=1)\n",
    "\n",
    "# Merge remaining unmatched cases using Exchange Ticker \n",
    "# Note: Use ticker date ranges as exchange tickers are reused overtime\n",
    "\n",
    "_link2_1 = pd.merge(_nomatch3, _crsp_n2, how='inner', left_on=['oftic'], right_on=['crsp_ticker'])\n",
    "_link2_1 = _link2_1.loc[(_link2_1.ldate>=_link2_1.namedt) & (_link2_1.fdate<=_link2_1.nameenddt)]\n",
    "\n",
    "\n",
    "# Score using company name using 6-digit CUSIP and company name spelling distance\n",
    "_link2_1['name_ratio'] = _link2_1.apply(lambda x: fuzz.token_set_ratio(x.comnam, x.cname), axis=1)\n",
    "\n",
    "_link2_2 = _link2_1\n",
    "_link2_2['cusip6'] = _link2_2.apply(lambda x: x.cusip[:6], axis=1)\n",
    "_link2_2['ncusip6'] = _link2_2.apply(lambda x: x.ncusip[:6], axis=1)\n",
    "\n",
    "# Score using company name using 6-digit CUSIP and company name spelling distance\n",
    "\n",
    "def score2(row):\n",
    "    if (row['cusip6']==row['ncusip6']) & (row['name_ratio'] >= name_ratio_p10):\n",
    "        score = 0\n",
    "    elif (row['cusip6']==row['ncusip6']):\n",
    "        score = 4\n",
    "    elif row['name_ratio'] >= name_ratio_p10:\n",
    "        score = 5\n",
    "    else:\n",
    "        score = 6\n",
    "    return score\n",
    "\n",
    "# assign size portfolio\n",
    "_link2_2['score']=_link2_2.apply(score2, axis=1)\n",
    "\n",
    "# Some companies may have more than one TICKER-PERMNO link\n",
    "# so re-sort and keep the case (PERMNO & Company name from CRSP)\n",
    "# that gives the lowest score for each IBES TICKER \n",
    "\n",
    "_link2_2 = _link2_2[['ticker','permno','cname','comnam', 'name_ratio', 'score']].sort_values(by=['ticker','score'])\n",
    "_link2_2_score = _link2_2.groupby(['ticker']).score.min().reset_index()\n",
    "\n",
    "_link2_3 = pd.merge(_link2_2, _link2_2_score, how='inner', on=['ticker', 'score'])\n",
    "_link2_3 = _link2_3[['ticker','permno','cname','comnam','score']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detect the data is type of \"pandas.core.frame.DataFrame\", but you are trying to save as \"pkl\". Be careful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"iclink\" saved as \"iclink.pkl\" (1.5 MB) (0s)\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Step 3: Finalize LInks and Scores #\n",
    "#####################################\n",
    "\n",
    "iclink = _link1_2.append(_link2_3, sort=True)\n",
    "\n",
    "# Storing iclink for other program usage\n",
    "sv('iclink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# SUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update: 2021-01-18\n",
    "- Update: 2020-04-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n",
      "\"iclink.feather\" (865.2 KB) loaded (<1s)\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Post Earnings Announcement Drift  #\n",
    "# June 2019                         #\n",
    "# Qingyi (Freda) Song Drechsler     #\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###################\n",
    "# Connect to WRDS #\n",
    "###################\n",
    "conn=wrds.Connection(wrds_username='xiaomowu')\n",
    "\n",
    "# set sample date range\n",
    "begdate = '01/01/2005'\n",
    "enddate = '01/18/2021'\n",
    "\n",
    "# set CRSP date range a bit wider to guarantee collecting all information\n",
    "crsp_begdate = '01/01/2004'\n",
    "crsp_enddate = '01/18/2021'\n",
    "\n",
    "#################################\n",
    "# Step 0: Read in ICLINK output #\n",
    "#################################\n",
    "\n",
    "# iclink.pt is the output from the python program iclink\n",
    "# it contains the linking between crsp and ibes\n",
    "ld('iclink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"_sp500\" saved as \"_sp500.feather\" (882.0 B) (<1s)\n"
     ]
    }
   ],
   "source": [
    "_sp500 = conn.raw_sql(\"\"\" select gvkey from compa.idxcst_his where gvkeyx='000003' \"\"\")\n",
    "sv('_sp500', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "_ccm = conn.raw_sql(\"\"\" select gvkey, lpermco as permco, lpermno as permno, linkdt, linkenddt \n",
    "                        from crsp.ccmxpf_linktable \n",
    "                        where usedflag=1 and linkprim in ('P', 'C')\"\"\")\n",
    "sv('_ccm', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "_sec = conn.raw_sql(\"\"\" select ibtic, gvkey from comp.security \"\"\")\n",
    "sv('_sec', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "ibes_temp = conn.raw_sql(f\"\"\"\n",
    "                        select ticker, estimator, analys, pdf, fpi, value, fpedats, revdats, revtims, anndats, anntims\n",
    "                        from ibes.detu_epsus \n",
    "                        where fpedats between '{begdate}' and '{enddate}'\n",
    "                        and (fpi='6' or fpi='7')\n",
    "                        \"\"\", date_cols = ['revdats', 'anndats', 'fpedats'])\n",
    "sv('ibes_temp', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "ibes_act = conn.raw_sql(f\"\"\"\n",
    "                        select ticker, anndats as repdats, value as act, pends as fpedats, pdicity\n",
    "                        from ibes.actu_epsus \n",
    "                        where pends between '{begdate}' and '{enddate}'\n",
    "                        and pdicity='QTR'\n",
    "                        \"\"\", date_cols = ['repdats', 'fpedats'])\n",
    "sv('ibes_act', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "crsp_dats = conn.raw_sql(\"\"\" \n",
    "                            select date \n",
    "                            from crsp.dsi \n",
    "                         \"\"\", date_cols=['date'])\n",
    "sv('crsp_dats', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "cfacshr = conn.raw_sql(f\"\"\"\n",
    "                        select permno, date, cfacshr\n",
    "                        from crsp.dsf\n",
    "                        where date between '{crsp_begdate}' and '{crsp_enddate}'\n",
    "                        \"\"\", date_cols = ['date'])\n",
    "sv('cfacshr', path=WRDS_DOWNLOAD_DIR)\n",
    "fundq = conn.raw_sql(f\"\"\"\n",
    "                        select gvkey, fyearq, fqtr, conm, datadate, rdq, epsfxq, epspxq, prccq, \n",
    "                        ajexq, spiq, cshoq, cshprq, cshfdq, saleq, atq, fyr, datafqtr, cshoq*prccq as mcap  \n",
    "                        from comp.fundq \n",
    "                        where consol='C' and popsrc='D' and indfmt='INDL' and datafmt='STD'\n",
    "                        and datadate between '{crsp_begdate}' and '{crsp_enddate}' \n",
    "                        \"\"\", date_cols = ['datadate', 'datafqtr', 'rdq'])\n",
    "\n",
    "sv('fundq', path=WRDS_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP500 universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"_sp500.feather\" (15.7 KB) loaded (<1s)\n",
      "\"_ccm.feather\" (680.9 KB) loaded (<1s)\n",
      "\"_sec.feather\" (689.4 KB) loaded (<1s)\n",
      "\"iclink.feather\" (865.2 KB) loaded (<1s)\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Step 1. S&P 500 Index Universe #\n",
    "##################################\n",
    "\n",
    "# All companies that were ever included in S&P 500 index as an example \n",
    "# Linking Compustat GVKEY and IBES Tickers using ICLINK               \n",
    "# For unmatched GVKEYs, use header IBTIC link in Compustat Security file \n",
    "\n",
    "ld('_sp500', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "ld('_ccm', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "ld('_sec', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "ld('iclink', force=True)\n",
    "\n",
    "_ccm[['permco', 'permno']] = _ccm[['permco', 'permno']].astype(int)\n",
    "_ccm['linkdt'] = pd.to_datetime(_ccm['linkdt'])\n",
    "_ccm['linkenddt'] = pd.to_datetime(_ccm['linkenddt'])\n",
    "\n",
    "import datetime\n",
    "today = datetime.datetime.today()\n",
    "\n",
    "# Fill linkenddt missing value (.E in SAS dataset) with today's date\n",
    "_ccm['linkenddt'] = _ccm.linkenddt.fillna(today)\n",
    "\n",
    "# Start the sequence of left join\n",
    "gvkey = pd.merge(_sp500, _ccm, how='left', on=['gvkey'])\n",
    "gvkey = pd.merge(gvkey, _sec.loc[_sec.ibtic.notna()], how='left', on=['gvkey'])\n",
    "\n",
    "# high quality links from iclink\n",
    "# score = 0 or 1\n",
    "iclink_hq = iclink.loc[(iclink.score <=1)]\n",
    "\n",
    "gvkey = pd.merge(gvkey, iclink_hq, how='left', on=['permno'])\n",
    "\n",
    "# fill missing ticker with ibtic\n",
    "gvkey.ticker = np.where(gvkey.ticker.notnull(), gvkey.ticker, gvkey.ibtic)\n",
    "\n",
    "# Keep relevant columns and drop duplicates if there is any\n",
    "gvkey = gvkey[['gvkey', 'permco', 'permno', 'linkdt', 'linkenddt','ticker']]\n",
    "\n",
    "gvkey = gvkey.drop_duplicates()\n",
    "\n",
    "# date ranges from gvkey\n",
    "\n",
    "# min linkdt for ticker and permno combination\n",
    "gvkey_mindt = gvkey.groupby(['ticker','permno']).linkdt.min().reset_index()\n",
    "\n",
    "# max linkenddt for ticker and permno combination\n",
    "gvkey_maxdt = gvkey.groupby(['ticker','permno']).linkenddt.max().reset_index()\n",
    "\n",
    "# link date range \n",
    "gvkey_dt = pd.merge(gvkey_mindt, gvkey_maxdt, how='inner', on=['ticker','permno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract estimates from IBES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ibes_temp.feather\" (145.6 MB) loaded (<1s)\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Step 2. Extract Estimates from IBES #\n",
    "#######################################\n",
    "\n",
    "# Extract estimates from IBES Unadjusted file and select    \n",
    "# the latest estimate for a firm within broker-analyst group\n",
    "# \"fpi in (6,7)\" selects quarterly forecast for the current \n",
    "# and the next fiscal quarter    \n",
    "\n",
    "ld('ibes_temp', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "# merge to get date range linkdt and linkenddt to fulfill date requirement\n",
    "ibes_temp = pd.merge(ibes_temp, gvkey_dt, how='left', on=['ticker'])\n",
    "ibes_temp = ibes_temp.loc[(ibes_temp.linkdt<=ibes_temp.anndats) & (ibes_temp.anndats <= ibes_temp.linkenddt)]\n",
    "\n",
    "# Count number of estimates reported on primary/diluted basis \n",
    "p_sub = ibes_temp[['ticker','fpedats','pdf']].loc[ibes_temp.pdf=='P']\n",
    "d_sub = ibes_temp[['ticker','fpedats','pdf']].loc[ibes_temp.pdf=='D']\n",
    "\n",
    "p_count = p_sub.groupby(['ticker','fpedats']).pdf.count().reset_index().rename(columns={'pdf':'p_count'})\n",
    "d_count = d_sub.groupby(['ticker','fpedats']).pdf.count().reset_index().rename(columns={'pdf':'d_count'})\n",
    "\n",
    "ibes = pd.merge(ibes_temp, d_count, how = 'left', on=['ticker', 'fpedats'])\n",
    "ibes = pd.merge(ibes, p_count, how='left', on =['ticker','fpedats'])\n",
    "ibes['d_count'] = ibes.d_count.fillna(0)\n",
    "ibes['p_count'] = ibes.p_count.fillna(0)\n",
    "\n",
    "# Determine whether most analysts report estimates on primary/diluted basis\n",
    "# following Livnat and Mendenhall (2006)                                   \n",
    "\n",
    "ibes['basis']=np.where(ibes.p_count>ibes.d_count, 'P', 'D')\n",
    "\n",
    "ibes = ibes.sort_values(by=['ticker','fpedats','estimator','analys','anndats', 'anntims', 'revdats', 'revtims'])\\\n",
    ".drop(['linkdt', 'linkenddt','p_count','d_count', 'pdf', 'fpi'], axis=1)\n",
    "\n",
    "# Keep the latest observation for a given analyst\n",
    "# Group by company fpedats estimator analys then pick the last record in the group\n",
    "\n",
    "ibes_1 = ibes.groupby(['ticker','fpedats','estimator','analys']).apply(lambda x: x.index[-1]).to_frame().reset_index()\n",
    "\n",
    "# reset index to the old dataframe index for join in the next step\n",
    "ibes_1=ibes_1.set_index(0)\n",
    "\n",
    "# Inner join with the last analyst record per group\n",
    "ibes = pd.merge(ibes, ibes_1[['analys']], left_index=True, right_index=True)\n",
    "\n",
    "# drop duplicate column\n",
    "ibes=ibes.drop(['analys_y'], axis=1).rename(columns={'analys_x': 'analys'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link estimates with actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ibes_act.feather\" (5.3 MB) loaded (<1s)\n",
      "\"crsp_dats.feather\" (194.8 KB) loaded (<1s)\n",
      "\"cfacshr.feather\" (42.7 MB) loaded (<1s)\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Step 3. Link Estimates with Actuals #\n",
    "#######################################\n",
    "\n",
    "# Link Unadjusted estimates with Unadjusted actuals and CRSP permnos  \n",
    "# Keep only the estimates issued within 90 days before the report date\n",
    "\n",
    "\n",
    "# Getting actual piece of data\n",
    "\n",
    "ld('ibes_act', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "# Join with the estimate piece of the data\n",
    "\n",
    "ibes1 = pd.merge(ibes, ibes_act, how='left', on = ['ticker','fpedats'])\n",
    "ibes1['dgap'] = ibes1.repdats - ibes1.anndats\n",
    "\n",
    "ibes1['flag'] = np.where( (ibes1.dgap>=datetime.timedelta(days=0)) & (ibes1.dgap<=datetime.timedelta(days=90)) & (ibes1.repdats.notna()) & (ibes1.anndats.notna()), 1, 0)\n",
    "\n",
    "ibes1 = ibes1.loc[ibes1.flag==1].drop(['flag', 'dgap', 'pdicity'], axis=1)\n",
    "\n",
    "\n",
    "# Select all relevant combinations of Permnos and Date\n",
    "\n",
    "ibes1_dt1 = ibes1[['permno', 'anndats']].drop_duplicates()\n",
    "\n",
    "ibes1_dt2 = ibes1[['permno', 'repdats']].drop_duplicates().rename(columns={'repdats':'anndats'})\n",
    "\n",
    "ibes_anndats = pd.concat([ibes1_dt1, ibes1_dt2]).drop_duplicates()\n",
    "\n",
    "# Adjust all estimate and earnings announcement dates to the closest\n",
    "# preceding trading date in CRSP to ensure that adjustment factors won't\n",
    "# be missing after the merge  \n",
    "\n",
    "# unique anndats from ibes\n",
    "uniq_anndats = ibes_anndats[['anndats']].drop_duplicates()\n",
    "\n",
    "# unique trade dates from crsp.dsi\n",
    "ld('crsp_dats', path=WRDS_DOWNLOAD_DIR)\n",
    "\n",
    "# Create up to 5 days prior dates relative to anndats\n",
    "\n",
    "for i in range(0, 5):\n",
    "    uniq_anndats[i] = uniq_anndats.anndats - datetime.timedelta(days=i)\n",
    "\n",
    "# reshape (transpose) the df for later join with crsp trading dates\n",
    "\n",
    "expand_anndats = uniq_anndats.set_index('anndats').stack().reset_index().\\\n",
    "rename(columns={'level_1':'prior', 0:'prior_date'})\n",
    "\n",
    "# merge with crsp trading dates\n",
    "tradedates = pd.merge(expand_anndats, crsp_dats, how='left', left_on=['prior_date'], right_on=['date'])\n",
    "\n",
    "# create the dgap (days gap) variable for min selection\n",
    "tradedates['dgap'] = tradedates.anndats-tradedates.date\n",
    "\n",
    "# choosing the row with the smallest dgap for a given anndats\n",
    "# tradedates = tradedates.loc[tradedates.index.intersection(tradedates.groupby('anndats')['dgap'].idxmin())]\n",
    "tradedates = tradedates.reindex(tradedates.groupby('anndats')['dgap'].idxmin())\n",
    "\n",
    "tradedates = tradedates[['anndats', 'date']]\n",
    "\n",
    "\n",
    "# merge the CRSP adjustment factors for all estimate and report dates\n",
    "\n",
    "# extract CRSP adjustment factors\n",
    "ld('cfacshr', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "ibes_anndats = pd.merge(ibes_anndats, tradedates, how='left', on = ['anndats'])\n",
    "\n",
    "ibes_anndats = pd.merge(ibes_anndats, cfacshr, how='left', on=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjust estimates with `CFACSHR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Step 4. Adjust Estimates with CFACSHR #\n",
    "#########################################\n",
    "\n",
    "# Put the estimate on the same per share basis as\n",
    "# company reported EPS using CRSP Adjustment factors. \n",
    "# New_value is the estimate adjusted to be on the \n",
    "# same basis with reported earnings.\n",
    "\n",
    "ibes1 = pd.merge(ibes1, ibes_anndats, how='inner', on=['permno', 'anndats'])\n",
    "ibes1 = ibes1.drop(['anndats','date'], axis=1).rename(columns={'cfacshr':'cfacshr_ann'})\n",
    "\n",
    "ibes1 = pd.merge(ibes1, ibes_anndats, how='inner', left_on=['permno', 'repdats'], right_on=['permno','anndats'])\n",
    "ibes1 = ibes1.drop(['anndats','date'], axis=1).rename(columns={'cfacshr':'cfacshr_rep'})\n",
    "\n",
    "ibes1['new_value'] = (ibes1.cfacshr_rep/ibes1.cfacshr_ann)*ibes1.value\n",
    "\n",
    "# Sanity check: there should be one most recent estimate for \n",
    "# a given firm-fiscal period end combination \n",
    "ibes1 = ibes1.sort_values(by=['ticker','fpedats','estimator','analys']).drop_duplicates()\n",
    "\n",
    "# Compute the median forecast based on estimates in the 90 days prior to the EAD\n",
    "\n",
    "grp_permno = ibes1.groupby(['ticker','fpedats', 'basis','repdats', 'act']).permno.max().reset_index()\n",
    "\n",
    "medest = ibes1.groupby(['ticker','fpedats', 'basis','repdats', 'act']).new_value.agg(['median','count','std']).reset_index()\n",
    "medest = pd.merge(medest, grp_permno, how='inner', on=['ticker','fpedats','basis', 'repdats', 'act'])\n",
    "medest = medest.rename(columns={'median': 'medest', 'count':'numest', 'std':'stdest'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge with compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fundq.feather\" (52.9 MB) loaded (<1s)\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Step 5. Merge with Compustat Data  #\n",
    "######################################\n",
    "\n",
    "# get items from fundq\n",
    "ld('fundq', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "fundq = fundq.loc[((fundq.atq>0) | (fundq.saleq.notna())) & (fundq.datafqtr.notna())]\n",
    "\n",
    "# Calculate link date ranges for givken gvkey and ticker combination\n",
    "\n",
    "gvkey_mindt1 = gvkey.groupby(['gvkey', 'ticker']).linkdt.min().reset_index().rename(columns={'linkdt':'mindate'})\n",
    "gvkey_maxdt1 = gvkey.groupby(['gvkey', 'ticker']).linkenddt.max().reset_index().rename(columns={'linkenddt':'maxdate'})\n",
    "gvkey_dt1 = pd.merge(gvkey_mindt1, gvkey_maxdt1, how='inner', on=['gvkey','ticker'])\n",
    "\n",
    "\n",
    "# Use the date range to merge\n",
    "comp = pd.merge(fundq, gvkey_dt1, how='left', on =['gvkey'])\n",
    "comp = comp.loc[(comp.ticker.notna()) & (comp.datadate<=comp.maxdate) & (comp.datadate>=comp.mindate)]\n",
    "\n",
    "# Merge with the median esitmates\n",
    "comp = pd.merge(comp, medest, how = 'left', left_on=['ticker','datadate'], right_on=['ticker', 'fpedats'])\n",
    "\n",
    "# Sort data and drop duplicates\n",
    "comp = comp.sort_values(by=['gvkey','fqtr','fyearq']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate SUEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Step 6. Calculate SUEs  #\n",
    "###########################\n",
    "\n",
    "# block handling lag eps\n",
    "\n",
    "sue = comp.sort_values(by=['gvkey','fqtr','fyearq'])\n",
    "\n",
    "sue['dif_fyearq'] = sue.groupby(['gvkey', 'fqtr']).fyearq.diff()\n",
    "sue['laggvkey']   = sue.gvkey.shift(1)\n",
    "\n",
    "# handling same qtr previous year\n",
    "\n",
    "cond_year = sue.dif_fyearq==1 # year increment is 1\n",
    "\n",
    "sue['lagadj']     = np.where(cond_year, sue.ajexq.shift(1), None)\n",
    "sue['lageps_p']   = np.where(cond_year, sue.epspxq.shift(1), None)\n",
    "sue['lageps_d']   = np.where(cond_year, sue.epsfxq.shift(1), None)\n",
    "sue['lagshr_p']   = np.where(cond_year, sue.cshprq.shift(1), None)\n",
    "sue['lagshr_d']   = np.where(cond_year, sue.cshfdq.shift(1), None)\n",
    "sue['lagspiq']    = np.where(cond_year, sue.spiq.shift(1), None)\n",
    "\n",
    "# handling first gvkey\n",
    "\n",
    "cond_gvkey = sue.gvkey != sue.laggvkey # first.gvkey\n",
    "\n",
    "sue['lagadj']     = np.where(cond_gvkey, None, sue.lagadj)\n",
    "sue['lageps_p']   = np.where(cond_gvkey, None, sue.lageps_p)\n",
    "sue['lageps_d']   = np.where(cond_gvkey, None, sue.lageps_d)\n",
    "sue['lagshr_p']   = np.where(cond_gvkey, None, sue.lagshr_p)\n",
    "sue['lagshr_d']   = np.where(cond_gvkey, None, sue.lagshr_d)\n",
    "sue['lagspiq']    = np.where(cond_gvkey, None, sue.lagspiq)\n",
    "\n",
    "\n",
    "# handling reporting basis \n",
    "\n",
    "# Basis = P and missing are treated the same\n",
    "\n",
    "sue['actual1'] = np.where(sue.basis=='D', sue.epsfxq/sue.ajexq, sue.epspxq/sue.ajexq)\n",
    "\n",
    "sue['actual2'] = np.where(sue.basis=='D', \\\n",
    "                            (sue.epsfxq.fillna(0)-(0.65*sue.spiq/sue.cshfdq).fillna(0))/sue.ajexq, \\\n",
    "                            (sue.epspxq.fillna(0)-(0.65*sue.spiq/sue.cshprq).fillna(0))/sue.ajexq\n",
    "                           )\n",
    "\n",
    "sue['expected1'] = np.where(sue.basis=='D', sue.lageps_d/sue.lagadj, sue.lageps_p/sue.lagadj)\n",
    "sue['expected2'] = np.where(sue.basis=='D', \\\n",
    "                              (sue.lageps_d.fillna(0)-(0.65*sue.lagspiq/sue.lagshr_d).fillna(0))/sue.lagadj, \\\n",
    "                              (sue.lageps_p.fillna(0)-(0.65*sue.lagspiq/sue.lagshr_p).fillna(0))/sue.lagadj\n",
    "                             )\n",
    "\n",
    "# SUE calculations\n",
    "sue['sue1'] = (sue.actual1 - sue.expected1) / (sue.prccq/sue.ajexq)\n",
    "sue['sue2'] = (sue.actual2 - sue.expected2) / (sue.prccq/sue.ajexq)\n",
    "sue['sue3'] = (sue.act - sue.medest) / sue.prccq\n",
    "sue['se'] = sue.act / sue.prccq\n",
    "\n",
    "sue = sue[['ticker','permno','gvkey','conm','fyearq','fqtr','fyr','datadate','repdats','rdq', \\\n",
    "           'sue1','sue2','sue3','basis','act', 'se', 'medest','numest', 'stdest','prccq','mcap']]\n",
    "\n",
    "\n",
    "# Shifting the announcement date to be the next trading day\n",
    "# Defining the day after the following quarterly EA as leadrdq1\n",
    "\n",
    "# unique rdq \n",
    "uniq_rdq = comp[['rdq']].drop_duplicates()\n",
    "\n",
    "# Create up to 5 days post rdq relative to rdq\n",
    "for i in range(0, 5):\n",
    "    uniq_rdq[i] = uniq_rdq.rdq + datetime.timedelta(days=i)\n",
    "\n",
    "# reshape (transpose) for later join with crsp trading dates\n",
    "expand_rdq = uniq_rdq.set_index('rdq').stack().reset_index().\\\n",
    "rename(columns={'level_1':'post', 0:'post_date'})\n",
    "\n",
    "# merge with crsp trading dates\n",
    "eads1 = pd.merge(expand_rdq, crsp_dats, how='left', left_on=['post_date'], right_on=['date'])\n",
    "\n",
    "# create the dgap (days gap) variable for min selection\n",
    "eads1['dgap'] = eads1.date-eads1.rdq\n",
    "eads1 = eads1.reindex(eads1.groupby('rdq')['dgap'].idxmin()).rename(columns={'date':'rdq1'})\n",
    "\n",
    "# create sue_final\n",
    "sue_final = pd.merge(sue, eads1[['rdq','rdq1']], how='left', on=['rdq'])\n",
    "sue_final = sue_final.sort_values(by=['gvkey', 'fyearq','fqtr'], ascending=[True, False, False]).drop_duplicates()\n",
    "\n",
    "#  Filter from Livnat & Mendenhall (2006):                                \n",
    "#- earnings announcement date is reported in Compustat                   \n",
    "#- the price per share is available from Compustat at fiscal quarter end  \n",
    "#- price is greater than $1                                              \n",
    "#- the market (book) equity at fiscal quarter end is available and is larger than $5 mil. \n",
    "#- EADs in Compustat and in IBES (if available)should not differ by more  \n",
    "# than one calendar day                               \n",
    "\n",
    "sue_final['leadrdq1'] = sue_final.rdq1.shift(1) # next consecutive EAD\n",
    "sue_final['leadgvkey'] = sue_final.gvkey.shift(1)\n",
    "\n",
    "# If first gvkey then leadrdq1 = rdq1+3 months\n",
    "# Else leadrdq1 = previous rdq1\n",
    "\n",
    "sue_final['leadrdq1'] = np.where(sue_final.gvkey == sue_final.leadgvkey, \n",
    "                                  sue_final.rdq1.shift(1), \n",
    "                                  sue_final.rdq1 + pd.offsets.MonthEnd(3))\n",
    "\n",
    "sue_final['dgap'] = (sue_final.repdats - sue_final.rdq).fillna(pd.Timedelta(seconds=0))\n",
    "sue_final = sue_final.loc[(sue_final.rdq1 != sue_final.leadrdq1)]\n",
    "\n",
    "# Various conditioning for filtering\n",
    "cond1 = (sue_final.sue1.notna()) & (sue_final.sue2.notna()) & (sue_final.repdats.isna())\n",
    "cond2 = (sue_final.repdats.notna()) & (sue_final.dgap<=datetime.timedelta(days=1)) & (sue_final.dgap>=datetime.timedelta(days=-1))\n",
    "sue_final = sue_final.loc[cond1 | cond2]\n",
    "\n",
    "# Impose restriction on price and marketcap\n",
    "sue_final = sue_final.loc[(sue_final.rdq.notna()) & (sue_final.prccq>1) & (sue_final.mcap>5)]\n",
    "\n",
    "# Keep relevant columns\n",
    "sue_final = sue_final[['gvkey', 'ticker','permno','conm',\\\n",
    "                       'fyearq','fqtr','datadate','fyr','rdq','rdq1','leadrdq1','repdats',\\\n",
    "                       'mcap','medest','act', 'numest', 'stdest', 'basis','sue1','sue2','sue3', 'se']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sue_final\" saved as \"sue_final.feather\" (3.8 MB) (<1s)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gvkey</th>\n      <th>ticker</th>\n      <th>permno</th>\n      <th>conm</th>\n      <th>fyearq</th>\n      <th>fqtr</th>\n      <th>datadate</th>\n      <th>fyr</th>\n      <th>rdq</th>\n      <th>rdq1</th>\n      <th>...</th>\n      <th>mcap</th>\n      <th>medest</th>\n      <th>act</th>\n      <th>numest</th>\n      <th>stdest</th>\n      <th>basis</th>\n      <th>sue1</th>\n      <th>sue2</th>\n      <th>sue3</th>\n      <th>se</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>001013</td>\n      <td>ADCT</td>\n      <td>50906.0</td>\n      <td>ADCTELECOMMUNICATIONSINC</td>\n      <td>2010.0</td>\n      <td>4.0</td>\n      <td>2010-09-30</td>\n      <td>9.0</td>\n      <td>2010-11-23</td>\n      <td>2010-11-23</td>\n      <td>...</td>\n      <td>1231.52400</td>\n      <td>0.160</td>\n      <td>0.15</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>D</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.000789</td>\n      <td>0.011839</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>001013</td>\n      <td>ADCT</td>\n      <td>50906.0</td>\n      <td>ADCTELECOMMUNICATIONSINC</td>\n      <td>2010.0</td>\n      <td>3.0</td>\n      <td>2010-06-30</td>\n      <td>9.0</td>\n      <td>2010-08-04</td>\n      <td>2010-08-04</td>\n      <td>...</td>\n      <td>718.77000</td>\n      <td>0.195</td>\n      <td>0.21</td>\n      <td>10.0</td>\n      <td>0.008498</td>\n      <td>D</td>\n      <td>0.0836707</td>\n      <td>0.0780338</td>\n      <td>0.002024</td>\n      <td>0.028340</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>001013</td>\n      <td>ADCT</td>\n      <td>50906.0</td>\n      <td>ADCTELECOMMUNICATIONSINC</td>\n      <td>2010.0</td>\n      <td>2.0</td>\n      <td>2010-03-31</td>\n      <td>9.0</td>\n      <td>2010-05-05</td>\n      <td>2010-05-05</td>\n      <td>...</td>\n      <td>709.07000</td>\n      <td>0.060</td>\n      <td>0.10</td>\n      <td>11.0</td>\n      <td>0.019725</td>\n      <td>D</td>\n      <td>-0.00410397</td>\n      <td>-0.00154479</td>\n      <td>0.005472</td>\n      <td>0.013680</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>001013</td>\n      <td>ADCT</td>\n      <td>50906.0</td>\n      <td>ADCTELECOMMUNICATIONSINC</td>\n      <td>2010.0</td>\n      <td>1.0</td>\n      <td>2009-12-31</td>\n      <td>9.0</td>\n      <td>2010-02-08</td>\n      <td>2010-02-08</td>\n      <td>...</td>\n      <td>601.12800</td>\n      <td>-0.010</td>\n      <td>0.02</td>\n      <td>13.0</td>\n      <td>0.017097</td>\n      <td>D</td>\n      <td>0.723027</td>\n      <td>0.276756</td>\n      <td>0.004831</td>\n      <td>0.003221</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>001013</td>\n      <td>ADCT</td>\n      <td>NaN</td>\n      <td>ADCTELECOMMUNICATIONSINC</td>\n      <td>2009.0</td>\n      <td>3.0</td>\n      <td>2009-07-31</td>\n      <td>10.0</td>\n      <td>2009-09-01</td>\n      <td>2009-09-01</td>\n      <td>...</td>\n      <td>703.24800</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.00686813</td>\n      <td>-0.00195286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>66935</th>\n      <td>326688</td>\n      <td>NVT</td>\n      <td>17676.0</td>\n      <td>NVENTELECTRICPLC</td>\n      <td>2019.0</td>\n      <td>3.0</td>\n      <td>2019-09-30</td>\n      <td>12.0</td>\n      <td>2019-10-30</td>\n      <td>2019-10-30</td>\n      <td>...</td>\n      <td>3728.06600</td>\n      <td>0.470</td>\n      <td>0.49</td>\n      <td>6.0</td>\n      <td>0.010328</td>\n      <td>D</td>\n      <td>-0.00136116</td>\n      <td>-8.37446e-05</td>\n      <td>0.000907</td>\n      <td>0.022232</td>\n    </tr>\n    <tr>\n      <th>66932</th>\n      <td>326688</td>\n      <td>NVT</td>\n      <td>17676.0</td>\n      <td>NVENTELECTRICPLC</td>\n      <td>2019.0</td>\n      <td>2.0</td>\n      <td>2019-06-30</td>\n      <td>12.0</td>\n      <td>2019-08-01</td>\n      <td>2019-08-01</td>\n      <td>...</td>\n      <td>4192.33606</td>\n      <td>0.440</td>\n      <td>0.44</td>\n      <td>3.0</td>\n      <td>0.007638</td>\n      <td>D</td>\n      <td>0.00443727</td>\n      <td>0.000978397</td>\n      <td>0.000000</td>\n      <td>0.017749</td>\n    </tr>\n    <tr>\n      <th>66929</th>\n      <td>326688</td>\n      <td>NVT</td>\n      <td>17676.0</td>\n      <td>NVENTELECTRICPLC</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n      <td>2019-03-31</td>\n      <td>12.0</td>\n      <td>2019-04-25</td>\n      <td>2019-04-25</td>\n      <td>...</td>\n      <td>4712.40774</td>\n      <td>0.385</td>\n      <td>0.39</td>\n      <td>6.0</td>\n      <td>0.011690</td>\n      <td>D</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000185</td>\n      <td>0.014455</td>\n    </tr>\n    <tr>\n      <th>66934</th>\n      <td>326688</td>\n      <td>NVT</td>\n      <td>17676.0</td>\n      <td>NVENTELECTRICPLC</td>\n      <td>2018.0</td>\n      <td>3.0</td>\n      <td>2018-09-30</td>\n      <td>12.0</td>\n      <td>2018-10-25</td>\n      <td>2018-10-25</td>\n      <td>...</td>\n      <td>4873.21016</td>\n      <td>0.450</td>\n      <td>0.46</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>D</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000368</td>\n      <td>0.016937</td>\n    </tr>\n    <tr>\n      <th>66931</th>\n      <td>326688</td>\n      <td>NVT</td>\n      <td>17676.0</td>\n      <td>NVENTELECTRICPLC</td>\n      <td>2018.0</td>\n      <td>2.0</td>\n      <td>2018-06-30</td>\n      <td>12.0</td>\n      <td>2018-07-26</td>\n      <td>2018-07-26</td>\n      <td>...</td>\n      <td>4493.85380</td>\n      <td>0.420</td>\n      <td>0.44</td>\n      <td>7.0</td>\n      <td>0.026095</td>\n      <td>D</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000797</td>\n      <td>0.017530</td>\n    </tr>\n  </tbody>\n</table>\n<p>45849 rows Ã— 22 columns</p>\n</div>",
      "text/plain": "        gvkey ticker   permno                      conm  fyearq  fqtr  \\\n26     001013   ADCT  50906.0  ADCTELECOMMUNICATIONSINC  2010.0   4.0   \n20     001013   ADCT  50906.0  ADCTELECOMMUNICATIONSINC  2010.0   3.0   \n13     001013   ADCT  50906.0  ADCTELECOMMUNICATIONSINC  2010.0   2.0   \n6      001013   ADCT  50906.0  ADCTELECOMMUNICATIONSINC  2010.0   1.0   \n19     001013   ADCT      NaN  ADCTELECOMMUNICATIONSINC  2009.0   3.0   \n...       ...    ...      ...                       ...     ...   ...   \n66935  326688    NVT  17676.0          NVENTELECTRICPLC  2019.0   3.0   \n66932  326688    NVT  17676.0          NVENTELECTRICPLC  2019.0   2.0   \n66929  326688    NVT  17676.0          NVENTELECTRICPLC  2019.0   1.0   \n66934  326688    NVT  17676.0          NVENTELECTRICPLC  2018.0   3.0   \n66931  326688    NVT  17676.0          NVENTELECTRICPLC  2018.0   2.0   \n\n        datadate   fyr        rdq       rdq1  ...        mcap medest   act  \\\n26    2010-09-30   9.0 2010-11-23 2010-11-23  ...  1231.52400  0.160  0.15   \n20    2010-06-30   9.0 2010-08-04 2010-08-04  ...   718.77000  0.195  0.21   \n13    2010-03-31   9.0 2010-05-05 2010-05-05  ...   709.07000  0.060  0.10   \n6     2009-12-31   9.0 2010-02-08 2010-02-08  ...   601.12800 -0.010  0.02   \n19    2009-07-31  10.0 2009-09-01 2009-09-01  ...   703.24800    NaN   NaN   \n...          ...   ...        ...        ...  ...         ...    ...   ...   \n66935 2019-09-30  12.0 2019-10-30 2019-10-30  ...  3728.06600  0.470  0.49   \n66932 2019-06-30  12.0 2019-08-01 2019-08-01  ...  4192.33606  0.440  0.44   \n66929 2019-03-31  12.0 2019-04-25 2019-04-25  ...  4712.40774  0.385  0.39   \n66934 2018-09-30  12.0 2018-10-25 2018-10-25  ...  4873.21016  0.450  0.46   \n66931 2018-06-30  12.0 2018-07-26 2018-07-26  ...  4493.85380  0.420  0.44   \n\n       numest    stdest  basis        sue1         sue2      sue3        se  \n26        1.0       NaN      D         NaN          NaN -0.000789  0.011839  \n20       10.0  0.008498      D   0.0836707    0.0780338  0.002024  0.028340  \n13       11.0  0.019725      D -0.00410397  -0.00154479  0.005472  0.013680  \n6        13.0  0.017097      D    0.723027     0.276756  0.004831  0.003221  \n19        NaN       NaN    NaN -0.00686813  -0.00195286       NaN       NaN  \n...       ...       ...    ...         ...          ...       ...       ...  \n66935     6.0  0.010328      D -0.00136116 -8.37446e-05  0.000907  0.022232  \n66932     3.0  0.007638      D  0.00443727  0.000978397  0.000000  0.017749  \n66929     6.0  0.011690      D         NaN          NaN  0.000185  0.014455  \n66934     1.0       NaN      D         NaN          NaN  0.000368  0.016937  \n66931     7.0  0.026095      D         NaN          NaN  0.000797  0.017530  \n\n[45849 rows x 22 columns]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv('sue_final')\n",
    "sue_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# DGTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Characteristics-Based Benchmarks       #\n",
    "# May 2018                               #                        \n",
    "# Qingyi (Freda) Song Drechsler          #\n",
    "##########################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import wrds\n",
    "import psycopg2 \n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import *\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# last download: 2020-1-27\n",
    "crsp_m = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.permco, b.ncusip, a.date, \n",
    "                      b.shrcd, b.exchcd, b.siccd,\n",
    "                      a.ret, a.vol, a.shrout, a.prc, a.cfacpr, a.cfacshr\n",
    "                      from crsp.msf as a\n",
    "                      left join crsp.msenames as b\n",
    "                      on a.permno=b.permno\n",
    "                      and b.namedt<=a.date\n",
    "                      and a.date<=b.nameendt\n",
    "                      where a.date between '01/01/2005' and '12/31/2019'\n",
    "                      and b.shrcd between 10 and 11\n",
    "                      \"\"\") \n",
    "sv('crsp_m', path=WRDS_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-comp- saved\n"
     ]
    }
   ],
   "source": [
    "# last download: 2020-1-27\n",
    "comp = conn.raw_sql(\"\"\"\n",
    "                    select gvkey, datadate, cusip, \n",
    "                    sich, seq, pstkrv, pstkl, pstk, txdb, itcb\n",
    "                    from comp.funda\n",
    "                    where indfmt='INDL' \n",
    "                    and datafmt='STD'\n",
    "                    and popsrc='D'\n",
    "                    and consol='C'\n",
    "                    and datadate >= '01/01/1970'\n",
    "                    \"\"\")\n",
    "sv('comp', path=WRDS_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ccm- saved\n"
     ]
    }
   ],
   "source": [
    "# last download: 2020-1-27\n",
    "ccm=conn.raw_sql(\"\"\"\n",
    "                  select gvkey, lpermco as permco, linktype, linkprim, \n",
    "                  linkdt, linkenddt\n",
    "                  from crsp.ccmxpf_linktable\n",
    "                  where (linktype ='LU' or linktype='LC')\n",
    "                  \"\"\")\n",
    "sv('ccm', path=WRDS_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRSP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-crsp_m- loaded\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "###################\n",
    "# CRSP Block      #\n",
    "###################\n",
    "# sql similar to crspmerge macro\n",
    "ld('crsp_m', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "# change variable format to int\n",
    "crsp_m[['permco','permno','shrcd','exchcd']]=\\\n",
    "    crsp_m[['permco','permno','shrcd','exchcd']].astype(int)\n",
    "\n",
    "# Line up date to be end of month\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date'])\n",
    "crsp_m['jdate']=crsp_m['date']+MonthEnd(0)\n",
    "crsp_m['p']=crsp_m['prc'].abs()/crsp_m['cfacpr'] # price adjusted\n",
    "crsp_m['tso']=crsp_m['shrout']*crsp_m['cfacshr']*1e3 # total shares out adjusted\n",
    "crsp_m['me'] = crsp_m['p']*crsp_m['tso']/1e6 # market cap in $mil\n",
    "\n",
    "# sum of me across different permno belonging to same permco a given date\n",
    "crsp_summe = crsp_m.groupby(['jdate','permco'])['me'].sum().reset_index()\\\n",
    "    .rename(columns={'me':'me_comp'})\n",
    "crsp_m=pd.merge(crsp_m, crsp_summe, how='inner', on=['jdate','permco'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compustat block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-comp- loaded\n",
      "Wall time: 822 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "###################\n",
    "# Compustat Block #\n",
    "###################\n",
    "\n",
    "ld('comp', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "comp['datadate']=pd.to_datetime(comp['datadate']) #convert datadate to date fmt\n",
    "comp['year']=comp['datadate'].dt.year\n",
    "\n",
    "comp = comp[comp['seq']>0]\n",
    "\n",
    "# create preferrerd stock:\n",
    "# 1st choice: Preferred stock - Redemption Value\n",
    "# 2nd choice: Preferred stock - Liquidating Value\n",
    "# 3rd choice: Preferred stock - Carrying Value, Stock (Capital) - Total\n",
    "comp['pref']=np.where(comp['pstkrv'].isnull(), comp['pstkl'], comp['pstkrv'])\n",
    "comp['pref']=np.where(comp['pref'].isnull(),comp['pstk'], comp['pref'])\n",
    "comp['pref']=np.where(comp['pref'].isnull(),0,comp['pref'])\n",
    "\n",
    "# fill in missing values for deferred taxes and investment tax credit\n",
    "comp['txdb']=comp['txdb'].fillna(0)\n",
    "comp['itcb']=comp['itcb'].fillna(0)\n",
    "\n",
    "# create book equity\n",
    "# Daniel and Titman (JF 1997):    \n",
    "# BE = stockholders' equity + deferred taxes + investment tax credit - Preferred Stock\n",
    "comp['be']=comp['seq']+comp['txdb']+comp['itcb']-comp['pref']\n",
    "\n",
    "# keep only records with non-negative book equity\n",
    "comp = comp[comp['be']>=0]\n",
    "comp=comp[['gvkey','datadate','year','be','sich']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add historical `permco`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ccm- loaded\n",
      "Wall time: 681 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# Add Historical PERMCO #\n",
    "#########################\n",
    "\n",
    "ld('ccm', path=WRDS_DOWNLOAD_DIR, force=True)\n",
    "\n",
    "ccm['linkdt']=pd.to_datetime(ccm['linkdt'])\n",
    "ccm['linkenddt']=pd.to_datetime(ccm['linkenddt'])\n",
    "# if linkenddt is missing then set to today date\n",
    "ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "ccm1=pd.merge(comp,ccm,how='left',on=['gvkey'])\n",
    "ccm1['jdate']=ccm1['datadate']+MonthEnd(0)\n",
    "ccm1['year']=ccm1.datadate.dt.year\n",
    "\n",
    "# set link date bounds\n",
    "comp2=ccm1[(ccm1['datadate']>=ccm1['linkdt'])&(ccm1['datadate']<=ccm1['linkenddt'])]\n",
    "comp2=comp2[['gvkey','permco','datadate', 'year','jdate', 'be', 'sich', 'linkprim']]\n",
    "\n",
    "\n",
    "# link comp and crsp to calculate book-to-market ratio each fiscal year end\n",
    "comp3=pd.merge(comp2, crsp_m[['permno','permco','date','jdate','siccd','me','me_comp']],\\\n",
    "               how='inner', on=['permco', 'jdate'])\n",
    "comp3['bm']=comp3['be'].div(comp3['me_comp'])\n",
    "\n",
    "comp3 = comp3.sort_values(['permno', 'year', 'datadate', 'linkprim', 'bm'])\\\n",
    "    .drop_duplicates()\n",
    "\n",
    "# pick max datadate for a given permno year combo (firm changes fiscal period)\n",
    "maxdatadate=comp3.groupby(['permno','year'])['datadate'].max()\\\n",
    "    .reset_index()\n",
    "\n",
    "comp3 = pd.merge(comp3, maxdatadate, how='inner', on=['permno','year','datadate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign ff-48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 571 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# Assign Fama-French 48 #\n",
    "#########################\n",
    "\n",
    "# function to assign ffi48 classification\n",
    "def ffi48(row):\n",
    "    if (100<=row['sic'] <=299) or (700<=row['sic']<=799) or (910<=row['sic']<=919) or (row['sic']==2048):\n",
    "        ffi48=1\n",
    "        ffi48_desc='Agric'\n",
    "    elif (2000<=row['sic']<=2046) or (2050<=row['sic']<=2063) or (2070<=row['sic']<=2079)\\\n",
    "    or (2090<=row['sic']<=2092) or (row['sic']==2095) or (2098<=row['sic']<=2099):\n",
    "        ffi48=2\n",
    "        ffi48_desc='Food'\n",
    "    elif (2064<=row['sic']<=2068) or (2086<=row['sic']<=2087) or (2096<=row['sic']<=2097):\n",
    "        ffi48=3\n",
    "        ffi48_desc='Soda'\n",
    "    elif (row['sic']==2080) or (2082<=row['sic']<=2085):\n",
    "        ffi48=4\n",
    "        ffi48_desc='Beer'\n",
    "    elif (2100<=row['sic']<=2199):\n",
    "        ffi48=5\n",
    "        ffi48_desc='Smoke'\n",
    "    elif (920<=row['sic']<=999) or (3650<=row['sic']<=3652) or (row['sic']==3732) or (3930<=row['sic']<=3931) or (3940<=row['sic']<=3949):\n",
    "        ffi48=6\n",
    "        ffi48_desc='Toys'\n",
    "    elif (7800<=row['sic']<=7833) or (7840<=row['sic']<=7841) or(row['sic']==7900)or (7910<=row['sic']<=7911) or (7920<=row['sic']<=7933)\\\n",
    "    or (7940<=row['sic']<=7949) or (row['sic']==7980) or (7990<=row['sic']<=7999):\n",
    "        ffi48=7\n",
    "        ffi48_desc='Fun'\n",
    "    elif (2700<=row['sic']<=2749) or (2770<=row['sic']<=2771) or (2780<=row['sic']<=2799):\n",
    "        ffi48=8\n",
    "        ffi48_desc='Books'\n",
    "    elif (row['sic']==2047) or (2391<=row['sic']<=2392) or (2510<=row['sic']<=2519) or (2590<=row['sic']<=2599) or (2840<=row['sic']<=2844)\\\n",
    "    or (3160<=row['sic']<=3161) or (3170<=row['sic']<=3172) or (3190<=row['sic']<=3199) or (row['sic']==3229) or (row['sic']==3260)\\\n",
    "    or (3262<=row['sic']<=3263) or (row['sic']==3269) or (3230<=row['sic']<=3231) or(3630<=row['sic']<=3639) or (3750<=row['sic']<=3751)\\\n",
    "    or (row['sic']==3800) or (3860<=row['sic']<=3861) or (3870<=row['sic']<=3873) or (3910<=row['sic']<=3911) or (3914<=row['sic']<=3915)\\\n",
    "    or (3960<=row['sic']<=3962) or (row['sic']==3991) or (row['sic']==3995):\n",
    "        ffi48=9\n",
    "        ffi48_desc='Hshld'\n",
    "    elif (2300<=row['sic']<=2390) or (3020<=row['sic']<=3021) or (3100<=row['sic']<=3111)\\\n",
    "    or (3130<=row['sic']<=3131) or (3140<=row['sic']<=3151) or (3963<=row['sic']<=3965):\n",
    "        ffi48=10\n",
    "        ffi48_desc='Clths'\n",
    "    elif (8000<=row['sic']<=8099):\n",
    "        ffi48=11\n",
    "        ffi48_desc='Hlth'\n",
    "    elif (row['sic']==3693) or (3840<=row['sic']<=3851):\n",
    "        ffi48=12\n",
    "        ffi48_desc='MedEq'\n",
    "    elif (2830<=row['sic']<=2831) or (2833<=row['sic']<=2836):\n",
    "        ffi48=13\n",
    "        ffi48_desc='Drugs'\n",
    "    elif (2800<=row['sic']<=2829) or (2850<=row['sic']<=2879) or (2890<=row['sic']<=2899):\n",
    "        ffi48=14\n",
    "        ffi48_desc='Chems'\n",
    "    elif (row['sic']==3031) or (row['sic']==3041) or (3050<=row['sic']<=3053) or (3060<=row['sic']<=3069) or (3070<=row['sic']<=3099):\n",
    "        ffi48=15\n",
    "        ffi48_desc='Rubbr'\n",
    "    elif (2200<=row['sic']<=2284) or (2290<=row['sic']<=2295) or (2297<=row['sic']<=2299) or (2393<=row['sic']<=2395) or (2397<=row['sic']<=2399):\n",
    "        ffi48=16\n",
    "        ffi48_desc='Txtls'\n",
    "    elif (800<=row['sic']<=899) or (2400<=row['sic']<=2439) or (2450<=row['sic']<=2459) or (2490<=row['sic']<=2499) or (2660<=row['sic']<=2661)\\\n",
    "    or (2950<=row['sic']<=2952) or (row['sic']==3200) or (3210<=row['sic']<=3211) or (3240<=row['sic']<=3241) or (3250<=row['sic']<=3259)\\\n",
    "    or (row['sic']==3261) or (row['sic']==3264) or (3270<=row['sic']<=3275) or (3280<=row['sic']<=3281) or (3290<=row['sic']<=3293)\\\n",
    "    or (3295<=row['sic']<=3299) or (3420<=row['sic']<=3433) or (3440<=row['sic']<=3442) or (row['sic']==3446) or (3448<=row['sic']<=3452)\\\n",
    "    or (3490<=row['sic']<=3499) or (row['sic']==3996):\n",
    "        ffi48=17\n",
    "        ffi48_desc='BldMt'\n",
    "    elif (1500<=row['sic']<=1511) or (1520<=row['sic']<=1549) or (1600<=row['sic']<=1799):\n",
    "        ffi48=18\n",
    "        ffi48_desc='Cnstr'\n",
    "    elif (row['sic']==3300) or (3310<=row['sic']<=3317) or (3320<=row['sic']<=3325) or (3330<=row['sic']<=3341) or(3350<=row['sic']<=3357)\\\n",
    "    or (3360<=row['sic']<=3379) or (3390<=row['sic']<=3399):\n",
    "        ffi48=19\n",
    "        ffi48_desc='Steel'\n",
    "    elif (row['sic']==3400) or (3443<=row['sic']<=3444) or (3460<=row['sic']<=3479):\n",
    "        ffi48=20\n",
    "        ffi48_desc='FabPr'\n",
    "    elif (3510<=row['sic']<=3536) or (row['sic']==3538) or (3540<=row['sic']<=3569)\\\n",
    "    or (3580<=row['sic']<=3582) or (3585<=row['sic']<=3586) or (3589<=row['sic']<=3599):\n",
    "        ffi48=21\n",
    "        ffi48_desc='Mach'\n",
    "    elif (row['sic']==3600) or (3610<=row['sic']<=3613) or (3620<=row['sic']<=3621) or (3623<=row['sic']<=3629) or (3640<=row['sic']<=3646)\\\n",
    "    or (3648<=row['sic']<=3649) or (row['sic']==3660) or (3690<=row['sic']<=3692) or (row['sic']==3699):\n",
    "            ffi48=22\n",
    "            ffi48_desc='ElcEq'\n",
    "    elif (row['sic']==2296) or (row['sic']==2396) or (3010<=row['sic']<=3011) or (row['sic']==3537) or (row['sic']==3647) or (row['sic']==3694)\\\n",
    "    or (row['sic']==3700) or (3710<=row['sic']<=3711) or (3713<=row['sic']<=3716) or (3790<=row['sic']<=3792) or (row['sic']==3799):\n",
    "        ffi48=23\n",
    "        ffi48_desc='Autos'\n",
    "    elif (3720<=row['sic']<=3721) or (3723<=row['sic']<=3725) or (3728<=row['sic']<=3729):\n",
    "        ffi48=24\n",
    "        ffi48_desc='Aero'\n",
    "    elif (3730<=row['sic']<=3731) or (3740<=row['sic']<=3743):\n",
    "        ffi48=25\n",
    "        ffi48_desc='Ships'\n",
    "    elif (3760<=row['sic']<=3769) or (row['sic']==3795) or (3480<=row['sic']<=3489):\n",
    "        ffi48=26\n",
    "        ffi48_desc='Guns'\n",
    "    elif (1040<=row['sic']<=1049):\n",
    "        ffi48=27\n",
    "        ffi48_desc='Gold'\n",
    "    elif (1000<=row['sic']<=1039) or (1050<=row['sic']<=1119) or (1400<=row['sic']<=1499):\n",
    "        ffi48=28\n",
    "        ffi48_desc='Mines'\n",
    "    elif (1200<=row['sic']<=1299):\n",
    "        ffi48=29\n",
    "        ffi48_desc='Coal'\n",
    "    elif (row['sic']==1300) or (1310<=row['sic']<=1339) or (1370<=row['sic']<=1382) or (row['sic']==1389) or (2900<=row['sic']<=2912) or (2990<=row['sic']<=2999):\n",
    "        ffi48=30\n",
    "        ffi48_desc='Oil'\n",
    "    elif (row['sic']==4900) or (4910<=row['sic']<=4911) or (4920<=row['sic']<=4925) or (4930<=row['sic']<=4932) or (4939<=row['sic']<=4942):\n",
    "        ffi48=31\n",
    "        ffi48_desc='Util'\n",
    "    elif (row['sic']==4800) or (4810<=row['sic']<=4813) or (4820<=row['sic']<=4822) or (4830<=row['sic']<=4841) or (4880<=row['sic']<=4892) or (row['sic']==4899):\n",
    "        ffi48=32\n",
    "        ffi48_desc='Telcm'\n",
    "    elif (7020<=row['sic']<=7021) or (7030<=row['sic']<=7033) or (row['sic']==7200) or (7210<=row['sic']<=7212) or (7214<=row['sic']<=7217)\\\n",
    "    or (7219<=row['sic']<=7221) or (7230<=row['sic']<=7231) or (7240<=row['sic']<=7241) or (7250<=row['sic']<=7251) or (7260<=row['sic']<=7299)\\\n",
    "    or (row['sic']==7395) or (row['sic']==7500) or (7520<=row['sic']<=7549) or (row['sic']==7600) or (row['sic']==7620)\\\n",
    "    or (7622<=row['sic']<=7623) or (7629<=row['sic']<=7631) or (7640<=row['sic']<=7641) or (7690<=row['sic']<=7699) or (8100<=row['sic']<=8499)\\\n",
    "    or (8600<=row['sic']<=8699) or (8800<=row['sic']<=8899) or (7510<=row['sic']<=7515):\n",
    "        ffi48=33\n",
    "        ffi48_desc='PerSv'\n",
    "    elif (2750<=row['sic']<=2759) or (row['sic']==3993) or (row['sic']==7218) or (row['sic']==7300) or (7310<=row['sic']<=7342)\\\n",
    "    or (7349<=row['sic']<=7353) or (7359<=row['sic']<=7372) or (7374<=row['sic']<=7385) or (7389<=row['sic']<=7394) or (7396<=row['sic']<=7397)\\\n",
    "    or (row['sic']==7399) or (row['sic']==7519) or (row['sic']==8700) or (8710<=row['sic']<=8713) or (8720<=row['sic']<=8721) \\\n",
    "    or (8730<=row['sic']<=8734) or (8740<=row['sic']<=8748) or (8900<=row['sic']<=8911) or (8920<=row['sic']<=8999) or (4220<=row['sic']<=4229):\n",
    "        ffi48=34\n",
    "        ffi48_desc='BusSv'\n",
    "    elif (3570<=row['sic']<=3579) or (3680<=row['sic']<=3689) or (row['sic']==3695) or (row['sic']==7373):\n",
    "        ffi48=35\n",
    "        ffi48_desc='Comps'\n",
    "    elif (row['sic']==3622) or (3661<=row['sic']<=3666) or (3669<=row['sic']<=3679) or (row['sic']==3810) or (row['sic']==3812):\n",
    "        ffi48=36\n",
    "        ffi48_desc='Chips'\n",
    "    elif (row['sic']==3811) or (3820<=row['sic']<=3827) or (3829<=row['sic']<=3839):\n",
    "        ffi48=37\n",
    "        ffi48_desc='LabEq'\n",
    "    elif (2520<=row['sic']<=2549) or (2600<=row['sic']<=2639) or (2670<=row['sic']<=2699) or (2760<=row['sic']<=2761) or (3950<=row['sic']<=3955):\n",
    "        ffi48=38\n",
    "        ffi48_desc='Paper'\n",
    "    elif (2440<=row['sic']<=2449) or (2640<=row['sic']<=2659) or (3220<=row['sic']<=3221) or (3410<=row['sic']<=3412):\n",
    "        ffi48=39\n",
    "        ffi48_desc='Boxes'\n",
    "    elif (4000<=row['sic']<=4013) or (4040<=row['sic']<=4049) or (row['sic']==4100)  or (4110<=row['sic']<=4121) or (4130<=row['sic']<=4131)\\\n",
    "    or (4140<=row['sic']<=4142) or (4150<=row['sic']<=4151) or (4170<=row['sic']<=4173) or (4190<=row['sic']<=4200)\\\n",
    "    or (4210<=row['sic']<=4219) or (4230<=row['sic']<=4231) or (4240<=row['sic']<=4249) or (4400<=row['sic']<=4700) or (4710<=row['sic']<=4712)\\\n",
    "    or (4720<=row['sic']<=4749) or (row['sic']==4780) or (4782<=row['sic']<=4785) or (row['sic']==4789):\n",
    "        ffi48=40\n",
    "        ffi48_desc='Trans'\n",
    "    elif (row['sic']==5000) or (5010<=row['sic']<=5015) or (5020<=row['sic']<=5023) or (5030<=row['sic']<=5060) or (5063<=row['sic']<=5065)\\\n",
    "    or (5070<=row['sic']<=5078) or (5080<=row['sic']<=5088) or (5090<=row['sic']<=5094) or (5099<=row['sic']<=5100)\\\n",
    "    or (5110<=row['sic']<=5113) or (5120<=row['sic']<=5122) or (5130<=row['sic']<=5172) or (5180<=row['sic']<=5182) or (5190<=row['sic']<=5199):\n",
    "        ffi48=41\n",
    "        ffi48_desc='Whlsl'\n",
    "    elif (row['sic']==5200) or (5210<=row['sic']<=5231) or (5250<=row['sic']<=5251) or (5260<=row['sic']<=5261) or (5270<=row['sic']<=5271)\\\n",
    "    or (row['sic']==5300) or (5310<=row['sic']<=5311) or (row['sic']==5320) or (5330<=row['sic']<=5331) or (row['sic']==5334)\\\n",
    "    or (5340<=row['sic']<=5349) or (5390<=row['sic']<=5400) or (5410<=row['sic']<=5412) or (5420<=row['sic']<=5469) or (5490<=row['sic']<=5500)\\\n",
    "    or (5510<=row['sic']<=5579) or (5590<=row['sic']<=5700) or (5710<=row['sic']<=5722) or (5730<=row['sic']<=5736) or (5750<=row['sic']<=5799)\\\n",
    "    or (row['sic']==5900) or (5910<=row['sic']<=5912) or (5920<=row['sic']<=5932) or (5940<=row['sic']<=5990) or (5992<=row['sic']<=5995) or (row['sic']==5999):\n",
    "        ffi48=42\n",
    "        ffi48_desc='Rtail'\n",
    "    elif (5800<=row['sic']<=5829) or (5890<=row['sic']<=5899) or (row['sic']==7000) or (7010<=row['sic']<=7019) or (7040<=row['sic']<=7049) or (row['sic']==7213):\n",
    "        ffi48=43\n",
    "        ffi48_desc='Meals'\n",
    "    elif (row['sic']==6000) or (6010<=row['sic']<=6036) or (6040<=row['sic']<=6062) or (6080<=row['sic']<=6082) or (6090<=row['sic']<=6100)\\\n",
    "    or (6110<=row['sic']<=6113) or (6120<=row['sic']<=6179) or (6190<=row['sic']<=6199):\n",
    "        ffi48=44\n",
    "        ffi48_desc='Banks'\n",
    "    elif (row['sic']==6300) or (6310<=row['sic']<=6331) or (6350<=row['sic']<=6351) or (6360<=row['sic']<=6361) or (6370<=row['sic']<=6379) or (6390<=row['sic']<=6411):\n",
    "        ffi48=45\n",
    "        ffi48_desc='Insur'\n",
    "    elif (row['sic']==6500) or (row['sic']==6510) or (6512<=row['sic']<=6515) or (6517<=row['sic']<=6532) or (6540<=row['sic']<=6541)\\\n",
    "    or (6550<=row['sic']<=6553) or (6590<=row['sic']<=6599) or (6610<=row['sic']<=6611):\n",
    "        ffi48=46\n",
    "        ffi48_desc='RlEst'\n",
    "    elif (6200<=row['sic']<=6299) or (row['sic']==6700) or (6710<=row['sic']<=6726) or (6730<=row['sic']<=6733) or (6740<=row['sic']<=6779)\\\n",
    "    or (6790<=row['sic']<=6795) or (6798<=row['sic']<=6799):\n",
    "        ffi48=47\n",
    "        ffi48_desc='Fin'\n",
    "    elif (4950<=row['sic']<=4961) or (4970<=row['sic']<=4971) or (4990<=row['sic']<=4991) or (row['sic']==9999):\n",
    "        ffi48=48\n",
    "        ffi48_desc='Other'\n",
    "    else:\n",
    "        ffi48=np.nan\n",
    "        ffi48_desc=''\n",
    "    return pd.Series({'sic': row['sic'], 'ffi48': ffi48, 'ffi48_desc': ffi48_desc})\n",
    "\n",
    "# assign SIC code\n",
    "comp4 = comp3\n",
    "# First use historical Compustat SIC Code\n",
    "# Then if missing use historical CRSP SIC Code\n",
    "comp4['sic']=np.where(comp4['sich']>0, comp4['sich'], comp4['siccd'])\n",
    "\n",
    "# and adjust some SIC code to fit F&F 48 ind delineation\n",
    "comp4['sic']=np.where((comp4['sic'].isin([3990, 9995, 9997])) & (comp4['siccd']>0) & (comp4['sic'] != comp4['siccd']), \\\n",
    "             comp4['siccd'], comp4['sic'])\n",
    "comp4['sic']=np.where(comp4['sic'].isin([3990,3999]), 3991, comp4['sic'])\n",
    "comp4['sic']=comp4.sic.astype(int)\n",
    "\n",
    "# assign the ffi48 function to comp4\n",
    "_sic = comp4['sic'].unique()\n",
    "_sicff = pd.DataFrame(_sic).rename(columns={0:'sic'})\n",
    "_sicff = _sicff.apply(ffi48, axis=1)\n",
    "comp4 = pd.merge(comp4, _sicff, how='left', on=['sic'])\n",
    "\n",
    "# keep only records with non-missing bm and ffi48 classification\n",
    "comp4 = comp4[(comp4['bm'] != np.NaN) & (comp4['ffi48_desc'] !='')] \n",
    "comp4 = comp4.drop(['sich','siccd','datadate'], axis=1)\n",
    "comp4=comp4.sort_values(['ffi48','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## industry BM average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# Industry BM Average   #\n",
    "#########################\n",
    "\n",
    "# Calculate BM Industry Average Each Period\n",
    "comp4_tmp = comp4[(comp4['ffi48']>0)&(comp4['bm']>=0)]\n",
    "bm_ind = comp4_tmp.groupby(['ffi48','year'])['bm'].mean().reset_index().rename(columns={'bm':'bmind'})\n",
    "\n",
    "# Calculate Long-Term Industry BtM Average\n",
    "bm_ind['n'] = bm_ind.groupby(['ffi48'])['year'].cumcount()\n",
    "bm_ind['sumbm']=bm_ind.groupby(['ffi48'])['bmind'].cumsum()\n",
    "bm_ind['bmavg'] = bm_ind['sumbm']/(bm_ind['n']+1)\n",
    "bm_ind = bm_ind.drop(['n','sumbm'], axis=1)\n",
    "\n",
    "# Adjust Firm-Specific BtM with Industry Averages\n",
    "comp5 = pd.merge(comp4, bm_ind, how='left',on=['ffi48','year'])\n",
    "comp5['bm_adj'] = comp5['bm']-comp5['bmavg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## momentum factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# Momentum Factor       #\n",
    "#########################\n",
    "\n",
    "# Create (12,1) Momentum Factor with at least 6 months of returns\n",
    "_tmp_crsp = crsp_m[['permno','date','ret', 'me', 'exchcd']].sort_values(['permno','date']).set_index('date')\n",
    "#replace missing return with 0\n",
    "_tmp_crsp['ret']=_tmp_crsp['ret'].fillna(0)\n",
    "_tmp_crsp['logret']=np.log(1+_tmp_crsp['ret'])\n",
    "_tmp_cumret = _tmp_crsp.groupby(['permno'])['logret'].rolling(12, min_periods=7).sum()\n",
    "_tmp_cumret = _tmp_cumret.reset_index()\n",
    "_tmp_cumret['cumret']=np.exp(_tmp_cumret['logret'])-1\n",
    "\n",
    "sizemom = pd.merge(_tmp_crsp.reset_index(), _tmp_cumret[['permno','date','cumret']], how='left', on=['permno','date'])\n",
    "sizemom['mom']=sizemom.groupby('permno')['cumret'].shift(1)\n",
    "sizemom=sizemom[sizemom['date'].dt.month==6].drop(['logret','cumret'], axis=1).rename(columns={'me':'size'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYSE size breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# NYSE Size Breakpoint  #\n",
    "#########################\n",
    "\n",
    "# Get Size Breakpoints for NYSE firms\n",
    "sizemom=sizemom.sort_values(['date','permno']).drop_duplicates()\n",
    "nyse = sizemom[sizemom['exchcd']==1]\n",
    "nyse_break = nyse.groupby(['date'])['size'].describe(percentiles=[.2,.4,.6,.8]).reset_index()\n",
    "nyse_break = nyse_break[['date','20%','40%','60%','80%']]\\\n",
    ".rename(columns={'20%':'dec20', '40%':'dec40', '60%':'dec60','80%':'dec80'})\n",
    "\n",
    "sizemom = pd.merge(sizemom, nyse_break, how='left', on='date')\n",
    "\n",
    "# Add NYSE Size Breakpoints to the Data\n",
    "def size_group(row):\n",
    "    if 0<=row['size'] < row['dec20']:\n",
    "        value = 1\n",
    "    elif row['size'] < row['dec40']:\n",
    "        value=2\n",
    "    elif row['size'] < row['dec60']:\n",
    "        value=3\n",
    "    elif row['size'] < row['dec80']:\n",
    "        value=4\n",
    "    elif row['size'] >= row['dec80']:\n",
    "        value=5\n",
    "    else:\n",
    "        value=np.nan\n",
    "    return value\n",
    "\n",
    "sizemom['group']=sizemom.apply(size_group, axis=1)\n",
    "sizemom['year']=sizemom['date'].dt.year-1\n",
    "sizemom=sizemom[['permno','date','year','mom','group','size','ret']]\n",
    "\n",
    "# Adjusted BtM from the calendar year preceding the formation date\n",
    "comp6=comp5[['gvkey','permno','year','bm_adj']]\n",
    "comp6=pd.merge(comp6, sizemom, how='inner', on=['permno','year'])\n",
    "comp6=comp6.dropna(subset=['size','mom','bm_adj','ret'], how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## size BM MOM portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#########################\n",
    "# Size BM MOM Portfolio #\n",
    "#########################\n",
    "\n",
    "# Start the Triple Sort on Size, Book-to-Market, and Momentum\n",
    "port1=comp6.sort_values(['date','group','permno']).drop_duplicates()\n",
    "port1['bmr']=port1.groupby(['date','group'])['bm_adj'].transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop'))\n",
    "port2 = port1.sort_values(['date','group','bmr'])\n",
    "port2['momr']=port2.groupby(['date','group','bmr'])['mom'].transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop'))\n",
    "\n",
    "# DGTW_PORT 1 for Bottom Quintile, 5 for Top Quintile\n",
    "port3=port2\n",
    "port3['bmr']=port3['bmr']+1\n",
    "port3['momr']=port3['momr']+1\n",
    "port3[['group','bmr','momr']]=port3[['group','bmr','momr']].astype(int).astype(str)\n",
    "port3['dgtw_port']=port3['group']+port3['bmr']+port3['momr']\n",
    "port4 = port3[['permno','gvkey','date','size','mom','bm_adj','dgtw_port']]\n",
    "port4['date']=port4['date']+MonthEnd(0)\n",
    "port4['jyear']=port4['date'].dt.year\n",
    "port4=port4.sort_values(['permno','date'])\n",
    "port4=port4.rename(columns={'date':'formdate', 'size':'sizew'})\n",
    "port4=port4[['permno','formdate','jyear','sizew','dgtw_port']]\n",
    "\n",
    "crsp_m1= crsp_m[['permno','date','ret']]\n",
    "crsp_m1['date']=crsp_m1['date']+MonthEnd(0)\n",
    "crsp_m1['jdate']=crsp_m1['date']+MonthEnd(-6)\n",
    "crsp_m1['jyear']=crsp_m1['jdate'].dt.year\n",
    "\n",
    "crsp_m1 = pd.merge(crsp_m1.drop(['jdate'],axis=1), port4, how='left', on=['permno','jyear'])\n",
    "crsp_m1 = crsp_m1.dropna(subset=['formdate','sizew','dgtw_port'], how='any')\n",
    "\n",
    "crsp_m1 = crsp_m1.sort_values(['date','dgtw_port','permno'])\n",
    "\n",
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "# Calculate Weighted Average Returns    \n",
    "dgtw_vwret = crsp_m1.groupby(['date','dgtw_port']).apply(wavg, 'ret','sizew')\n",
    "dgtw_vwret = dgtw_vwret.reset_index().rename(columns={0:'dgtw_vwret'})\n",
    "\n",
    "# Calculate DGTW Excess Return\n",
    "dgtw_returns = pd.merge(crsp_m1.drop(['sizew'], axis=1), dgtw_vwret, how='left', on =['dgtw_port','date'])\n",
    "dgtw_returns['dgtw_xret']=dgtw_returns['ret']-dgtw_returns['dgtw_vwret']\n",
    "dgtw_returns = dgtw_returns.sort_values(['permno','date']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp6[['gvkey', 'permno', 'date', 'size', 'bm_adj', 'mom']].to_csv('data/dgtw_port.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>permno</th>\n",
       "      <th>year</th>\n",
       "      <th>bm_adj</th>\n",
       "      <th>date</th>\n",
       "      <th>mom</th>\n",
       "      <th>group</th>\n",
       "      <th>size</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>013155</td>\n",
       "      <td>11082</td>\n",
       "      <td>2005</td>\n",
       "      <td>-0.620109</td>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>-0.299828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.4565</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gvkey  permno  year    bm_adj       date       mom  group     size  \\\n",
       "0  013155   11082  2005 -0.620109 2006-06-30 -0.299828    1.0  16.4565   \n",
       "\n",
       "        ret  \n",
       "0  0.007888  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp6.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th>ret</th>\n",
       "      <th>jyear</th>\n",
       "      <th>formdate</th>\n",
       "      <th>dgtw_port</th>\n",
       "      <th>dgtw_vwret</th>\n",
       "      <th>dgtw_xret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1542</td>\n",
       "      <td>10001</td>\n",
       "      <td>2006-07-31</td>\n",
       "      <td>0.157417</td>\n",
       "      <td>2006</td>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>0.178656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      permno       date       ret  jyear   formdate dgtw_port  dgtw_vwret  \\\n",
       "1542   10001 2006-07-31  0.157417   2006 2006-06-30       114   -0.021239   \n",
       "\n",
       "      dgtw_xret  \n",
       "1542   0.178656  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgtw_returns.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgtw_returns.to_csv('data/dgtw_returns.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Fama French Factors\n",
    "# April 2018\n",
    "# Qingyi (Freda) Song Drechsler\n",
    "##########################################\n",
    "\n",
    "import datetime as dt\n",
    "import psycopg2 \n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import *\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###################\n",
    "# Compustat Block #\n",
    "###################\n",
    "comp = conn.raw_sql(\"\"\"\n",
    "                    select gvkey, datadate, at, pstkl, txditc,\n",
    "                    pstkrv, seq, pstk\n",
    "                    from comp.funda\n",
    "                    where indfmt='INDL' \n",
    "                    and datafmt='STD'\n",
    "                    and popsrc='D'\n",
    "                    and consol='C'\n",
    "                    and datadate >= '01/01/1959'\n",
    "                    \"\"\")\n",
    "\n",
    "comp['datadate']=pd.to_datetime(comp['datadate']) #convert datadate to date fmt\n",
    "comp['year']=comp['datadate'].dt.year\n",
    "\n",
    "# create preferrerd stock\n",
    "comp['ps']=np.where(comp['pstkrv'].isnull(), comp['pstkl'], comp['pstkrv'])\n",
    "comp['ps']=np.where(comp['ps'].isnull(),comp['pstk'], comp['ps'])\n",
    "comp['ps']=np.where(comp['ps'].isnull(),0,comp['ps'])\n",
    "\n",
    "comp['txditc']=comp['txditc'].fillna(0)\n",
    "\n",
    "# create book equity\n",
    "comp['be']=comp['seq']+comp['txditc']-comp['ps']\n",
    "comp['be']=np.where(comp['be']>0, comp['be'], np.nan)\n",
    "\n",
    "# number of years in Compustat\n",
    "comp=comp.sort_values(by=['gvkey','datadate'])\n",
    "comp['count']=comp.groupby(['gvkey']).cumcount()\n",
    "\n",
    "comp=comp[['gvkey','datadate','year','be','count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###################\n",
    "# CRSP Block      #\n",
    "###################\n",
    "# sql similar to crspmerge macro\n",
    "crsp_m = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.permco, a.date, b.shrcd, b.exchcd,\n",
    "                      a.ret, a.retx, a.shrout, a.prc\n",
    "                      from crsp.msf as a\n",
    "                      left join crsp.msenames as b\n",
    "                      on a.permno=b.permno\n",
    "                      and b.namedt<=a.date\n",
    "                      and a.date<=b.nameendt\n",
    "                      where a.date between '01/01/1959' and '12/31/2017'\n",
    "                      and b.exchcd between 1 and 3\n",
    "                      \"\"\") \n",
    "\n",
    "# change variable format to int\n",
    "crsp_m[['permco','permno','shrcd','exchcd']]=crsp_m[['permco','permno','shrcd','exchcd']].astype(int)\n",
    "\n",
    "# Line up date to be end of month\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date'])\n",
    "crsp_m['jdate']=crsp_m['date']+MonthEnd(0)\n",
    "\n",
    "# add delisting return\n",
    "dlret = conn.raw_sql(\"\"\"\n",
    "                     select permno, dlret, dlstdt \n",
    "                     from crsp.msedelist\n",
    "                     \"\"\")\n",
    "dlret.permno=dlret.permno.astype(int)\n",
    "dlret['dlstdt']=pd.to_datetime(dlret['dlstdt'])\n",
    "dlret['jdate']=dlret['dlstdt']+MonthEnd(0)\n",
    "\n",
    "crsp = pd.merge(crsp_m, dlret, how='left',on=['permno','jdate'])\n",
    "crsp['dlret']=crsp['dlret'].fillna(0)\n",
    "crsp['ret']=crsp['ret'].fillna(0)\n",
    "crsp['retadj']=(1+crsp['ret'])*(1+crsp['dlret'])-1\n",
    "crsp['me']=crsp['prc'].abs()*crsp['shrout'] # calculate market equity\n",
    "crsp=crsp.drop(['dlret','dlstdt','prc','shrout'], axis=1)\n",
    "crsp=crsp.sort_values(by=['jdate','permco','me'])\n",
    "\n",
    "### Aggregate Market Cap ###\n",
    "# sum of me across different permno belonging to same permco a given date\n",
    "crsp_summe = crsp.groupby(['jdate','permco'])['me'].sum().reset_index()\n",
    "# largest mktcap within a permco/date\n",
    "crsp_maxme = crsp.groupby(['jdate','permco'])['me'].max().reset_index()\n",
    "# join by jdate/maxme to find the permno\n",
    "crsp1=pd.merge(crsp, crsp_maxme, how='inner', on=['jdate','permco','me'])\n",
    "# drop me column and replace with the sum me\n",
    "crsp1=crsp1.drop(['me'], axis=1)\n",
    "# join with sum of me to get the correct market cap info\n",
    "crsp2=pd.merge(crsp1, crsp_summe, how='inner', on=['jdate','permco'])\n",
    "# sort by permno and date and also drop duplicates\n",
    "crsp2=crsp2.sort_values(by=['permno','jdate']).drop_duplicates()\n",
    "\n",
    "# keep December market cap\n",
    "crsp2['year']=crsp2['jdate'].dt.year\n",
    "crsp2['month']=crsp2['jdate'].dt.month\n",
    "decme=crsp2[crsp2['month']==12]\n",
    "decme=decme[['permno','date','jdate','me','year']].rename(columns={'me':'dec_me'})\n",
    "\n",
    "### July to June dates\n",
    "crsp2['ffdate']=crsp2['jdate']+MonthEnd(-6)\n",
    "crsp2['ffyear']=crsp2['ffdate'].dt.year\n",
    "crsp2['ffmonth']=crsp2['ffdate'].dt.month\n",
    "crsp2['1+retx']=1+crsp2['retx']\n",
    "crsp2=crsp2.sort_values(by=['permno','date'])\n",
    "\n",
    "# cumret by stock\n",
    "crsp2['cumretx']=crsp2.groupby(['permno','ffyear'])['1+retx'].cumprod()\n",
    "# lag cumret\n",
    "crsp2['lcumretx']=crsp2.groupby(['permno'])['cumretx'].shift(1)\n",
    "\n",
    "# lag market cap\n",
    "crsp2['lme']=crsp2.groupby(['permno'])['me'].shift(1)\n",
    "\n",
    "# if first permno then use me/(1+retx) to replace the missing value\n",
    "crsp2['count']=crsp2.groupby(['permno']).cumcount()\n",
    "crsp2['lme']=np.where(crsp2['count']==0, crsp2['me']/crsp2['1+retx'], crsp2['lme'])\n",
    "\n",
    "# baseline me\n",
    "mebase=crsp2[crsp2['ffmonth']==1][['permno','ffyear', 'lme']].rename(columns={'lme':'mebase'})\n",
    "\n",
    "# merge result back together\n",
    "crsp3=pd.merge(crsp2, mebase, how='left', on=['permno','ffyear'])\n",
    "crsp3['wt']=np.where(crsp3['ffmonth']==1, crsp3['lme'], crsp3['mebase']*crsp3['lcumretx'])\n",
    "\n",
    "decme['year']=decme['year']+1\n",
    "decme=decme[['permno','year','dec_me']]\n",
    "\n",
    "# Info as of June\n",
    "crsp3_jun = crsp3[crsp3['month']==6]\n",
    "\n",
    "crsp_jun = pd.merge(crsp3_jun, decme, how='inner', on=['permno','year'])\n",
    "crsp_jun=crsp_jun[['permno','date', 'jdate', 'shrcd','exchcd','retadj','me','wt','cumretx','mebase','lme','dec_me']]\n",
    "crsp_jun=crsp_jun.sort_values(by=['permno','jdate']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#######################\n",
    "# CCM Block           #\n",
    "#######################\n",
    "ccm=conn.raw_sql(\"\"\"\n",
    "                  select gvkey, lpermno as permno, linktype, linkprim, \n",
    "                  linkdt, linkenddt\n",
    "                  from crsp.ccmxpf_linktable\n",
    "                  where substr(linktype,1,1)='L'\n",
    "                  and (linkprim ='C' or linkprim='P')\n",
    "                  \"\"\")\n",
    "\n",
    "ccm['linkdt']=pd.to_datetime(ccm['linkdt'])\n",
    "ccm['linkenddt']=pd.to_datetime(ccm['linkenddt'])\n",
    "# if linkenddt is missing then set to today date\n",
    "ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "ccm1=pd.merge(comp[['gvkey','datadate','be', 'count']],ccm,how='left',on=['gvkey'])\n",
    "ccm1['yearend']=ccm1['datadate']+YearEnd(0)\n",
    "ccm1['jdate']=ccm1['yearend']+MonthEnd(6)\n",
    "\n",
    "# set link date bounds\n",
    "ccm2=ccm1[(ccm1['jdate']>=ccm1['linkdt'])&(ccm1['jdate']<=ccm1['linkenddt'])]\n",
    "ccm2=ccm2[['gvkey','permno','datadate','yearend', 'jdate','be', 'count']]\n",
    "\n",
    "# link comp and crsp\n",
    "ccm_jun=pd.merge(crsp_jun, ccm2, how='inner', on=['permno', 'jdate'])\n",
    "ccm_jun['beme']=ccm_jun['be']*1000/ccm_jun['dec_me']\n",
    "\n",
    "# select NYSE stocks for bucket breakdown\n",
    "# exchcd = 1 and positive beme and positive me and shrcd in (10,11) and at least 2 years in comp\n",
    "nyse=ccm_jun[(ccm_jun['exchcd']==1) & (ccm_jun['beme']>0) & (ccm_jun['me']>0) & (ccm_jun['count']>1) & ((ccm_jun['shrcd']==10) | (ccm_jun['shrcd']==11))]\n",
    "# size breakdown\n",
    "nyse_sz=nyse.groupby(['jdate'])['me'].median().to_frame().reset_index().rename(columns={'me':'sizemedn'})\n",
    "# beme breakdown\n",
    "nyse_bm=nyse.groupby(['jdate'])['beme'].describe(percentiles=[0.3, 0.7]).reset_index()\n",
    "nyse_bm=nyse_bm[['jdate','30%','70%']].rename(columns={'30%':'bm30', '70%':'bm70'})\n",
    "\n",
    "nyse_breaks = pd.merge(nyse_sz, nyse_bm, how='inner', on=['jdate'])\n",
    "# join back size and beme breakdown\n",
    "ccm1_jun = pd.merge(ccm_jun, nyse_breaks, how='left', on=['jdate'])\n",
    "\n",
    "\n",
    "# function to assign sz and bm bucket\n",
    "def sz_bucket(row):\n",
    "    if row['me']==np.nan:\n",
    "        value=''\n",
    "    elif row['me']<=row['sizemedn']:\n",
    "        value='S'\n",
    "    else:\n",
    "        value='B'\n",
    "    return value\n",
    "\n",
    "def bm_bucket(row):\n",
    "    if 0<=row['beme']<=row['bm30']:\n",
    "        value = 'L'\n",
    "    elif row['beme']<=row['bm70']:\n",
    "        value='M'\n",
    "    elif row['beme']>row['bm70']:\n",
    "        value='H'\n",
    "    else:\n",
    "        value=''\n",
    "    return value\n",
    "\n",
    "# assign size portfolio\n",
    "ccm1_jun['szport']=np.where((ccm1_jun['beme']>0)&(ccm1_jun['me']>0)&(ccm1_jun['count']>=1), ccm1_jun.apply(sz_bucket, axis=1), '')\n",
    "# assign book-to-market portfolio\n",
    "ccm1_jun['bmport']=np.where((ccm1_jun['beme']>0)&(ccm1_jun['me']>0)&(ccm1_jun['count']>=1), ccm1_jun.apply(bm_bucket, axis=1), '')\n",
    "# create positivebmeme and nonmissport variable\n",
    "ccm1_jun['posbm']=np.where((ccm1_jun['beme']>0)&(ccm1_jun['me']>0)&(ccm1_jun['count']>=1), 1, 0)\n",
    "ccm1_jun['nonmissport']=np.where((ccm1_jun['bmport']!=''), 1, 0)\n",
    "\n",
    "# store portfolio assignment as of June\n",
    "june=ccm1_jun[['permno','date', 'jdate', 'bmport','szport','posbm','nonmissport']]\n",
    "june['ffyear']=june['jdate'].dt.year\n",
    "\n",
    "# merge back with monthly records\n",
    "crsp3 = crsp3[['date','permno','shrcd','exchcd','retadj','me','wt','cumretx','ffyear','jdate']]\n",
    "ccm3=pd.merge(crsp3, \n",
    "        june[['permno','ffyear','szport','bmport','posbm','nonmissport']], how='left', on=['permno','ffyear'])\n",
    "\n",
    "# keeping only records that meet the criteria\n",
    "ccm4=ccm3[(ccm3['wt']>0)& (ccm3['posbm']==1) & (ccm3['nonmissport']==1) & \n",
    "          ((ccm3['shrcd']==10) | (ccm3['shrcd']==11))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>shrcd</th>\n",
       "      <th>exchcd</th>\n",
       "      <th>retadj</th>\n",
       "      <th>me</th>\n",
       "      <th>wt</th>\n",
       "      <th>cumretx</th>\n",
       "      <th>ffyear</th>\n",
       "      <th>jdate</th>\n",
       "      <th>szport</th>\n",
       "      <th>bmport</th>\n",
       "      <th>posbm</th>\n",
       "      <th>nonmissport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1988-07-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>6386.0</td>\n",
       "      <td>6200.000000</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1988</td>\n",
       "      <td>1988-07-31</td>\n",
       "      <td>S</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1988-08-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>6572.0</td>\n",
       "      <td>6385.999996</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1988</td>\n",
       "      <td>1988-08-31</td>\n",
       "      <td>S</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  permno  shrcd  exchcd    retadj      me           wt  cumretx  \\\n",
       "47 1988-07-29   10001     11       3  0.030000  6386.0  6200.000000     1.03   \n",
       "48 1988-08-31   10001     11       3  0.029126  6572.0  6385.999996     1.06   \n",
       "\n",
       "    ffyear      jdate szport bmport  posbm  nonmissport  \n",
       "47    1988 1988-07-31      S      H    1.0          1.0  \n",
       "48    1988 1988-08-31      S      H    1.0          1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccm4.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ccm4- saved as -ff3_port-\n"
     ]
    }
   ],
   "source": [
    "sv('ccm4', 'ff3_port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Form Fama French Factors #\n",
    "############################\n",
    "\n",
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "\n",
    "# value-weigthed return\n",
    "vwret=ccm4.groupby(['jdate','szport','bmport']).apply(wavg, 'retadj','wt').to_frame().reset_index().rename(columns={0: 'vwret'})\n",
    "vwret['sbport']=vwret['szport']+vwret['bmport']\n",
    "\n",
    "# firm count\n",
    "vwret_n=ccm4.groupby(['jdate','szport','bmport'])['retadj'].count().reset_index().rename(columns={'retadj':'n_firms'})\n",
    "vwret_n['sbport']=vwret_n['szport']+vwret_n['bmport']\n",
    "\n",
    "# tranpose\n",
    "ff_factors=vwret.pivot(index='jdate', columns='sbport', values='vwret').reset_index()\n",
    "ff_nfirms=vwret_n.pivot(index='jdate', columns='sbport', values='n_firms').reset_index()\n",
    "\n",
    "# create SMB and HML factors\n",
    "ff_factors['WH']=(ff_factors['BH']+ff_factors['SH'])/2\n",
    "ff_factors['WL']=(ff_factors['BL']+ff_factors['SL'])/2\n",
    "ff_factors['WHML'] = ff_factors['WH']-ff_factors['WL']\n",
    "\n",
    "ff_factors['WB']=(ff_factors['BL']+ff_factors['BM']+ff_factors['BH'])/3\n",
    "ff_factors['WS']=(ff_factors['SL']+ff_factors['SM']+ff_factors['SH'])/3\n",
    "ff_factors['WSMB'] = ff_factors['WS']-ff_factors['WB']\n",
    "ff_factors=ff_factors.rename(columns={'jdate':'date'})\n",
    "\n",
    "# n firm count\n",
    "ff_nfirms['H']=ff_nfirms['SH']+ff_nfirms['BH']\n",
    "ff_nfirms['L']=ff_nfirms['SL']+ff_nfirms['BL']\n",
    "ff_nfirms['HML']=ff_nfirms['H']+ff_nfirms['L']\n",
    "\n",
    "ff_nfirms['B']=ff_nfirms['BL']+ff_nfirms['BM']+ff_nfirms['BH']\n",
    "ff_nfirms['S']=ff_nfirms['SL']+ff_nfirms['SM']+ff_nfirms['SH']\n",
    "ff_nfirms['SMB']=ff_nfirms['B']+ff_nfirms['S']\n",
    "ff_nfirms['TOTAL']=ff_nfirms['SMB']\n",
    "ff_nfirms=ff_nfirms.rename(columns={'jdate':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sbport</th>\n",
       "      <th>date</th>\n",
       "      <th>BH</th>\n",
       "      <th>BL</th>\n",
       "      <th>BM</th>\n",
       "      <th>SH</th>\n",
       "      <th>SL</th>\n",
       "      <th>SM</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>HML</th>\n",
       "      <th>B</th>\n",
       "      <th>S</th>\n",
       "      <th>SMB</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-07-31</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sbport       date  BH  BL  BM  SH  SL  SM   H   L  HML   B   S  SMB  TOTAL\n",
       "0      1962-07-31  15  12  21  14  17  18  29  29   58  48  49   97     97"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_nfirms.iloc[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size-port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Size Portfolio for CRSP Securitie      #\n",
    "# July 2018                              #                        \n",
    "# Qingyi (Freda) Song Drechsler          #\n",
    "##########################################\n",
    "\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "################################################\n",
    "# Get CRSP Monthly Stocks for Decile Formation #\n",
    "################################################\n",
    "msf = conn.raw_sql(\"\"\"\n",
    "                      select a.permno, a.date, \n",
    "                      a.ret, a.shrout, a.prc \n",
    "                      from crsp.msf as a\n",
    "                      where a.date >= '12/01/1999'\n",
    "                      \"\"\", date_cols=['date'])\n",
    "\n",
    "# keep only records with non missing ret prc and shrout value\n",
    "msf = msf[(msf['prc'].notna()) & (msf['ret'].notna()) & (msf['shrout'].notna())]\n",
    "\n",
    "msf['permno'] = msf['permno'].astype(int)\n",
    "msf['size'] = msf['shrout'] * msf['prc'].abs()\n",
    "msf['year'] = msf['date'].dt.year\n",
    "msf['month'] = msf['date'].dt.month\n",
    "\n",
    "# create msf_dec\n",
    "msf_dec = msf[msf['month']==12][['date','permno','year','size']]\n",
    "\n",
    "# create msf_ls\n",
    "msf_ls = msf.sort_values(['permno', 'date'])\n",
    "msf_ls['year_prev'] = msf_ls['year']-1\n",
    "msf_ls['size_lag'] = msf_ls.groupby('permno')['size'].shift(1)\n",
    "msf_ls['size_lag'] = np.where(msf_ls['size_lag'].isna(),\\\n",
    " msf_ls['size']/(1+msf_ls['ret']), msf_ls['size_lag'])\n",
    "\n",
    "#################################\n",
    "# Compute Deciles for Each DEC  #\n",
    "#################################\n",
    "msf_dec = msf_dec.sort_values(['year'])\n",
    "msf_dec['decile']=1+msf_dec.groupby('year')['size']\\\n",
    ".transform(lambda x: pd.qcut(x, 10, labels=False))\n",
    "\n",
    "###################################\n",
    "# Assign Size Group to All Months #\n",
    "###################################\n",
    "msf_groups = pd.merge(msf_ls[['permno','date','ret','size_lag','year_prev']], \\\n",
    "                      msf_dec[['permno','year','decile']], how='left', \\\n",
    "                      left_on=['permno','year_prev'], right_on=['permno','year'])\n",
    "\n",
    "msf_groups=msf_groups[msf_groups['decile'].notna()]\n",
    "\n",
    "#################################\n",
    "# Compute Size Weighted Returns #\n",
    "#################################\n",
    "msf_groups = msf_groups.sort_values(['decile', 'date'])\n",
    "\n",
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "\n",
    "# value-weigthed return\n",
    "vwrets=msf_groups.groupby(['decile','date']).apply(wavg, 'ret','size_lag')\\\n",
    ".to_frame().reset_index().rename(columns={0: 'vwret'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "msf_groups.to_csv('data/size_port.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## \n",
    "# Compare Results with CRSP MSIX #\n",
    "##################################\n",
    "msix = conn.raw_sql(\"\"\"\n",
    "                      select caldt, decret1, decret2, decret3, decret4, decret5,\n",
    "                      decret6, decret7, decret8, decret9, decret10\n",
    "                      from crsp.msix where caldt >= '12/01/1999'\n",
    "                      \"\"\", date_cols=['caldt']) \n",
    "\n",
    "# transpose msix data\n",
    "msix1=pd.melt(msix, id_vars='caldt', \\\n",
    "              value_vars=['decret1','decret2', 'decret3', 'decret4', 'decret5', 'decret6', \\\n",
    "'decret7', 'decret8','decret9','decret10'])\n",
    "\n",
    "# extract decile information from decret\n",
    "msix1['decile'] = msix1['variable'].str[6:].astype(int)\n",
    "# rename return column\n",
    "msix1 = msix1.rename(columns={'value':'decret', 'caldt':'date'})\n",
    "msix1 = msix1.drop(['variable'], axis=1)\n",
    "\n",
    "decile_returns = pd.merge(vwrets, msix1, how='left', on=['date','decile'])\n",
    "\n",
    "###################\n",
    "# End of Program  #\n",
    "###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}