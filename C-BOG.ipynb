{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, HYPHENS\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'C:/Users/rossz/OneDrive/CC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner', 'tagger', 'sentencizer', 'parser'])\n",
    "\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        # EDIT: commented out regex that splits on hyphens between letters:\n",
    "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        # EDIT: '/' not considered as a separator:\n",
    "        r\"(?<=[{a}0-9])[:<>=](?=[{a}])\".format(a=ALPHA),\n",
    "    ])\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "nlp.max_length = 8000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = pd.read_feather(f'{DATA_DIR}/f_sue_keydevid_car_finratio_transcriptid_text.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%time cc['text_present'] = list(nlp.pipe(cc['text_present'])) # 2min 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%time cc['text_qa'] = list(nlp.pipe(cc['text_qa'])) # 8min 38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 4s\n"
     ]
    }
   ],
   "source": [
    "%time cc['text_all'] = list(nlp.pipe(cc['text_all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Only use lowercase (`use_norm=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_count(doc, use_norm=False):\n",
    "    bigram = defaultdict(int)\n",
    "    for token in doc[:(len(doc)-1)]:\n",
    "        nbor = token.nbor()\n",
    "        if token.is_punct or nbor.is_punct or token.is_space or nbor.is_space \\\n",
    "            or token.like_num or nbor.like_num or token.is_currency or nbor.is_currency \\\n",
    "            or token.like_url or nbor.like_url or token.like_email or nbor.like_email:\n",
    "            continue\n",
    "        if use_norm:\n",
    "            token, nbor = token.norm_, nbor.norm_\n",
    "        bigram[f'{token} {nbor}'] += 1\n",
    "    return bigram\n",
    "\n",
    "def get_unigram_count(doc, use_norm=False):\n",
    "    unigram = defaultdict(int)\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or token.like_num or token.is_currency \\\n",
    "            or token.like_url or token.like_email:\n",
    "            continue\n",
    "        if use_norm: \n",
    "            token = token.norm_\n",
    "        unigram[f'{token}'] += 1\n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cc['text_present_unigram_count'] = cc.text_present.apply(get_unigram_count, use_norm=True)\n",
    "cc['text_present_bigram_count'] = cc.text_present.apply(get_bigram_count, use_norm=True)\n",
    "cc['text_present_allgram_count'] = [{**row.text_present_unigram_count, **row.text_present_bigram_count} for row in cc.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cc['text_qa_unigram_count'] = cc.text_qa.apply(get_unigram_count, use_norm=True)\n",
    "cc['text_qa_bigram_count'] = cc.text_qa.apply(get_bigram_count, use_norm=True)\n",
    "cc['text_qa_allgram_count'] = [{**row.text_qa_unigram_count, **row.text_qa_bigram_count} for row in cc.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "cc['text_all_unigram_count'] = cc.text_all.apply(get_unigram_count, use_norm=True) \n",
    "cc['text_all_bigram_count'] = cc.text_all.apply(get_bigram_count, use_norm=True) \n",
    "cc['text_all_allgram_count'] = [{**row.text_all_unigram_count, **row.text_all_bigram_count} for row in cc.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "cc[['docid', \n",
    "    'text_present_unigram_count', 'text_present_bigram_count', 'text_present_allgram_count',\n",
    "    'text_qa_unigram_count', 'text_qa_bigram_count', 'text_qa_allgram_count',\n",
    "    'text_all_unigram_count', 'text_all_bigram_count', 'text_all_allgram_count']].to_pickle('data/cc_ngram.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get `term_freq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure cc is already in the global!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-cc_ngram- loaded\n"
     ]
    }
   ],
   "source": [
    "ld('cc_ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCorpus():\n",
    "    def __init__(self, ngram_type: str):\n",
    "        '''\n",
    "        ngram_type: 'unigram' or 'bigram'\n",
    "        '''\n",
    "        assert 'cc_ngram' in globals(), 'Load `cc_ngram` first!'\n",
    "        global cc_ngram\n",
    "        \n",
    "        self.ngram_type = ngram_type\n",
    "        self.docid = cc_ngram.docid.to_numpy()\n",
    "        self.ndoc = len(self.docid)\n",
    "        \n",
    "        # create vocab\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        vocab = set(itertools.chain(*[d.keys() for d in cc_ngram[f'text_all_{self.ngram_type}_count']]))\n",
    "        vocab = np.array(list(vocab))\n",
    "        self.nvocab = len(vocab)\n",
    "        self.word2idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def get_dtm_by_cctype(self, cc_type:str):\n",
    "        '''\n",
    "        cc_type: 'present' or 'qa'\n",
    "        '''\n",
    "        global cc_ngram\n",
    "\n",
    "        text_col = f'text_{cc_type}_{self.ngram_type}_count'\n",
    "        \n",
    "        n_nonzero = sum(len(d) for d in cc_ngram[text_col])\n",
    "\n",
    "        # make a list of document names\n",
    "        # the order will be the same as in the dict\n",
    "        vocab_sorter = np.argsort(self.vocab)    # indices that sort \"vocab\"\n",
    "\n",
    "        data = np.empty(n_nonzero, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "        rows = np.empty(n_nonzero, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "        cols = np.empty(n_nonzero, dtype=np.intc)     # column index for kth data item (kth term freq.)\n",
    "\n",
    "        ind = 0     # current index in the sparse matrix data\n",
    "\n",
    "        # fill dtm\n",
    "        for doc_i, row in tqdm(enumerate(cc_ngram.itertuples()), total=len(cc_ngram), desc=f'Building {cc_type.upper()} dtm'):\n",
    "            # find indices into  such that, if the corresponding elements in  were\n",
    "            # inserted before the indices, the order of  would be preserved\n",
    "            # -> array of indices of  in \n",
    "            unique_indices = vocab_sorter[np.searchsorted(self.vocab, list(getattr(row, text_col).keys()), sorter=vocab_sorter)]\n",
    "\n",
    "            # count the unique terms of the document and get their vocabulary indices\n",
    "            counts = np.array(list(getattr(row, text_col).values()), dtype=np.int32)\n",
    "\n",
    "            n_vals = len(unique_indices)  # = number of unique terms\n",
    "            ind_end = ind + n_vals  #  to  is the slice that we will fill with data\n",
    "\n",
    "            data[ind:ind_end] = counts                  # save the counts (term frequencies)\n",
    "            cols[ind:ind_end] = unique_indices            # save the column index: index in \n",
    "            rows[ind:ind_end] = np.repeat(doc_i, n_vals)  # save it as repeated value\n",
    "\n",
    "            ind = ind_end  # resume with next document -> add data to the end\n",
    "        \n",
    "        # final dtm\n",
    "        dtm = coo_matrix((data, (rows, cols)), shape=(self.ndoc, self.nvocab), dtype=np.intc)\n",
    "        if cc_type=='present': self.dtm_present = dtm\n",
    "        if cc_type=='qa': self.dtm_qa = dtm\n",
    "        if cc_type=='all': self.dtm_all = dtm\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_dtm_all(self):\n",
    "        self.get_dtm_by_cctype('present')\n",
    "        self.get_dtm_by_cctype('qa')\n",
    "        self.get_dtm_by_cctype('all')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ed087af1bc4ef9aaeb9c2d77fbf9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building PRESENT dtm', max=25652.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9c423b1c0d40a68424372017122957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building QA dtm', max=25652.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80921bd5de1407cba9b9afec67393cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building ALL dtm', max=25652.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-corp_unigram- saved\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corp_unigram = CCCorpus('unigram').get_dtm_all()\n",
    "sv('corp_unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77713c4d4bc347b5beae5498e92f605a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building PRESENT dtm', max=25652.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed24208b5604dda8b1c357f9ac3f74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building QA dtm', max=25652.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd26eca14e348b3b2183d2a76e6960d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building ALL dtm', max=25652.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-corp_bigram- saved\n",
      "Wall time: 28min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corp_bigram = CCCorpus('bigram').get_dtm_all()\n",
    "sv('corp_bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_allgram = CCCorpus('allgram').get_dtm_all()\n",
    "sv('corp_allgram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-corp_unigram- already exists, will not load again!\n",
      "-corp_bigram- already exists, will not load again!\n",
      "-corp_allgram- already exists, will not load again!\n"
     ]
    }
   ],
   "source": [
    "ld('corp_unigram')\n",
    "ld('corp_bigram')\n",
    "ld('corp_allgram')\n",
    "\n",
    "def get_similarity(corp_list):\n",
    "    sim_df = []\n",
    "    for corp in corp_list:\n",
    "        sim = np.array(corp.dtm_present.multiply(corp.dtm_qa).sum(1)/corp.nvocab).squeeze() # should be (N,)\n",
    "        sim_df.append(pd.DataFrame({'docid':corp.docid, f'similarity_{corp.ngram_type}':sim}).set_index('docid'))\n",
    "    \n",
    "    sim_df = pd.concat(sim_df, axis=1).reset_index()\n",
    "    sim_df.to_feather('data/similarity.feather')\n",
    "    return sim_df \n",
    "    \n",
    "sim_df = get_similarity([corp_unigram, corp_bigram, corp_allgram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Add `sim_ngram` to `cc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = pd.read_feather(f'{DATA_DIR}/f_sue_keydevid_car_finratio_transcriptid_text.feather')\n",
    "cc['sim_unigram'] = sim_unigram\n",
    "cc['sim_bigram'] = sim_bigram\n",
    "cc['sim_allgram'] = sim_allgram\n",
    "cc.to_feather(f'{DATA_DIR}/f_sue_keydevid_car_finratio_transcriptid_text_sim.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- I didn't use `STOPWORDS`, because our project is very domain-specific and common stopwords might have important meaning.\n",
    "- I only remove the most least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-corp_unigram- loaded as -corp_unigram- (forced)\n",
      "-corp_bigram- loaded as -corp_bigram- (forced)\n",
      "-corp_allgram- loaded as -corp_allgram- (forced)\n"
     ]
    }
   ],
   "source": [
    "ld('corp_unigram', force=True)\n",
    "ld('corp_bigram', force=True)\n",
    "ld('corp_allgram', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep top 10000 words\n",
      "Keep top 10000 words\n",
      "Keep top 10000 words\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def filter_corpus(corpus, keep_size):\n",
    "    '''\n",
    "    criterion: which ngram to use as frequency count base, e.g., if `filter_ngram==all`, then will find the most \n",
    "        frequent words, regardless it's unigram or bigram.\n",
    "    '''\n",
    "    word_freq_sort_idx = np.argsort(np.asarray(corpus.dtm_all.sum(0)).squeeze())[::-1]\n",
    "    \n",
    "    keep_vocab_idx = word_freq_sort_idx[:keep_size]\n",
    "    keep_vocab = corpus.vocab[keep_vocab_idx]\n",
    "    \n",
    "    # overwrite corpus.vocab\n",
    "    corpus.vocab = keep_vocab\n",
    "    \n",
    "    # overwrite corpus.dtm\n",
    "    corpus.dtm_present = corpus.dtm_present.tocsc()[:,keep_vocab_idx]\n",
    "    corpus.dtm_qa = corpus.dtm_qa.tocsc()[:,keep_vocab_idx]\n",
    "    corpus.dtm_all = corpus.dtm_all.tocsc()[:,keep_vocab_idx]\n",
    "    print(f'Keep top {len(keep_vocab)} words')\n",
    "\n",
    "    return corpus\n",
    "\n",
    "keep_size = 10000\n",
    "\n",
    "f_corp_unigram = filter_corpus(corp_unigram, keep_size)\n",
    "f_corp_bigram = filter_corpus(corp_bigram, keep_size)\n",
    "f_corp_allgram = filter_corpus(corp_allgram, keep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f_corp_unigram- saved\n",
      "-f_corp_bigram- saved\n",
      "-f_corp_allgram- saved\n"
     ]
    }
   ],
   "source": [
    "sv('f_corp_unigram')\n",
    "sv('f_corp_bigram')\n",
    "sv('f_corp_allgram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-corp_unigram- loaded as -corp_unigram- (forced)\n",
      "-corp_bigram- loaded as -corp_bigram- (forced)\n",
      "-corp_allgram- loaded as -corp_allgram- (forced)\n"
     ]
    }
   ],
   "source": [
    "ld('f_corp_unigram', force=True)\n",
    "ld('f_corp_bigram', force=True)\n",
    "ld('f_corp_allgram', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_tfidf(dtm):\n",
    "    return TfidfTransformer().fit_transform(dtm)\n",
    "\n",
    "tfidf_present_unigram = get_tfidf(f_corp_unigram.dtm_present)\n",
    "tfidf_qa_unigram = get_tfidf(f_corp_unigram.dtm_qa)\n",
    "tfidf_all_unigram = get_tfidf(f_corp_unigram.dtm_all)\n",
    "\n",
    "tfidf_present_bigram = get_tfidf(f_corp_bigram.dtm_present)\n",
    "tfidf_qa_bigram = get_tfidf(f_corp_bigram.dtm_qa)\n",
    "tfidf_all_bigram = get_tfidf(f_corp_bigram.dtm_all)\n",
    "\n",
    "tfidf_present_allgram = get_tfidf(f_corp_allgram.dtm_present)\n",
    "tfidf_qa_allgram = get_tfidf(f_corp_allgram.dtm_qa)\n",
    "tfidf_all_allgram = get_tfidf(f_corp_allgram.dtm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a MatrixMarket for regression in R\n",
    "for cc_type in ['present', 'qa', 'all']:\n",
    "    for ngram_type in ['unigram', 'bigram', 'allgram']:\n",
    "        tfidf_name = f'tfidf_{cc_type}_{ngram_type}'\n",
    "        scipy.io.mmwrite(f'data/{tfidf_name}.mtx', globals()[tfidf_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
